{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#naren-kubernetes-solutions","title":"Naren Kubernetes Solutions","text":"Date Topic 02/10/2022 How to create, package , push, host, deploy Private Helm Charts using Chartmuseum 20/11/2022 Setting up a secured local registry (local docker or k8s kind cluster) 25/11/2022 Setting up ArgoCD in k8s cluster with local User &amp; RBAC 15/01/2023 Emissary-ingress quick start in KIND Cluster (Windows) 01/02/2023 Setting up your Own PKI with OpenSSL 12/07/2023 Resource Management for Pods and Containers 26/08/2023 Setting up ArgoCD with OIDC login in development environment (insecure)  03/09/2023 Configuring User Access to Your Kubernetes Cluster: A Step-by-Step Guide 18/09/2023 Extract, transform, and load (ETL) of the data using python(1/n) 30/09/2023 Host your test kubernetes cluster  21/10/2023 Setup Production Grade Keycloak in Kubernetes(1/n)  20/11/2023 Setting up Monitoring Stack in a Node (docker container) 20/11/2023 Check all running images's vulnerability &amp; size running in k8s cluster 27/11/2023 Install basic Loki Promtail Grafana stack in KIND cluster  05/01/2024 Local setup for testing vmbackup and vmrestore  14/01/2024 Securing Kubernetes Traffic: A Step-by-Step Guide to Setting up Ingress Controller with Self-Signed Certificates and mTLS 03/02/2024 Setting up Basic Harbor Registry in a Kubernetes Cluster 04/02/2024 Setting up Thanos for long term storage of prometheus metrics 04/02/2024 Setting up NKS PKI (Own Certificate via Cert-Manager in a Kubernetes cluster) 20/02/2024 Deploying Basic Kafka cluster with the help of Strimzi operators (1/n) 20/02/2024 Setup Minio S3 with Transport Layer Security (TLS) 1.2+ encryption 31/03/2024 Multiple Sources for an ArgoCD Application 11/04/2024 Install Simple Scalable Loki Promtail Grafana stack in KIND cluster 28/04/2024 Deploy a batch monitoring stack with Prometheus PushGateway in Kubernetes cluster 05/05/2024 Securing Kubernetes Secrets with Argocd-vault-plugin 03/07/2024 Interacting with Strimzi Kafka cluster through Kafka Bridge(2/n) 24/07/2024 Orchestrating Thousands of Kubernetes Clusters through ArgoCD 10/08/2024 Enhancing Kubernetes Monitoring &amp; Automation with Robusta! 15/09/2024 Step-by-Step Guide to Setting Up Multiple Kind Clusters on a Single Host 17/08/2024 Setup Argocd with extra cluster and git integrated 14/09/2024 Step-by-Step Guide to Setting Up Multiple Kind Clusters on a Single Host 22/09/2024 Take Control of Your Observability Data with vector.dev 29/09/2024 Set Up S3 Bucket Replication(MINIO) 04/01/2025 This is my First Attempt to interact with a AI model via an Agent 31/03/2025 Exploring VictoriaLogs: A Fast and Resource-Efficient Log Management Solution 01/07/2025 AI-Powered YouTube Summarizer \u2014 From Video to Google Spreadsheets 20/07/2025 Unlocking GitHub Copilot Metrics 01/08/2025 Naren GPT - Your Personal Local AI Chat Assistant 27/08/2025 Chroma: The Open-Source Vector Store You Control 21/09/2025 Why, When, How, and What about Kafka"},{"location":"about/","title":"Narendranath Panda","text":"<p>copyright: \u00a9 2024 Narendranath Panda</p> <ol> <li> <p>To me, giving back is so important. It makes others feel good which then in return makes me feel good \u2764\ufe0f at https://github.com/naren4b/nks.\u00a0\u21a9</p> </li> </ol>"},{"location":"access-kube-api/","title":"Access kube-api server by curl","text":"<p>NS=kube-system kubectl config set-context --current --namespace=$NS</p> <pre><code>kubectl create serviceaccount api-explorer\ncat&lt;&lt;EOF &gt; api-explorer.yaml\nkind: ClusterRole\napiVersion: rbac.authorization.k8s.io/v1beta1\nmetadata:\n  name: log-reader\nrules:\n- apiGroups: [\"\"] # \"\" indicates the core API group\n  resources: [\"pods\", \"pods/log\"]\n  verbs: [\"get\", \"watch\", \"list\"]\nEOF\nkubectl apply -f api-explorer.yaml\nkubectl create rolebinding api-explorer:log-reader --clusterrole log-reader --serviceaccount $NS:api-explorer\n\nSERVICE_ACCOUNT=api-explorer\n\n# Get the ServiceAccount's token Secret's name\nSECRET=$(kubectl get serviceaccount ${SERVICE_ACCOUNT} -o json | jq -Mr '.secrets[].name | select(contains(\"token\"))')\n\n# Extract the Bearer token from the Secret and decode\nTOKEN_B64=$(kubectl get secret ${SECRET} -o json | jq -Mr '.data.token')\nTOKEN=$(echo $TOKEN_B64 | base64 -d)\n\n# Extract, decode and write the ca.crt to a temporary location\nkubectl get secret ${SECRET} -o json | jq -Mr '.data[\"ca.crt\"]' | base64 -d &gt; /tmp/ca.crt\n\n# Get the API Server location\nAPISERVER=https://$(kubectl -n default get endpoints kubernetes --no-headers | awk '{ print $2 }')\n\ncurl -s $APISERVER/openapi/v2  --header \"Authorization: Bearer $TOKEN\" --cacert /tmp/ca.crt | less\n\ncurl -s $APISERVER/api/v1/namespaces/default/pods/ --header \"Authorization: Bearer $TOKEN\" --cacert /tmp/ca.crt | jq -rM '.items[].metadata.name'\n\npodName=nginx-5dc7fbd98-hvv6s\ncurl -s $APISERVER/api/v1/namespaces/default/pods/${podName}/log  --header \"Authorization: Bearer $TOKEN\" --cacert /tmp/ca.crt\n</code></pre>"},{"location":"adv-monitoring-with-robusta/","title":"Enhancing Kubernetes Monitoring &amp; Automation with Robusta!","text":"<p>Find a comprehensive setup for Kubernetes monitoring, notifications, troubleshooting, and automation using Robusta. This powerful tool integrates seamlessly with Prometheus, enabling real-time monitoring and instant alerts via Slack and Robusta UI.</p>"},{"location":"adv-monitoring-with-robusta/#key-highlights","title":"Key Highlights:","text":"<ul> <li>Deployed a Kubernetes cluster and Prometheus stack.</li> <li>Integrated Slack for instant alert notifications in the #devops channel.</li> <li>Set up Robusta for advanced monitoring and automated issue resolution.</li> <li>Monitoring has never been easier or more effective. Excited to see how this enhances our DevOps processes! \ud83d\udcbb\ud83d\udd27</li> </ul>"},{"location":"adv-monitoring-with-robusta/#prerequisites","title":"Prerequisites:","text":"<ol> <li>Workspace must have <code>Docker, Helm, Kubectl</code> installed</li> <li>Kubernetes Cluster</li> <li>A Prometheus installation</li> <li>Set up a Sink</li> <li>Create a sink account. I have chosen the slack My Slack and created a channel <code>#devops</code>. more on #Slack-Sink</li> <li>Create a Free Registration at Robust and create an app in it My Account: My Robusta-UI . more on #robusta-ui</li> </ol>"},{"location":"adv-monitoring-with-robusta/#installation","title":"Installation:","text":""},{"location":"adv-monitoring-with-robusta/#lets-create-a-kind-kubernetes-cluster","title":"Let's Create a KIND Kubernetes Cluster","text":"<pre><code> kind create cluster --name=robusta-demo\n</code></pre>"},{"location":"adv-monitoring-with-robusta/#setup-of-prometheus-stack","title":"Setup of Prometheus stack","text":"<pre><code>helm repo add prometheus-community https://prometheus-community.github.io/helm-charts &amp;&amp; helm repo update\n\ncat&lt;&lt;EOF &gt;robusta-demo-values.yaml\nprometheus: # collect rules from all namespaces and ignore label filters\n    ruleNamespaceSelector: {}\n    ruleSelector: {}\n    ruleSelectorNilUsesHelmValues: false\ndefaultRules: # those rules are now managed by Robusta\n    rules:\n      alertmanager: false\n      etcd: false\n      configReloaders: false\n      general: false\n      kubeApiserverSlos: false\n      kubeControllerManager: false\n      kubeProxy: false\n      kubernetesApps: false\n      kubernetesResources: false\n      kubernetesStorage: false\n      kubernetesSystem: false\n      kubeSchedulerAlerting: false\n      kubeStateMetrics: false\n      network: false\n      nodeExporterAlerting: false\n      prometheus: false\n      prometheusOperator: false\nEOF\n\nhelm upgrade --install prometheus prometheus-community/kube-prometheus-stack -f robusta-demo-values.yaml --namespace monitoring --create-namespace\n</code></pre> <p>more</p>"},{"location":"adv-monitoring-with-robusta/#setup-the-sink","title":"Setup the Sink","text":"<pre><code>Generate the Slack Channel\nName: devops\nEmail address: XXX.YYYY@gmail.com\nAccount: naren4biz\n</code></pre> <pre><code>curl -fsSL -o robusta https://docs.robusta.dev/master/_static/robusta\nchmod +x robusta\n./robusta gen-config --no-enable-prometheus-stack --cluster-name=robusta-demo --slack-channel=devops\n</code></pre>"},{"location":"adv-monitoring-with-robusta/#install-robusta-forwarder-runner","title":"Install robusta forwarder &amp; Runner","text":"<pre><code>helm repo add robusta https://robusta-charts.storage.googleapis.com &amp;&amp; helm repo update\nhelm upgrade --install robusta robusta/robusta -f ./generated_values.yaml --set clusterName=robusta-demo --set isSmallCluster=true --set enabledManagedConfiguration=true\n</code></pre>"},{"location":"adv-monitoring-with-robusta/#lets-deploy-a-crashpod","title":"Let's Deploy a crashpod","text":"<pre><code>kubectl apply -f https://gist.githubusercontent.com/robusta-lab/283609047306dc1f05cf59806ade30b6/raw\n</code></pre>"},{"location":"adv-monitoring-with-robusta/#check-the-channel","title":"Check the Channel","text":"<p>My Slack: My Slack </p>"},{"location":"adv-monitoring-with-robusta/#check-the-robusta-ui","title":"Check the Robusta UI","text":"<p>My Account: Robusta-UI </p>"},{"location":"adv-remotewrite-using-vmagent/","title":"Adv remotewrite using vmagent","text":"<p>helm repo add vm https://victoriametrics.github.io/helm-charts/ helm repo update helm show values vm/victoria-metrics-cluster &gt; values.yaml helm install vmcluster vm/victoria-metrics-cluster -f values.yaml </p>"},{"location":"adv-remotewrite-using-vmagent/#certificate-configuration","title":"Certificate Configuration","text":"<pre><code>CA_NAME=\"NKS Certificate Authority\"\nopenssl genrsa -out ca.key 2048\nopenssl req -x509 -new -nodes -key ca.key -subj \"/CN=${CA_NAME}\" -days 10000 -out ca.crt\n</code></pre> <pre><code>mkdir vms\ncd vms\n\n\nCERT_NAME=\"victoria-metrics-server\"\nNODE_IP=127.0.0.1\nDOMAIN=nks.in\n\n\ncat&lt;&lt; EOF &gt;victoria-metrics-server-csr.conf\n[ req ]\ndefault_bits = 2048\nprompt = no\ndefault_md = sha256\nreq_extensions = req_ext\ndistinguished_name = dn\n\n[ dn ]\nC = IN\nST = Karnataka\nL = Bangalore\nO = victoria-metrics\nOU = nks\nCN = $CERT_NAME\n\n[ req_ext ]\nsubjectAltName = @alt_names\n\n[ alt_names ]\nDNS.1 = *.$NODE_IP.nip.io\nDNS.2 = $DOMAIN\nDNS.3 = *.$DOMAIN\nIP.1 = $NODE_IP\n\n\n[ v3_ext ]\nauthorityKeyIdentifier=keyid,issuer:always\nbasicConstraints=CA:FALSE\nkeyUsage=keyEncipherment,dataEncipherment,digitalSignature,nonRepudiation\nextendedKeyUsage=serverAuth,clientAuth\nsubjectAltName=@alt_names\nEOF\n\n\nopenssl genrsa -out victoria-metrics-server-tls.key 2048\nopenssl req -new -key victoria-metrics-server-tls.key -out victoria-metrics-server-tls.csr -config victoria-metrics-server-csr.conf\nopenssl x509 -req -in victoria-metrics-server-tls.csr -CA ../ca.crt -CAkey ../ca.key -CAcreateserial -out victoria-metrics-server-tls.crt -days 10000     -extensions v3_ext -extfile victoria-metrics-server-csr.conf -sha256\n\ncd ..\ncp ca.crt vms\n</code></pre>"},{"location":"adv-remotewrite-using-vmagent/#victoria-metric-agentclient-configuration","title":"Victoria Metric Agent(Client) Configuration","text":"<pre><code>mkdir vma\ncd vma\nCERT_NAME=\"vmagent-client\"\nNODE_IP=127.0.0.1\nDOMAIN=nks.in\n\n\ncat&lt;&lt; EOF &gt;vmagent-client-csr.conf\n[ req ]\ndefault_bits = 2048\nprompt = no\ndefault_md = sha256\nreq_extensions = req_ext\ndistinguished_name = dn\n\n[ dn ]\nC = IN\nST = Karnataka\nL = Bangalore\nO = victoria-metrics\nOU = nks\nCN = $CERT_NAME\n\n[ req_ext ]\nsubjectAltName = @alt_names\n\n[ alt_names ]\nDNS.1 = *.$NODE_IP.nip.io\nDNS.2 = $DOMAIN\nDNS.3 = *.$DOMAIN\nIP.1 = $NODE_IP\n\n\n[ v3_ext ]\nauthorityKeyIdentifier=keyid,issuer:always\nbasicConstraints=CA:FALSE\nkeyUsage=keyEncipherment,dataEncipherment,digitalSignature,nonRepudiation\nextendedKeyUsage=serverAuth,clientAuth\nsubjectAltName=@alt_names\nEOF\n\n\nopenssl genrsa -out vmagent-client-tls.key 2048\nopenssl req -new -key vmagent-client-tls.key -out vmagent-client-tls.csr -config vmagent-client-csr.conf\nopenssl x509 -req -in vmagent-client-tls.csr -CA ../ca.crt -CAkey ../ca.key -CAcreateserial -out vmagent-client-tls.crt -days 10000     -extensions v3_ext -extfile vmagent-client-csr.conf -sha256\ncd ..\ncp ca.crt vma\n</code></pre>"},{"location":"adv-remotewrite-using-vmagent/#install-vmagent-run-vmagentsh","title":"Install vmagent | run-vmagent.sh","text":"<p>VM Agent which will scrape selected time series from local Prometheus server, where <code>service=\"naren\"</code></p> <pre><code>#! /bin/bash\nname=$1\ndefault_value=\"demo\"\nname=${name:-$default_value}\nvmagent_name=${name}-vmagent\nmkdir -p ${PWD}/$vmagent_name\n\nremoteWrite_url=\"http://localhost:8428/api/v1/write\"\n\n\ndocker build -t victoriametrics/vmagent:$vmagent_name ${PWD}/$vmagent_name/\nrm -rf ${PWD}/$vmagent_name/Dockerfile\n\ncat &lt;&lt;EOF &gt; ${PWD}/$vmagent_name/relabel.yml\n- target_label: \"node\"\n  replacement: \"local\"\nEOF\n\ncat &lt;&lt;EOF &gt;${PWD}/$vmagent_name/prometheus.yml\nscrape_configs:\n  - job_name: 'federate'\n    scrape_interval: 15s\n\n    honor_labels: true\n    metrics_path: '/federate'\n\n    params:\n      'match[]':\n        - '{service=\"naren\"}'\n        - '{__name__=~\"up|vm_.*\"}'\n    static_configs:\n      - targets:\n          - localhost:9090\nEOF\n\ndocker volume create vmagentdata\ndocker rm ${vmagent_name} -f\n\ndocker run -d --restart unless-stopped --network host \\\n  --name=${vmagent_name} \\\n  -v ${PWD}/$vmagent_name:/etc/prometheus/ \\\n  -v ${PWD}/vma:/opt/ \\\n  -v vmagentdata:/vmagentdata \\\n  victoriametrics/vmagent -remoteWrite.url=$remoteWrite_url -remoteWrite.urlRelabelConfig=/etc/prometheus/relabel.yml -remoteWrite.forceVMProto -promscrape.config=/etc/prometheus/prometheus.yml -remoteWrite.tlsCAFile=/opt/ca.crt -remoteWrite.tlsCertFile=/opt/vmagent-client-tls.crt -remoteWrite.tlsKeyFile=/opt/vmagent-client-tls.key -remoteWrite.tlsInsecureSkipVerify=false\n\ndocker ps -l\n</code></pre>"},{"location":"adv-remotewrite-using-vmagent/#victoria-metric-server-configuration","title":"Victoria Metric Server Configuration","text":"<pre><code>name=$1\ndefault_value=\"demo\"\nname=${name:-$default_value}\ndocker volume create victoria-metrics-data\n# victoria-metrics\nvictoria_metrics_name=${name}-victoria-metrics\nvictoria_metrics_host_port=8428\ndocker rm ${victoria_metrics_name} -f\ndocker run -d --restart unless-stopped --network host \\\n    --name=${victoria_metrics_name} \\\n    -v victoria-metrics-data:/victoria-metrics-data \\\n    -v ${PWD}/vms:/opt/ \\\n    victoriametrics/victoria-metrics -tls=true -tlsKeyFile=/opt/victoria-metrics-server-tls.key -tlsCertFile=/opt/victoria-metrics-server-tls.crt\n\ndocker ps -l\n</code></pre> <p>cat &lt;\\({PWD}/\\)vmagent_name/Dockerfile FROM victoriametrics/vmagent ENTRYPOINT [\"/vmagent-prod\"] CMD [\"-remoteWrite.url=$remoteWrite_url\" ,\"-remoteWrite.urlRelabelConfig=/etc/prometheus/relabel.yml\", \"-remoteWrite.forceVMProto\",\"-promscrape.config=/etc/prometheus/prometheus.yml\",\"-remoteWrite.tlsCAFile=/opt/ca.crt\",\"-remoteWrite.tlsCertFile=/opt/vmagent-client-tls.crt\",\"-remoteWrite.tlsKeyFile=/opt/vmagent-client-tls.key\",\"-remoteWrite.tlsInsecureSkipVerify=false\"] EOF <p>-tls Whether to enable TLS for incoming HTTP requests at -httpListenAddr (aka https). -tlsCertFile and -tlsKeyFile must be set if -tls is set -tlsCertFile string Path to file with TLS certificate if -tls is set. Prefer ECDSA certs instead of RSA certs as RSA certs are slower. The provided certificate file is automatically re-read every second, so it can be dynamically updated -tlsCipherSuites array Optional list of TLS cipher suites for incoming requests over HTTPS if -tls is set. See the list of supported cipher suites at https://pkg.go.dev/crypto/tls#pkg-constants Supports an array of values separated by comma or specified via multiple flags. -tlsKeyFile string</p> <pre><code>  -remoteWrite.flushInterval duration\n     Interval for flushing the data to remote storage. This option takes effect only when less than 10K data points per second are pushed to -remoteWrite.url (default 1s)\n\n  -remoteWrite.maxBlockSize size\n     The maximum block size to send to remote storage. Bigger blocks may improve performance at the cost of the increased memory usage. See also -remoteWrite.maxRowsPerBlock\n     Supports the following optional suffixes for size values: KB, MB, GB, TB, KiB, MiB, GiB, TiB (default 8388608)\n\n  -remoteWrite.queues int\n     The number of concurrent queues to each -remoteWrite.url. Set more queues if default number of queues isn't enough for sending high volume of collected data to remote storage. Default      value is 2 * numberOfAvailableCPUs (default 32)\n\n  -remoteWrite.rateLimit array\n     Optional rate limit in bytes per second for data sent to the corresponding -remoteWrite.url. By default, the rate limit is disabled. It can be useful for limiting load on remote      storage when big amounts of buffered data is sent after temporary unavailability of the remote storage (default 0)\n     Supports array of values separated by comma or specified via multiple flags.\n\n  -remoteWrite.sendTimeout array\n     Timeout for sending a single block of data to the corresponding -remoteWrite.url (default 1m0s)\n     Supports array of values separated by comma or specified via multiple flags.\n\n  -remoteWrite.tlsCAFile array\n     Optional path to TLS CA file to use for verifying connections to the corresponding -remoteWrite.url. By default, system CA is used\n     Supports an array of values separated by comma or specified via multiple flags.\n  -remoteWrite.tlsCertFile array\n     Optional path to client-side TLS certificate file to use when connecting to the corresponding -remoteWrite.url\n     Supports an array of values separated by comma or specified via multiple flags.\n  -remoteWrite.tlsInsecureSkipVerify array\n     Whether to skip tls verification when connecting to the corresponding -remoteWrite.url\n     Supports array of values separated by comma or specified via multiple flags.\n  -remoteWrite.tlsKeyFile array\n     Optional path to client-side TLS certificate key to use when connecting to the corresponding -remoteWrite.url\n     Supports an array of values separated by comma or specified via multiple flags.\n  -remoteWrite.vmProtoCompressLevel int\n     The compression level for VictoriaMetrics remote write protocol. Higher values reduce network traffic at the cost of higher CPU usage. Negative values reduce CPU usage at the cost of increased network traffic. See https://docs.victoriametrics.com/vmagent.html#victoriametrics-remote-write-protocol\n</code></pre>"},{"location":"ai-llm/","title":"\ud83e\udde0 Naren GPT - Your Personal Local AI Chat Assistant","text":"<p>Naren GPT is a self-hosted, privacy-first conversational assistant that brings ChatGPT-like capabilities directly to your desktop \u2014 no subscriptions, no limits, no cloud dependencies. Powered by Ollama it lets you run powerful LLMs (like LLaMA 3, Qwen, Mistral, etc.) locally with a clean, persistent chat interface.</p> <p></p> <p>\ud83d\udc49\ud83c\udffc https://github.com/naren4b/naren-gpt</p>"},{"location":"ai-llm/#features","title":"\u2728 Features","text":"<ul> <li>\u2705 Runs locally \u2014 your data never leaves your machine</li> <li>\u2705 No per-token or monthly subscription costs</li> <li>\u2705 Works offline \u2014 even without internet</li> <li>\u2705 No rate limits or usage throttling</li> <li>\u2705 Faster, low-latency responses</li> <li>\u2705 Easy model switching with Ollama</li> <li>\u2705 Use arbitrary prompt templates, agents, and pipelines</li> <li>\u2705 Extend with voice, vision, or image generation</li> <li>\u2705 Compatible with CPUs and GPUs \u2014 no special hardware needed</li> </ul>"},{"location":"ai-llm/#getting-started","title":"\ud83c\udfc1 Getting Started","text":""},{"location":"ai-llm/#1-prerequisites","title":"1. Prerequisites","text":"<ul> <li>Python 3.10+</li> <li>Ollama installed <code>curl -fsSL https://ollama.com/install.sh | sh</code></li> <li>At least 8\u201316 GB RAM for optimal performance (depending on model)</li> <li>Install streamlit <code>pip install streamlit</code></li> </ul>"},{"location":"ai-llm/#2-clone-and-run","title":"2. Clone and Run","text":"<pre><code># Server Mode\nollama serve\n# pull the model \nollama pull llama3\n\ncurl http://127.0.0.1:11434/api/tags\ngit clone https://github.com/yourusername/naren-gpt.git\ncd naren-gpt\n# python3 -m pip install --upgrade pip\npip install -r requirements.txt\nbash run.sh\n</code></pre>"},{"location":"ai-llm/#open-the-browser","title":"Open The browser","text":"<p>http://localhost:8501 </p> <p>*It will be slow in local machines </p>"},{"location":"ai-llm/#naren-gpt-a-local-ai-chat-app-using-streamlit-ollama-llms","title":"Naren GPT \u2013 A Local AI Chat App using Streamlit + Ollama + LLMs","text":"<p>Naren GPT is a lightweight, locally hosted AI assistant built with Streamlit and powered by Ollama and open-source LLMs like LLaMA 3. It provides a clean, persistent chat interface \u2014 similar to ChatGPT \u2014 but with full control, privacy, and offline functionality.</p>"},{"location":"ai-llm/#when-to-self-host","title":"When to Self-Host?","text":"<p>Scale: Cost-effective once GPU utilization is high.</p> <p>Performance: Better for specialized workloads (e.g., RAG, embeddings).</p> <p>Privacy/Sovereignty: Legal or regulatory constraints, on-prem, hybrid/multi-cloud.</p>"},{"location":"ai-llm/#features_1","title":"\ud83d\ude80 Features","text":"<ul> <li>\ud83e\udde0 Supports local LLMs (LLaMA 3, Mistral, etc.)</li> <li>\ud83d\udcac Persistent multi-turn chat with conversation memory</li> <li>\ud83c\udfa8 Simple, clean UI built using Streamlit</li> <li>\ud83d\udd12 Runs completely offline \u2013 your data stays with you</li> <li>\u2699\ufe0f Easily customizable and extendable</li> </ul>"},{"location":"argocd-multi-source/","title":"Argocd multi source","text":""},{"location":"argocd-multi-source/#multiple-sources-for-an-argocd-application","title":"Multiple Sources for an ArgoCD Application","text":"<p>ref: Argo CD has the ability to specify multiple sources for a single Application. Argo CD compiles all the sources and reconciles the combined resources.  (argocd-multi-sources)[https://argo-cd.readthedocs.io/en/stable/user-guide/multiple_sources/]</p>"},{"location":"argocd-multi-source/#install-argocd","title":"Install ArgoCD","text":"<pre><code>kubectl create namespace argocd\nkubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml\nkubectl patch deployments.apps -n argocd  argocd-server  \\\n--type=json \\\n-p='[{\"op\": \"add\", \"path\": \"/spec/template/spec/containers/0/args/-\", \"value\": \"--insecure\"}]'\n</code></pre>"},{"location":"argocd-multi-source/#install-argocd-cli","title":"Install argocd CLI","text":"<pre><code>curl -sSL -o argocd-linux-amd64 https://github.com/argoproj/argo-cd/releases/latest/download/argocd-linux-amd64\nsudo install -m 555 argocd-linux-amd64 /usr/local/bin/argocd\nrm argocd-linux-amd64\n</code></pre>"},{"location":"argocd-multi-source/#argocd-cli-login","title":"ArgoCD CLI login","text":"<pre><code>kubectl get secrets -n argocd argocd-initial-admin-secret -o jsonpath=\"{.data.password}\" | base64 -d &amp;&amp; echo \nkubectl port-forward service/argocd-server -n argocd 8080:443 &amp;\nargocd login localhost:8080  --insecure\n</code></pre>"},{"location":"argocd-multi-source/#add-git-repo","title":"Add Git repo","text":"<pre><code>USERNAME=npanda\nTOKEN=hello\n\n# Add Template Repo\nREPO_URL=https://github.com/naren4b/demo-app.git\nargocd repo add ${REPO_URL} --username ${USERNAME} --password ${TOKEN}\n\n# ADD Value Repo\nREPO_URL=https://github.com/naren4b/argocd-multi-source-demo.git\nargocd repo add ${REPO_URL} --username ${USERNAME} --password ${TOKEN}\n</code></pre>"},{"location":"argocd-multi-source/#deploy-the-app","title":"Deploy the App","text":"<pre><code>git clone https://github.com/naren4b/argocd-multi-source-demo.git\nkubectl  apply -f dev-demo-argo-application.yaml -f staging-demo-argo-application.yaml\n</code></pre>"},{"location":"argocd-multi-source/#check-the-manifest-file-match-with-the-values-given","title":"Check the manifest file &amp; match with the values given","text":"<p><pre><code>kubectl get cm -n demo-dev demo -o jsonpath=\"{.data.env}\" &amp;&amp; echo \nkubectl get cm -n demo-staging demo -o jsonpath=\"{.data.env}\" &amp;&amp; echo \n</code></pre> </p>"},{"location":"argocd-multi-source/#to-check-the-templating-is-ok-before-applying","title":"To check the templating is OK before applying","text":"<pre><code>cat &lt;&lt;EOF &gt;my-app.yaml\nproject: default\ndestination:\n  server: 'https://kubernetes.default.svc'\n  namespace: demo-dev\nsources:\n  - repoURL: 'https://github.com/naren4b/demo-app.git'\n    path: helm-chart\n    targetRevision: main\n    helm:\n      valueFiles:\n        - $values/env/dev-values.yaml\n  - repoURL: 'https://github.com/naren4b/argocd-multi-source-demo.git'\n    targetRevision: main\n    ref: values\n\nEOF\n</code></pre>"},{"location":"argocd-multi-source/#test-the-argocd-application","title":"Test the ArgoCD Application","text":"<pre><code>wget https://raw.githubusercontent.com/naren4b/argocd-multi-source-demo/main/decodeArgoApp.py\npython decodeArgoApp.py --app my-app --dry-run=true # for full execution --dry-run=false\n</code></pre>"},{"location":"argocd-multiple-deployment/","title":"Orchestrating Thousands of Kubernetes Clusters through ArgoCD","text":"<p>If your organization has thousands of Kubernetes clusters spread across different geographical regions and you want to deploy and configure your application seamlessly, using a tool like ArgoCD can significantly streamline this process. Here are some key strategies and considerations to achieve seamless deployment and configuration:</p>"},{"location":"argocd-multiple-deployment/#strategies-for-seamless-deployment-and-configuration","title":"Strategies for Seamless Deployment and Configuration","text":""},{"location":"argocd-multiple-deployment/#1-centralized-management-with-argocd","title":"1. Centralized Management with ArgoCD:","text":"<ul> <li>Root ArgoCD Instance: Set up a root ArgoCD instance that acts as the central management hub.</li> <li>Zone ArgoCD Instances: Deploy ArgoCD instances in each geographical region or zone. These zone instances will manage the clusters within their respective regions.</li> </ul>"},{"location":"argocd-multiple-deployment/#2-gitops-approach","title":"2. GitOps Approach:","text":"<ul> <li>Single Source of Truth: Use Git repositories to store the desired state of applications and configurations. This ensures consistency and version control.</li> <li>Automated Sync: ArgoCD will continuously monitor the Git repositories and synchronize the desired state with the actual state of the clusters.</li> </ul>"},{"location":"argocd-multiple-deployment/#3-scalability-and-high-availability","title":"3. Scalability and High Availability:","text":"<ul> <li>Distributed Architecture: Deploy ArgoCD in a highly available and scalable architecture to handle the load of managing thousands of clusters.</li> <li>Replication: Ensure that the ArgoCD instances in each zone are replicated and load-balanced to avoid single points of failure.</li> </ul>"},{"location":"argocd-multiple-deployment/#4-security-and-compliance","title":"4. Security and Compliance:","text":"<ul> <li>Secure Credentials Management: Use Kubernetes secrets and ArgoCD's secret management capabilities to securely store and manage cluster and Git credentials.</li> <li>Access Control: Implement Role-Based Access Control (RBAC) to manage permissions and access across different teams and regions.</li> </ul>"},{"location":"argocd-multiple-deployment/#5-monitoring-and-logging","title":"5. Monitoring and Logging:","text":"<ul> <li>Centralized Monitoring: Use tools like Prometheus and Grafana to monitor the health and performance of ArgoCD and the Kubernetes clusters.</li> <li>Logging: Implement centralized logging to capture and analyze logs from ArgoCD and the managed applications.</li> </ul>"},{"location":"argocd-multiple-deployment/#6-automated-rollbacks-and-self-healing","title":"6. Automated Rollbacks and Self-Healing:","text":"<ul> <li>Rollback on Failure: Configure ArgoCD to automatically roll back changes if a deployment fails, ensuring system stability.</li> <li>Self-Healing: Enable self-healing policies in ArgoCD to automatically reconcile any drift from the desired state.</li> </ul>"},{"location":"argocd-multiple-deployment/#considerations-for-implementation","title":"Considerations for Implementation","text":"<ul> <li>Network Latency: Consider the network latency between the root ArgoCD instance and the zone instances. Use a content delivery network (CDN) or other network optimization techniques to minimize latency.</li> <li>Data Residency and Compliance: Ensure that your deployment strategy complies with data residency and compliance requirements specific to each geographical region.</li> <li>Scalability: Plan for scalability by testing the performance of ArgoCD with a large number of clusters and applications. Adjust the resource allocation as needed.</li> <li>Disaster Recovery: Implement disaster recovery plans to ensure that you can quickly recover from failures or outages in any region.</li> <li>Documentation and Training: Provide comprehensive documentation and training for your teams to ensure they understand how to use ArgoCD effectively.</li> </ul> <p>I have given a try to solve this problem </p>"},{"location":"argocd-multiple-deployment/#setting-up-root-argocd","title":"Setting Up Root ArgoCD","text":""},{"location":"argocd-multiple-deployment/#1-install-root-argocd-via-helm-install","title":"1. Install Root ArgoCD via Helm install","text":"<p>ref: Install Argocd to standalon kubernetes cluster <pre><code>curl -sO https://gist.githubusercontent.com/naren4b/ac834254f2d348d7b5e91ebc32fcba6e/raw/3a35d8d083203d7203f58c286398b6cd3a656b7d/install-argocd.sh\nbash install-argocd.sh\n</code></pre></p>"},{"location":"argocd-multiple-deployment/#2-add-git-repo-credentials-declarativelyoptional","title":"2. Add git repo credentials - declaratively(Optional)","text":"<p>Steps: - Export Git token - Run the script to create credentials - Reference: Repository credentials Script, for using the same credentials in multiple repositories. Example: <pre><code># export MY_GIT_TOKEN={token}\ncurl -sO https://gist.githubusercontent.com/naren4b/fae65efb90998cb46a3c9ebed16df880/raw/443682b34a4a5bc6a212cca93cd41e32873f2eb2/create-https-repo-creds-secret.sh\n# vi create-https-repo-creds-secret.sh\nbash create-https-repo-creds-secret.sh\n</code></pre></p>"},{"location":"argocd-multiple-deployment/#3-add-cluster-credentials-declarativelyotional","title":"3. Add cluster credentials - declaratively(Otional)","text":"<p>In Argo CD, managed clusters are stored within Secrets in the Argo CD namespace. The ApplicationSet controller uses those same Secrets to generate parameters to identify and target available clusters. For each cluster registered with Argo CD, the Cluster generator produces parameters based on the list of items found within the cluster secret. It automatically provides the following parameter values to the Application template for each cluster</p> <p>more: https://argo-cd.readthedocs.io/en/stable/operator-manual/applicationset/Generators-Cluster/</p> <p>Steps: - Download and run the script to create cluster secrets - Reference: Register A Cluster  <pre><code>curl -sO https://gist.githubusercontent.com/naren4b/4af945b244f60d801ca77227cdeda861/raw/c83902c8b9644f225764d2b4890ef9b8d917470d/create-cluster-secret.sh\nbash create-cluster-secret.sh \n</code></pre></p>"},{"location":"argocd-multiple-deployment/#4-create-argocd-application-deploy","title":"4. Create Argocd Application Deploy","text":"<p>Create s seeding ArgoCD Application to trigger everything </p> <pre><code>cat&lt;&lt;EOF | kubectl apply -f -\napiVersion: argoproj.io/v1alpha1\nkind: Application\nmetadata:\n  name: seed-application\n  namespace: argocd\nspec:\n  project: default\n  source:\n    repoURL: https://github.com/naren4b/argo-cd.git\n    targetRevision: HEAD\n    path: charts/central-argocd\n  destination:\n    server: https://kubernetes.default.svc\n    namespace: argocd\n  syncPolicy:\n    automated:\n      allowEmpty: true\n      selfHeal: true\nEOF\n</code></pre>"},{"location":"argocd-multiple-deployment/#setting-up-zone-argocdin-a-region","title":"Setting Up Zone ArgoCD(in a Region)","text":"<p>Zone ArgoCD value file details at your Private value file repo  Key Points: - Cluster Settings  - Git repository settings - Example Configuration zone argocd Values</p>"},{"location":"argocd-multiple-deployment/#to-check-zone-argocd","title":"To Check zone Argocd","text":"<pre><code>kubectl -n in-cluster-zone-argocd get secret argocd-initial-admin-secret -o jsonpath=\"{.data.password}\" | base64 -d; echo\nnohup kubectl port-forward -n in-cluster-zone-argocd svc/in-cluster-zone-argocd-server 5000:80 --address 0.0.0.0 &amp;\n</code></pre>"},{"location":"argocd-multiple-deployment/#conclusion","title":"Conclusion","text":""},{"location":"argocd-multiple-deployment/#summary","title":"Summary:","text":"<ul> <li>Benefits of using ArgoCD for managing CD</li> <li>Scalability across multiple clusters</li> <li>Automation and self-healing capabilities</li> </ul>"},{"location":"argocd-multiple-deployment/#next-steps","title":"Next Steps:","text":"<ul> <li>Implementation roadmap</li> <li>Future improvements and enhancements</li> </ul>"},{"location":"argocd-oidc-setup/","title":"Setting up ArgoCD with OIDC login in development environment (insecure )","text":"<p>This functionality is clearly explained in the ArgoCD documentation, but there are still a few aspects that have been overlooked, potentially causing issues when applied in a development environment. For someone who is new or inexperienced, resolving these matters might prove to be a challenging task.</p> <p>Refer #azure-ad-app-registration-auth-using-oidc for detailed step.</p>"},{"location":"argocd-oidc-setup/#collect-these-informations-after-azure-app-is-created-note_","title":"Collect these informations after Azure App is created [^note]_","text":"<pre><code>client_id=1a7f5g81-6b25-1982-94e6-111aaabbb\ntenant_id=5x471345-9p75-428d-9z9b-70f44f8630b0\nclient_secret=YYYYYY~y.-qqo43539TlreSV.f0gR4gth4cXXXXXXX\nobject_id=12121212-4368-40ed-b07a-4e4e4r4r5r5r5\nurl=argocd.example.naren4biz.in\n</code></pre>"},{"location":"argocd-oidc-setup/#install-argocd-in-your-cluster","title":"Install argocd in your cluster","text":"<p>ref: Setting up ArgoCD in k8s cluster with local User &amp; RBAC</p>"},{"location":"argocd-oidc-setup/#update-the-argocd-config","title":"Update the argocd config","text":"<pre><code>cat&lt;&lt;EOF &gt; argocd-cm-oidc-patch.yaml\ndata:\n    oidc.tls.insecure.skip.verify: \"true\" # If you have inscure setup\n    policy.default: role:readonly\n    url: https://$url\n    oidc.config: |\n             name: Azure\n             issuer: https://login.microsoftonline.com/${tenant_id}/v2.0\n             clientID: ${client_id}\n             clientSecret: \\$oidc.azure.clientSecret\n             requestedIDTokenClaims:\n                groups:\n                   essential: true\n             requestedScopes:\n                - openid\n                - profile\n                - email\nEOF\n\nkubectl patch cm -n argocd argocd-cm --patch-file argocd-cm-oidc-patch.yaml\n</code></pre>"},{"location":"argocd-oidc-setup/#add-client-secret-to-the-argocd-secret","title":"Add Client Secret to the ArgoCD secret","text":"<pre><code>client_secret=$(echo -n $client_secret | base64)\ncat&lt;&lt;EOF &gt;argocd-secret-oidc.yaml\ndata:\n oidc.azure.clientSecret: ${client_secret}\nEOF\nkubectl patch secret -n argocd argocd-secret --patch-file argocd-secret-oidc.yaml\n</code></pre>"},{"location":"argocd-oidc-setup/#add-the-argocd-rbac","title":"Add the ArgoCD RBAC","text":"<pre><code>object_id==12121212-4368-40ed-b07a-4e4e4r4r5r5r5\ncat&lt;&lt; EOF &gt; argocd-rbac-cm-patch.yaml\ndata:\n  policy.csv: |\n    g, $object_id, role:admin\n  policy.default: role:readonly\nEOF\n\nkubectl patch cm -n argocd argocd-rbac-cm --patch-file argocd-rbac-cm-patch.yaml\n</code></pre>"},{"location":"argocd-oidc-setup/#thats-it-check-your-page-argocdexamplenaren4bizin","title":"That's it check your page argocd.example.naren4biz.in","text":""},{"location":"argocd-oidc-setup/#argocd-oidc-k8s-local-rookie-learning-weekend-beginners-secrets-devopsengineer-devops-sre-cicd-gitops","title":"argocd #OIDC #k8s #local #rookie #learning #weekend #beginners #secrets #devopsengineer #devops #sre #cicd #gitops","text":"<p>[1]Collect configuration from Azure App </p>"},{"location":"argocd-rbac/","title":"Setting up ArgoCD in k8s cluster with local User &amp; RBAC","text":""},{"location":"argocd-rbac/#install-the-argocd","title":"Install the argocd","text":"<pre><code>kubectl create namespace argocd\nkubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml\nkubectl patch deployments.apps -n argocd  argocd-server  \\\n--type=json \\\n-p='[{\"op\": \"add\", \"path\": \"/spec/template/spec/containers/0/args/-\", \"value\": \"--insecure\"}]'\n</code></pre>"},{"location":"argocd-rbac/#install-the-argocd-cli","title":"Install the Argocd CLI","text":"<pre><code>curl -sSL -o argocd-linux-amd64 https://github.com/argoproj/argo-cd/releases/latest/download/argocd-linux-amd64\nsudo install -m 555 argocd-linux-amd64 /usr/local/bin/argocd\nrm argocd-linux-amd64\n</code></pre>"},{"location":"argocd-rbac/#access-the-argocd","title":"Access the argocd","text":"<pre><code>kubectl port-forward svc/argocd-server -n argocd 8080:443\nkubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath=\"{.data.password}\" | base64 -d; echo\nargocd login localhost:8080  --insecure\nargocd account update-password\n</code></pre>"},{"location":"argocd-rbac/#set-up-default-policyrole","title":"Set up default policy,role","text":"<pre><code>kubectl patch -n argocd cm argocd-cm --patch='{\"data\":{\"policy.default\": \"role:readonly\" }}'\nkubectl patch -n argocd cm argocd-rbac-cm --patch='{\"data\":{\"policy.default\": \"role:readonly\" }}'\n</code></pre>"},{"location":"argocd-rbac/#create-a-local-user","title":"Create a local user","text":"<pre><code>username=naren\nnewpassword=newpassword\ncurentpassword=curentpassword\n\n\nkubectl patch -n argocd cm argocd-cm --patch='{\"data\":{\"accounts.'${username}'\": \"apikey,login\" }}'\nargocd account list\nargocd account update-password  --current-password=${curentpassword} --new-password=${newpassword} --account=${username}\n</code></pre> <p>ws: https://killercoda.com/kubernetes/scenario/a-playground</p>"},{"location":"argocd-rbac/#create-new-group","title":"Create new group","text":"<pre><code>cat&lt;&lt; EOF &gt; argocd-patch.yaml\ndata:\n  policy.csv: |\n    p, role:org-admin, applications, get, */*, allow\n    p, role:org-admin, applications, sync, */*, allow\n    p, role:org-admin, logs, get, *, allow\n\n    g, naren, role:admin\n  policy.default: role:readonly\nEOF\n\nkubectl patch cm -n argocd argocd-rbac-cm --patch-file argocd-patch.yaml\n</code></pre>"},{"location":"argocd-rbac/#deploy-an-application","title":"Deploy an application","text":"<pre><code>curl -s https://raw.githubusercontent.com/naren4b/argocd-multi-source-demo/main/sample-app.sh | bash\n</code></pre> <p>source : https://github.com/naren4b/nks/edit/main/docs/argocd-rbac.md</p>"},{"location":"basic-harbor-registry/","title":"Setting up Basic Harbor Registry in a Kubernetes Cluster","text":""},{"location":"basic-harbor-registry/#lets-have-kubernetes-cluster","title":"Let's have kubernetes cluster","text":"<pre><code>curl -s https://raw.githubusercontent.com/naren4b/dotfiles/main/ws/install.sh | bash\n</code></pre>"},{"location":"basic-harbor-registry/#setup-harbor-helm-chart","title":"Setup Harbor Helm-Chart","text":"<pre><code>curl -s https://raw.githubusercontent.com/naren4b/harbor-registry/main/setup.sh | bash\n</code></pre>"},{"location":"basic-harbor-registry/#install-ingress-controller","title":"Install Ingress controller","text":"<pre><code>curl -s  https://raw.githubusercontent.com/kubernetes/ingress-nginx/main/deploy/static/provider/kind/deploy.yaml \nkubectl label nodes controlplane ingress-ready=\"true\"\nkubectl apply -f deploy.yaml \nkubectl wait --for=condition=ready pod -n ingress-nginx -l app.kubernetes.io/component=controller\n</code></pre>"},{"location":"basic-harbor-registry/#install-harbor","title":"Install harbor","text":"<pre><code>curl -O https://raw.githubusercontent.com/naren4b/harbor-registry/main/harbor-values.yaml #change the values if you want \nHARBOR_URL=registry.127.0.0.1.nip.io\ncurl https://raw.githubusercontent.com/naren4b/harbor-registry/main/install.sh | bash\nkubectl wait --for=condition=ready pod -n registry -l app=harbor -l component=portal\n</code></pre>"},{"location":"basic-harbor-registry/#setup-client","title":"Setup Client","text":"<pre><code>URL=$HARBOR_URL\nCERT_PATH=/etc/docker/certs.d/${URL}\nsudo mkdir -p $CERT_PATH\nsudo openssl s_client -connect ${URL}:443 -showcerts &lt;/dev/null | sed -n -e '/-.BEGIN/,/-.END/ p' &gt; /etc/docker/certs.d/${URL}/ca.crt\nsudo systemctl restart docker\n# for any local URL \necho 127.0.0.1 $URL &gt;&gt; /etc/hosts \n</code></pre>"},{"location":"basic-harbor-registry/#load-an-image-to-registry","title":"Load an image to registry","text":"<pre><code>docker login registry.127.0.0.1.nip.io -u admin -p Harbor12345\ndocker pull nginx:latest\ndocker tag nginx:latest $HARBOR_URL/library/nginx:latest\ndocker push $HARBOR_URL/library/nginx:latest\n</code></pre>"},{"location":"basic-harbor-registry/#_1","title":"Setting up Basic Harbor Registry in a Kubernetes Cluster","text":"<p>NEXT: Let's try basic Harbor API</p> <p>Ref:  - Demo Environment</p>"},{"location":"bcp/","title":"RPO, RTO &amp; MTD in Business Continuity &amp; Disaster Recovery Plan(BCP &amp; DRS) \ud83c\udf31\ud83d\udca5\u2705\u270c","text":""},{"location":"bcp/#rpo-recovery-point-objective","title":"RPO: Recovery Point Objective","text":"<ul> <li>Recovery point objectives are about data loss tolerance. </li> <li>RPO is the term used in business continuity to identify the maximum targeted period in which data can be lost without severely impacting the recovery of operations.For example, if a business process could not lose more than one day's worth of data, then the RPO for that information would be 24 hours. </li> <li>RPO is very useful to help determine the frequency of backups for a given system.</li> </ul>"},{"location":"bcp/#rto-recovery-time-objective","title":"RTO: Recovery Time Objective","text":"<ul> <li>Recovery time objectives are about restoration goals.</li> <li>RTO is a term used in business continuity to identify the planned recovery time for a process or system which should occur before reaching the business process's maximum tolerable downtime. For example, if a business process could not sustain for more than one day without normal operations, then the first RTO should be less than 24 hours.</li> <li>RTOs can be helpful in determining what kind of recovery and/or redundancy may be required.</li> </ul>"},{"location":"bcp/#mtd-maximum-tolerable-downtime","title":"MTD: Maximum Tolerable Downtime","text":"<ul> <li>Maximum tolerable downtime, also sometimes referred to as Maximum Allowable Downtime (MAD), represents the total amount of downtime that can occur without causing significant harm to the organization's mission.</li> <li>MTD is important to define so continuity planners can select and implement appropriate recovery methods and procedures to ensure downtime does not exceed acceptable levels.</li> </ul>"},{"location":"chroma/","title":"Another Vector Store \"Chroma\"","text":"<p>Choosing the right vector database is a critical decision for any AI-powered application. Chroma is the open-source search and retrieval database for AI applications.</p> <ul> <li>It is open-source, highly customizable vector database.</li> <li>Chroma stands out for being free, open-source, and flexible. It\u2019s perfect for developers or small teams who want complete control over their infrastructure and are willing to handle their own scaling and maintenance.</li> </ul>"},{"location":"chroma/#1-what-it-offers","title":"1. What it Offers?","text":"<ul> <li>Store embeddings and their metadata</li> <li>Vector search</li> <li>Full-text search</li> <li>Document storage</li> <li>Metadata filtering</li> <li>Multi-modal retrieval</li> </ul>"},{"location":"chroma/#2-different-deployment-model","title":"2. Different Deployment model?","text":"<ul> <li>Local: as an embedded library - great for prototyping and experimentation.</li> <li>Single Node: as a single-node server - great for small to medium scale workloads of &lt; 10M records in a handful of collections.</li> <li>Distributed: as a scalable distributed system - great for large scale production workloads, supporting millions of collections.</li> <li>Chroma Cloud: https://www.trychroma.com/ is a managed offering of distributed Chroma.</li> </ul>"},{"location":"chroma/#3-the-architecture","title":"3. The Architecture","text":"<ul> <li>a. The Gateway: The entrypoint for all client traffic. (Handles authentication, rate-limiting, quota management, and request validation)</li> <li>b. The Logs: Chroma\u2019s write-ahead log ensures atomicity across multi-record writes. It is durability and replay in distributed deployments.</li> <li>c. The Query Executor: Vector similarity, full-text and metadata search. Maintains a combination of in-memory and on-disk indexes, and coordinates with the Log to serve consistent results.</li> <li>d. The Compactor: Reads from the Log and builds updated vector / full-text / metadata indexes. Writes materialized index data to shared storage. Updates the System Database with metadata about new index versions.</li> <li>e. The System Database: Chroma\u2019s internal catalog. Tracks tenants, collections, and their metadata. Backed by a SQL database.</li> <li>f. Storage &amp; Runtime: Deployment mode decides how they use storage and the runtime they operate in.</li> <li>In Local and Single Node mode, all components share a process and use the local filesystem for durability.</li> <li>In Distributed mode, components are deployed as independent services: - The log and built indexes are stored in cloud object storage. - The system catalog is backed by a SQL database. - All services use local SSDs as caches to reduce object storage latency and cost.</li> </ul>"},{"location":"chroma/#4-operation-flow-write","title":"4. Operation flow: Write","text":"<pre><code>sequenceDiagram\n    participant C as Client\n    participant G as Gateway\n    participant W as WAL Logs\n    participant X as Compactor\n    participant S as Storage\n    participant D as System DB\n\n    C-&gt;&gt;G: Request\n    G-&gt;&gt;G: Authenticate, Quota Check, Rate Limit, Transform to Ops Log\n    G-&gt;&gt;W: Forward Ops Log (Persist)\n    W--&gt;&gt;G: Ack Persistence\n    G--&gt;&gt;C: Ack Write\n\n    X-&gt;&gt;W: Pull Logs Periodically\n    X-&gt;&gt;X: Build Optimized Indexes (vector, full-text, metadata)\n    X-&gt;&gt;S: Write New Index Versions\n    X-&gt;&gt;D: Register Index in System DB\n</code></pre> <ol> <li>Request arrives at the gateway, where it is authenticated, checked against quota limits, rate limited and then transformed into a log of operations.</li> <li>The log of operations is forwarded to the write-ahead-log for persistence.</li> <li>After being persisted by the write-ahead-log, the gateway acknowledges the write.</li> <li>The compactor periodically pulls from the write-ahead-log and builds new index versions from the accumulated writes. These indexes are optimized for read performance and include vector, full-text, and metadata indexes.</li> <li>Once new index versions are built, they are written to storage and registered in the system database.</li> </ol>"},{"location":"chroma/#5-operation-flow-read","title":"5. Operation flow: Read","text":"<pre><code>sequenceDiagram\n    participant C as Client\n    participant G as Gateway\n    participant Q as Query Executor\n    participant S as Storage\n\n    C-&gt;&gt;G: Query Request\n    G-&gt;&gt;G: Authenticate, Quota Check, Rate Limit, Transform to Logical Plan\n    G-&gt;&gt;Q: Route Logical Plan (via Rendezvous Hash on Collection ID)\n    Q-&gt;&gt;Q: Transform Logical \u2192 Physical Plan\n    Q-&gt;&gt;S: Read from Storage Layer\n    Q-&gt;&gt;Q: Pull from WAL Logs for Consistent Read\n    S--&gt;&gt;Q: Retrieved Vector + Data\n    Q--&gt;&gt;G: Query Results\n    G--&gt;&gt;C: Response (IDs + Docs + Metadata)\n</code></pre> <ol> <li>Request arrives at the gateway, where it is authenticated, checked against quota limits, rate limited and transformed into a logical plan.</li> <li>This logical plan is routed to the relevant query executor. In distributed Chroma, a rendezvous hash on the collection id is used to route the query to the correct nodes and provide cache coherence.</li> <li>The query executor transforms the logical plan into a physical plan for execution, reads from its storage layer, and performs the query. The query executor pulls data from the log to ensure a consistent read.</li> <li>The request is returned to the gateway and subsequently to the client.</li> <li>By default, <code>.query</code> and <code>.get</code> always return the documents and metadatas. You can use the include argument to modify what gets returned. IDs are always returned.</li> </ol>"},{"location":"chroma/#6-example-code","title":"6. Example Code","text":"<pre><code>import chromadb\nchroma_client = chromadb.Client()\n\n# switch `create_collection` to `get_or_create_collection` to avoid creating a new collection every time\ncollection = chroma_client.get_or_create_collection(name=\"my_collection\")\n\n# switch `add` to `upsert` to avoid adding the same documents every time\ncollection.upsert(\n    documents=[\n        \"This is a document about pineapple\",\n        \"This is a document about oranges\"\n    ],\n    ids=[\"id1\", \"id2\"]\n)\n\nresults = collection.query(\n    query_texts=[\"This is a query document about florida\"], # Chroma will embed this for you\n    n_results=2 # how many results to return\n)\n\nprint(results)\n</code></pre>"},{"location":"cosign-syft-grype-kevyrno/","title":"\ud83d\udd10 Enhancing Software Supply Chain Security with Syft and Grype \ud83d\udee0\ufe0f","text":"<p>As software producers, we rely on various third-party components and technologies to build and distribute software. Ensuring the security of our entire supply chain has become more critical than ever. That\u2019s where tools like Syft and Grype come into play!</p> <p> A Guide to Generating SBOM with Syft and Grype</p>"},{"location":"cosign-syft-grype-kevyrno/#in-this-guide-i-walk-you-through","title":"In this guide, I walk you through:","text":"<ul> <li>1\ufe0f\u20e3 Generating SBOMs (Software Bill of Materials) with Syft</li> <li>2\ufe0f\u20e3 Scanning for vulnerabilities with Grype</li> <li>3\ufe0f\u20e3 Signing images with Cosign for integrity validation</li> <li>4\ufe0f\u20e3 Combining SBOM and vulnerability data for a comprehensive view of your software\u2019s security posture.</li> </ul>"},{"location":"cosign-syft-grype-kevyrno/#key-takeaways","title":"\ud83d\udccc Key takeaways:","text":"<ul> <li>Understand which components are vulnerable</li> <li>Prioritize remediation efforts based on vulnerability severity</li> <li>Ensure compliance with security regulations</li> <li>Continuously enhance your software\u2019s security</li> </ul>"},{"location":"cosign-syft-grype-kevyrno/#devsecops-cloudsecurity-softwaresupplychain-sbom-syft-grype-cosign-containers-kubernetes-security-devops-sbomgeneration","title":"DevSecOps #CloudSecurity #SoftwareSupplyChain #SBOM #Syft #Grype #Cosign #Containers #Kubernetes #Security #DevOps #SBOMGeneration","text":"<p>\ud83d\udca1 Ready to boost your software supply chain security? Check out the detailed guide here:</p>"},{"location":"cosign-syft-grype-kevyrno/#install-cosign","title":"Install cosign:","text":"<pre><code>wget \"https://github.com/sigstore/cosign/releases/download/v2.4.0/cosign-linux-amd64\" \nsudo mv cosign-linux-amd64 /usr/local/bin/cosign \nsudo chmod +x /usr/local/bin/cosign\n</code></pre>"},{"location":"cosign-syft-grype-kevyrno/#install-syft","title":"Install Syft:","text":"<pre><code>curl -sSfL https://raw.githubusercontent.com/anchore/syft/main/install.sh | sh -s -- -b /usr/local/bin\n</code></pre>"},{"location":"cosign-syft-grype-kevyrno/#install-grype","title":"Install Grype:","text":"<pre><code>curl -sSfL https://raw.githubusercontent.com/anchore/grype/main/install.sh | sh -s -- -b /usr/local/bin\n</code></pre>"},{"location":"cosign-syft-grype-kevyrno/#setup-your-info","title":"Setup your info","text":"<p><pre><code>export DOCKER_PASSWORD=&lt;password&gt;\nexport DOCKER_USERNAME=&lt;username&gt;\nexport IMAGE_NAME=hello-container\nexport IMAGE_TAG=latest\n</code></pre> </p>"},{"location":"cosign-syft-grype-kevyrno/#build-the-docker-image","title":"Build the docker image","text":"<pre><code>cat&lt;&lt;EOF &gt;Dockerfile\nFROM nginx:1.27.1\nMAINTAINER Narendranath Panda &lt;naren4biz@gmail.com&gt;\nCOPY static-html-directory /usr/share/nginx/html\nEOF\n\n#build\ndocker build --no-cache linux/amd64 -t $DOCKER_USERNAME/$IMAGE_NAME:$IMAGE_TAG .\n\n# test\ndocker run --name=$IMAGE_NAME --rm -d --network host  $DOCKER_USERNAME/$IMAGE_NAME:$IMAGE_TAG\ncurl http://localhost:80\n\n# push \ndocker push $DOCKER_USERNAME/$IMAGE_NAME:$IMAGE_TAG\n</code></pre>"},{"location":"cosign-syft-grype-kevyrno/#generate-and-the-cosign-signing-key-and-sign-the-image","title":"Generate and the cosign signing key and sign the image","text":"<pre><code>cosign generate-key-pair\n#cosign sign --key cosign.key docker-username/demo-container\ncosign sign --key cosign.key $DOCKER_USERNAME/$IMAGE_NAME:$IMAGE_TAG\n\n#cosign verify --key cosign.pub docker-username/demo-container\ncosign verify --key cosign.pub $DOCKER_USERNAME/$IMAGE_NAME:$IMAGE_TAG\n</code></pre>"},{"location":"cosign-syft-grype-kevyrno/#generate-the-sbom","title":"Generate the SBOM","text":"<pre><code>#If you prefer to receive the output in JSON format, run the following command:\u00a0\nsyft $DOCKER_USERNAME/$IMAGE_NAME:$IMAGE_TAG -o json &gt; $IMAGE_NAME-$IMAGE_TAG-sbom.json\n\n#To extract the package name, version, and license in a tabular format - enabling visibility into all packages and licenses used to create the product, run the following command:\n\ncat $IMAGE_NAME-$IMAGE_TAG.json |  jq -r '.artifacts[] | [.name, .version, (if .licenses == [] then \"No license\" else [ .licenses[].value ] | join(\", \") end)] | @tsv' | awk -F'\\t' 'BEGIN {print \"Package Name\\tPackage Version\\tLicense\"; print \"-------------\\t---------------\\t-------\"} {printf \"%s\\t%s\\t%s\\n\", $1, $2, $3}'\n\n#To extract the package name, version, and license in a tabular format - enabling visibility into all packages and licenses used to create the product, run the following command:\n\ncat $IMAGE_NAME-$IMAGE_TAG.json | jq '[.artifacts[] | {packageName: .name, packageVersion: .version, license: (if .licenses | length == 0 then \"No license\" else [ .licenses[].value ] | join(\", \") end), locations: [.locations[].path]}]' &gt; packages_with_locations.json\n\n#This may create separate sections for the same Package version and name but with different locations. The following command will unify these sections into one.\u00a0\ncat $IMAGE_NAME-$IMAGE_TAG.json | jq '[.artifacts[] | {packageName: .name, packageVersion: .version, license: (if .licenses | length == 0 then \"No license\" else [ .licenses[].value ] | join(\", \") end), locations: [.locations[].path]}] | group_by(.packageName + .packageVersion) | map({packageName: .[0].packageName, packageVersion: .[0].packageVersion, license: .[0].license, locations: map(.locations[]) | unique})' &gt; grouped_packages_with_locations.json\n\ncat $IMAGE_NAME-$IMAGE_TAG.json | jq -r '[.artifacts[] | {packageName: .name, packageVersion: .version, license: (if .licenses | length == 0 then \"No license\" else [ .licenses[].value ] | join(\", \") end), locations: [.locations[].path]}] | group_by(.packageName + .packageVersion) | map({packageName: .[0].packageName, packageVersion: .[0].packageVersion, license: .[0].license, locations: map(.locations[]) | unique}) | .[] | [.packageName, .packageVersion, .license, (.locations | join(\", \"))] | @tsv' | column -t -s $'\\t' &gt; grouped_packages_with_locations_tabular.txt\n\n# You can filter by GPL, to focus on packages that typically violate the organization\u2019s policy by running the following command:\ncat $IMAGE_NAME-$IMAGE_TAG.json | jq -r '[.artifacts[] | {packageName: .name, packageVersion: .version, license: (if .licenses | length == 0 then \"No license\" else [ .licenses[].value ] | join(\", \") end), locations: [.locations[].path]}] | group_by(.packageName + .packageVersion) | map({packageName: .[0].packageName, packageVersion: .[0].packageVersion, license: .[0].license, locations: map(.locations[]) | unique}) | map(select(.license | test(\"GPL\"))) | .[] | [.packageName, .packageVersion, .license, (.locations | join(\", \"))] | @tsv' | column -t -s $'\\t'&gt; gpl_filtered.json\n</code></pre>"},{"location":"cosign-syft-grype-kevyrno/#generate-the-vulnerability","title":"Generate the Vulnerability","text":"<pre><code>#Once you have extracted your SBOM in your preferred format with Syft, Grype will scan the SBOMs to identify and report vulnerabilities, enriching the SBOM with crucial security insights.\ngrype sbom:sbom.json -o json &gt; $IMAGE_NAME-$IMAGE_TAG-vulnerabilities.json\n</code></pre>"},{"location":"cosign-syft-grype-kevyrno/#combining-sbom-and-vulnerability-data","title":"Combining SBOM and Vulnerability Data","text":"<p>To effectively safeguard your software supply chain, it\u2019s essential to combine insights from SBOMs (Software Bill of Materials) with vulnerability data. This comprehensive approach allows you to better understand the current risk exposure of your technology stack.</p> <p>Using tools like Syft for generating SBOMs and Grype for vulnerability scanning, we can:  - \u2705 Understand Exposure: Identify vulnerable components and assess their severity.  - \u2705 Prioritize Efforts: Focus on remediating the most critical vulnerabilities.  - \u2705 Ensure Compliance: Keep a clear record of components and vulnerabilities to meet regulatory standards.  - \u2705 Enhance Security Posture: Continuously improve security by regularly scanning and remediating threats. try the script here add_vulnerabilities_to_packages.py</p>"},{"location":"cosign-syft-grype-kevyrno/#full-credit-to-a-guide-to-generating-sbom-with-syft-and-grype","title":"Full credit to \ud83d\ude4f a-guide-to-generating-sbom-with-syft-and-grype \ud83d\ude4f","text":"<pre><code>    # Define file paths\n    grouped_packages_path = '/path/to/grouped_packages_with_locations.json' # todo $IMAGE_NAME-$IMAGE_TAG-sbom.json\n    vulnerabilities_path = '/path/to/vulnerabilities.json' #$IMAGE_NAME-$IMAGE_TAG-vulnerabilities.json\n    output_path = '/path/to/final.json'  # todo $IMAGE_NAME-$IMAGE_TAG.json\n</code></pre>"},{"location":"cosign-syft-grype-kevyrno/#sign-the-sbom","title":"Sign the SBOM","text":"<pre><code>SHA_ID=$(docker inspect --format='{{.Id}}' $DOCKER_USERNAME/$IMAGE_NAME:$IMAGE_TAG)\ncosign sign --key cosign.key $DOCKER_USERNAME/$IMAGE_NAME:$SHA_ID.sbom\n</code></pre>"},{"location":"cosign-syft-grype-kevyrno/#what-next","title":"What Next","text":""},{"location":"cosign-syft-grype-kevyrno/#setup-the-kevyrno-policy","title":"Setup the Kevyrno Policy","text":""},{"location":"cosign-syft-grype-kevyrno/#test-at-a-k8s-cluster","title":"Test at a k8s cluster","text":""},{"location":"cosign-syft-grype-kevyrno/#ref","title":"Ref:","text":"<ul> <li>https://www.jit.io/resources/appsec-tools/a-guide-to-generating-sbom-with-syft-and-grype</li> <li>https://edu.chainguard.dev/open-source/sigstore/policy-controller/how-to-install-policy-controller/</li> <li>https://www.docker.com/blog/generate-sboms-with-buildkit/</li> <li>https://anchore.com/blog/add-sbom-generation-to-your-github-project-with-syft/</li> <li>https://www.youtube.com/watch?v=8GKFzJaEHac&amp;ab_channel=VMwareTanzu</li> <li>https://www.youtube.com/watch?v=nybVFJVXbww&amp;ab_channel=Computerphile</li> <li>https://edu.chainguard.dev/open-source/sigstore/policy-controller/how-to-install-policy-controller/</li> </ul>"},{"location":"cow_video_summary_bot/","title":"YouTube to Google Sheets Summarizer Bot","text":"<p>This project automates the process of watching a YouTube video, using Google's Gemini AI to extract structured information about specific topics (like stock charts), and populating a Google Sheet with the results.</p> <p>The project is broken into three main, modular scripts orchestrated by a main bot script.</p> <p>Complete code : https://github.com/naren4b/ai-journey/tree/main/cow_video_summary_bot </p>"},{"location":"cow_video_summary_bot/#workflow-diagram","title":"Workflow Diagram","text":"<p>The following diagram illustrates the complete data flow from the user's command to the final updated Google Sheet. </p>"},{"location":"cow_video_summary_bot/#project-structure","title":"Project Structure","text":"<ul> <li><code>video_summary_bot.py</code>: The main entry point and orchestrator. You run this script.</li> <li><code>transcript_fetcher.py</code>: Fetches the video transcript and saves it locally.</li> <li><code>gemini_processor.py</code>: Reads the local transcript, analyzes it with Gemini, and saves the structured results.</li> <li><code>sheets_updater.py</code>: Reads the structured results and updates the Google Sheet.</li> <li><code>credentials.json</code>: (You must provide this) Your secret key for the Google Cloud Service Account.</li> <li><code>.gitignore</code>: Tells version control which files to ignore.</li> </ul>"},{"location":"cow_video_summary_bot/#setup","title":"Setup","text":"<ol> <li> <p>Install Python Libraries: <pre><code>pip install google-generativeai youtube-transcript-api gspread google-auth-oauthlib\n</code></pre></p> </li> <li> <p>Google Cloud &amp; Sheets API:</p> <ul> <li>Create a Google Cloud Project and enable the Google Sheets API and Google Drive API.</li> <li>Create a Service Account with the Editor role.</li> <li>Create a JSON key for the service account and download it. Rename the file to <code>credentials.json</code> and place it in this project folder.</li> <li>Share your target Google Sheet (<code>Chart Of The Week</code>) with the <code>client_email</code> found inside your <code>credentials.json</code> file, giving it \"Editor\" permissions.</li> </ul> </li> <li> <p>Set Environment Variables:     This project requires environment variables for security.</p> <p>On Windows (Command Prompt): <pre><code>set GEMINI_API_KEY=\"YOUR_GEMINI_API_KEY_HERE\"\nset GOOGLE_CREDENTIALS_JSON=\"credentials.json\"\n</code></pre></p> <p>On macOS/Linux: <pre><code>export GEMINI_API_KEY=\"YOUR_GEMINI_API_KEY_HERE\"\nexport GOOGLE_CREDENTIALS_JSON=\"credentials.json\"\n</code></pre></p> </li> </ol>"},{"location":"cow_video_summary_bot/#how-to-run","title":"How to Run","text":"<p>Execute the main bot script from your terminal, providing the YouTube video URL as a command-line argument.</p> <pre><code>python video_summary_bot.py \"https://www.youtube.com/watch?v=15wSdRs7Z78\"\n</code></pre> <p>The script will run through the three steps (fetching, processing, updating) and print its progress. Upon completion, your Google Sheet will be updated with the extracted information.</p>"},{"location":"eks-auto-s3/","title":"Secure and cost-effective solution for integrating Amazon Simple Storage Service (S3) with our Amazon Elastic Kubernetes Service (EKS) Auto","text":"<p>As a Solution Architect, I have been tasked by management to design a secure and cost-effective solution for integrating Amazon Simple Storage Service (S3) with our Amazon Elastic Kubernetes Service (EKS) Auto Scaling cluster. </p>"},{"location":"eks-auto-s3/#the-solution-must-adhere-to-the-following-requirements","title":"The solution must adhere to the following requirements:","text":"<ul> <li>Permissions Boundary for EKS Auto Scaling Cluster: Implement fine-grained access controls to ensure that the EKS cluster operates within defined permissions.\u200b</li> <li>Encryption of Data in Transit:        Ensure that all data transferred to and from S3 is encrypted during transit.\u200b</li> <li>Block Public Access to S3 Buckets:        Prevent unauthorized public access to S3 buckets.\u200b</li> <li>Disable ACLs for S3 Access:        Manage access exclusively through policies by disabling Access Control Lists (ACLs)</li> <li>Use S3 Bucket Policies Specific to the Product Use Case:        Tailor bucket policies to align with specific application requirements.\u200b</li> <li>Implement Least Privilege Access:        Grant only the necessary permissions required for each user or service.\u200b</li> <li>Encryption of Data at Rest:        Ensure that data stored in S3 is encrypted.\u200b</li> <li>Implement S3 Object Lock for Write Once Read Many (WORM) Model:        Protect objects from being deleted or overwritten for a specified retention period</li> <li>Use S3 Versioning Only if Absolutely Required:        Enable versioning selectively to manage storage costs and complexity.\u200b</li> <li>Evaluate S3 Bucket Naming Autogeneration Against Maintenance Ease:        Consider the trade-offs between automatic naming conventions and ease of maintenance.\u200b</li> <li>Mutual TLS (mTLS) Access for S3:        Implement mTLS to authenticate both client and server during data transfer.\u200b</li> <li>Manage S3 Storage Lifecycle:        Define policies to transition objects between storage classes and expire them as needed</li> <li>Audit S3 Inventory:        Regularly review S3 inventory reports to monitor and manage stored objects.</li> <li>Use Cross-Region Replication Sparingly:        Limit replication to scenarios where it's absolutely necessary to control costs.\u200b</li> <li>Restrict Amazon S3 Access to Required VPCs:        Ensure that only specific Virtual Private Clouds (VPCs) have access to S3 resources.\u200b</li> <li>Enable CloudWatch Metrics for S3 Bucket Access if Necessary:        Monitor S3 access patterns and performance using CloudWatch.\u200b</li> <li>Enable CloudWatch Alerts for Critical S3 Operations:        Set up alerts for operations such as data deletion or policy changes.\u200b</li> <li>Enable S3 Access Logs:        Maintain logs of access requests to S3 for auditing and monitoring purposes.</li> </ul>"},{"location":"eks-auto-s3/#solution-overview","title":"Solution Overview:","text":"<p>To meet the above requirements, the following architecture and configurations are proposed:</p> <ul> <li> <p>VPC Gateway Endpoint for S3: Establish a VPC Gateway Endpoint to enable private, secure, and cost-effective communication between the EKS cluster and S3 without traversing the public internet. This approach eliminates data transfer costs associated with NAT gateways. \u200b </p> </li> <li> <p>IAM Roles for Service Accounts (IRSA): Utilize IRSA to associate IAM roles with Kubernetes service accounts, granting pods the minimal permissions necessary to access S3. This aligns with the principle of least privilege and simplifies permissions management.</p> </li> <li> <p>S3 Bucket Policies and ACLs:</p> </li> <li>Disable ACLs: Set the S3 Object Ownership setting to \"Bucket owner enforced\" to disable ACLs and manage access exclusively through policies. \u200b</li> <li>Block Public Access: Enable S3 Block Public Access settings to prevent unauthorized public access to S3 buckets.</li> <li> <p>Custom Bucket Policies: Define bucket policies tailored to the application's specific access requirements.</p> </li> <li> <p>Data Encryption:</p> </li> <li>In Transit: Enforce the use of HTTPS (TLS) for all data transfers to and from S3 to encrypt data in transit.</li> <li> <p>At Rest: Enable server-side encryption with Amazon S3-managed keys (SSE-S3) or AWS Key Management Service (SSE-KMS) to encrypt data stored in S3. \u200b</p> </li> <li> <p>S3 Object Lock and Versioning:</p> </li> <li>Object Lock: Implement S3 Object Lock in compliance mode to enforce a Write Once Read Many (WORM) model, protecting objects from being deleted or overwritten during a specified retention period. \u200b</li> <li>Versioning: Enable versioning only for buckets where it is necessary to retain multiple versions of objects, considering the associated storage costs.\u200b</li> <li>Mutual TLS (mTLS): Implement mTLS for services that require enhanced authentication mechanisms, ensuring both client and server verify each other's identity during data transfer.\u200b</li> <li>Storage Lifecycle Management: Define S3 Lifecycle policies to automatically transition objects between storage classes (e.g., from S3 Standard to S3 Glacier) and expire objects that are no longer needed, optimizing storage costs. \u200b</li> <li>Monitoring and Auditing:</li> <li>CloudWatch Metrics and Alerts: Enable CloudWatch metrics to monitor S3 access patterns and set up alerts for critical operations such as data deletions or policy changes.\u200b</li> <li>S3 Access Logs: Activate S3 server access logging to capture detailed records of requests made to the bucket, facilitating auditing and compliance.\u200b</li> <li>S3 Inventory: Use S3 Inventory reports to audit and report on the replication and encryption status of objects for compliance and cost management.\u200b</li> <li>Cross-Region Replication: Implement cross-region replication only when necessary for disaster recovery or compliance requirements, as it incurs additional costs.</li> </ul>"},{"location":"eks-auto-s3/#architecture-overview","title":"Architecture Overview","text":"<p>The architecture comprises the following components:</p> <ul> <li>Amazon Virtual Private Cloud (VPC): The isolated network environment hosting the resources.</li> <li>Public Subnet:<ul> <li>Internet Gateway (IGW): Facilitates internet access for resources within the VPC.</li> <li>NAT Gateway (NAT): Allows instances in the private subnet to initiate outbound IPv4 traffic to the internet while preventing unsolicited inbound traffic.</li> </ul> </li> <li>Private Subnet:<ul> <li>EKS Node Group: A group of Amazon EC2 instances managed by Amazon EKS, configured for auto scaling.</li> <li>EKS Pods: The Kubernetes pods running within the EKS Node Group.</li> </ul> </li> <li> <p>VPC Gateway Endpoint for S3: Enables private connectivity between the VPC and Amazon S3 without requiring internet access.</p> </li> <li> <p>Amazon S3 Bucket: The object storage service where data is stored and accessed by the EKS Pods.   create_secure_s3_bucket-sh</p> </li> </ul>"},{"location":"eks-auto-s3/#data-flow","title":"Data Flow","text":"<ol> <li>EKS Pods initiate requests to the Amazon S3 Bucket.</li> <li>These requests are routed through the VPC Gateway Endpoint for S3, ensuring that the traffic remains within the AWS network, enhancing security and reducing data transfer costs.</li> <li>For any outbound internet traffic (e.g., accessing external services), EKS Pods route requests through the NAT Gateway, which then communicates via the Internet Gateway.</li> </ol> Feature Description Region Match EKS and S3 in same region \u2192 no data cost VPC Gateway Endpoint Private route \u2192 no NAT gateway charges IRSA Least privilege IAM access per pod S3 Encryption + Logging Handled via bucket policies"},{"location":"eks-auto-s3/#benefits","title":"Benefits","text":"<ul> <li>Security: Traffic between EKS Pods and S3 does not traverse the public internet, reducing exposure to potential threats.</li> <li>Cost Efficiency: Utilizing the VPC Gateway Endpoint eliminates data transfer charges associated with accessing S3 over the internet.</li> <li>Performance: Private connectivity offers lower latency and higher throughput compared to internet-based access.</li> </ul>"},{"location":"eks-auto-s3/#references","title":"References","text":"<ul> <li>Gateway Endpoints for Amazon S3</li> <li>Access S3 Buckets from AWS EKS Cluster using IRSA</li> <li>AWS Architecture Icons</li> </ul>"},{"location":"emissary-ingress/","title":"Emissary-ingress quick start in KIND Cluster (Windows)","text":""},{"location":"emissary-ingress/#create-kind-cluster","title":"Create KIND cluster","text":"<pre><code>cat &gt; emissary-ingress-kind-config.yaml &lt;&lt;EOF\nkind: Cluster\napiVersion: kind.x-k8s.io/v1alpha4\nnodes:\n- role: control-plane\n  image: kindest/node:v1.23.12\n  extraPortMappings:\n  - containerPort: 443\n    hostPort: 443\n    listenAddress: \"0.0.0.0\"\n  - containerPort: 80\n    hostPort: 80\n    listenAddress: \"0.0.0.0\"\nEOF\nkind create cluster --name emissary-ingress-k8s --config emissary-ingress-kind-config.yaml\nkubectl get nodes -o wide\n</code></pre>"},{"location":"emissary-ingress/#install-emissary-ingress","title":"Install emissary-ingress","text":"<pre><code># Add the Repo:\nhelm repo add datawire https://app.getambassador.io\nhelm repo update\n\n# Create Namespace and Install:\nkubectl create namespace emissary &amp;&amp; \\\nkubectl apply -f https://app.getambassador.io/yaml/emissary/3.4.0/emissary-crds.yaml\nkubectl wait --timeout=90s --for=condition=available deployment emissary-apiext -n emissary-system\nhelm install emissary-ingress --namespace emissary datawire/emissary-ingress --set daemonSet=true --set dnsPolicy=ClusterFirstWithHostNet --set hostNetwork=true --set agent.enabled=false --set security.podSecurityContext.runAsUser=0  &amp;&amp; \\\nkubectl -n emissary wait --for condition=available --timeout=90s deploy -lapp.kubernetes.io/instance=emissary-ingress\n</code></pre>"},{"location":"emissary-ingress/#install-emissary-ingress-listener","title":"Install emissary-ingress Listener","text":"<pre><code>kubectl apply -f - &lt;&lt;EOF\n---\napiVersion: getambassador.io/v3alpha1\nkind: Listener\nmetadata:\n  name: emissary-ingress-https-listener\n  namespace: emissary\nspec:\n  port: 443\n  protocol: HTTPS\n  securityModel: XFP\n  hostBinding:\n    namespace:\n      from: ALL\n---\n\napiVersion: getambassador.io/v3alpha1\nkind: Listener\nmetadata:\n  name: emissary-ingress-http-listener\n  namespace: emissary\nspec:\n  port: 80\n  protocol: HTTP\n  securityModel: XFP\n  hostBinding:\n    namespace:\n      from: ALL\nEOF\n</code></pre>"},{"location":"emissary-ingress/#install-test-application-quickstartqotmyaml","title":"Install test Application <code>quickstart/qotm.yaml</code>","text":"<pre><code>kubectl apply -f https://app.getambassador.io/yaml/v2-docs/3.4.0/quickstart/qotm.yaml\n</code></pre>"},{"location":"emissary-ingress/#install-emissary-ingress-host","title":"Install emissary-ingress Host","text":"<pre><code>kubectl apply -f - &lt;&lt;EOF\n---\napiVersion: getambassador.io/v3alpha1\nkind: Host\nmetadata:\n  name: wildcard-host\nspec:\n  hostname: \"*\"\n  acmeProvider:\n    authority: none\n  tlsSecret:\n    name: tls-cert\nEOF\n</code></pre>"},{"location":"emissary-ingress/#install-emissary-ingress-mapping","title":"Install emissary-ingress Mapping","text":"<pre><code>kubectl apply -f - &lt;&lt;EOF\n---\napiVersion: getambassador.io/v3alpha1\nkind: Mapping\nmetadata:\n  name: quote-backend\nspec:\n  hostname: \"*\"\n  prefix: /backend/\n  service: quote\n  docs:\n    path: \"/.ambassador-internal/openapi-docs\"\nEOF\n</code></pre>"},{"location":"emissary-ingress/#test-the-http-link","title":"Test the http link","text":"<pre><code>curl -i http://localhost/backend/\n</code></pre>"},{"location":"emissary-ingress/#secure-communication","title":"Secure communication","text":""},{"location":"emissary-ingress/#create-the-certificate","title":"Create the Certificate","text":"<pre><code>#Linux\nopenssl req -x509 -newkey rsa:4096 -keyout key.pem -out cert.pem -subj '/CN=ambassador-cert' -nodes\n\n#Windows (git-bash)\nopenssl req -x509 -newkey rsa:4096 -keyout key.pem -out cert.pem -subj '//CN=ambassador-cert' -nodes\n\n#Verifiy\nls *.pem\n</code></pre>"},{"location":"emissary-ingress/#create-the-tls-secret","title":"Create the TLS secret","text":"<pre><code>kubectl create secret tls tls-cert --cert=cert.pem --key=key.pem\n</code></pre>"},{"location":"emissary-ingress/#test-the-https-link","title":"Test the https link","text":"<pre><code>curl -Lk https://localhost:8443/backend/\n</code></pre>"},{"location":"emissary-ingress/#uninstall","title":"Uninstall","text":"<pre><code>kind delete cluster --name=emissary-ingress-k8s\n</code></pre>"},{"location":"gcp-metrics-exporter/","title":"Unlocking GitHub Copilot Metrics: My Journey to a Custom Prometheus Exporter","text":"<p>This service fetches GitHub Copilot metrics for your organization and exposes them as Prometheus metrics.</p> <ul> <li>Periodically pull metrics from GitHub Enterprise.</li> <li>Expose these metrics via a standard /metrics HTTP endpoint, consumable by Prometheus.</li> <li>Store the historical data in Prometheus's time-series database (TSDB).</li> <li>Visualize the trends and key insights using Grafana.</li> </ul>"},{"location":"gcp-metrics-exporter/#you-can-find-the-project-on-my-github-naren4bgcp-metrics-exporter","title":"You can find the project on my GitHub: naren4b/gcp-metrics-exporter","text":""},{"location":"gcp-metrics-exporter/#architecture-at-a-glance","title":"Architecture at a Glance","text":"<p>The architecture is straightforward and integrates perfectly with standard observability practices: - Developer Activity: GitHub Copilot provides real-time assistance, influencing the developer's prompt and response patterns. - Telemetry Flow: This interaction generates telemetry data, which is crucial for understanding Copilot's effectiveness. - Prometheus Exporter (My Solution): This is where gcp-metrics-exporter comes in. It periodically pulls the Copilot telemetry directly from GitHub Enterprise. - Prometheus: Our Prometheus instance is configured to scrape the /metrics endpoint exposed by the exporter. This pulls the processed telemetry data and stores it in its TSDB for long-term retention. - Grafana: Finally, Grafana connects to Prometheus to query and visualize this rich dataset, allowing us to build insightful dashboards.</p>"},{"location":"gcp-metrics-exporter/#running-with-docker","title":"Running with Docker","text":"<ol> <li>Build the Docker image:</li> </ol> <pre><code>docker build -t copilot-metrics-exporter .\n</code></pre> <ol> <li>Run the container:</li> </ol> <pre><code>docker run -d \\\n     -e GHC_TOKEN=your_github_copilot_token \\\n     -e ORG=your_github_org \\\n     -e CACHE_TTL_SECONDS=3600 \\\n     -p 8000:8000 \\\n     --name copilot-metrics-exporter \\\n     naren4b/copilot-metrics-exporter:latest\n</code></pre> <ul> <li>Replace <code>your_github_copilot_token</code> with your GitHub Copilot API token.</li> <li>Replace <code>your_github_org</code> with your GitHub organization name.</li> <li> <p>Replace <code>CACHE_TTL_SECONDS</code> with your wish of metric pull duration in seconds.</p> </li> <li> <p>Access the metrics endpoint:</p> </li> </ul> <p>Open http://localhost:8000/metrics in your browser or Prometheus scrape config.</p>"},{"location":"gcp-metrics-exporter/#update-the-prometheus-configuration","title":"Update the prometheus configuration","text":"<pre><code>  - job_name: 'gcp'\n    scrape_interval: 15s\n    static_configs:\n      - targets: ['&lt;HOST-IP&gt;:8000']\n</code></pre> <p>Note:</p> <ul> <li>The container listens on port <code>8000</code> by default.</li> <li>Make sure your GitHub token has the necessary permissions to access the Copilot metrics API.</li> </ul>"},{"location":"gcp-metrics-exporter/#metrics-documentation","title":"Metrics Documentation","text":"<p>Below are the Prometheus metrics exposed by this exporter, along with their descriptions and usage:</p>"},{"location":"gcp-metrics-exporter/#1-copilot_exporter_requests_total","title":"1. <code>copilot_exporter_requests_total</code>","text":"<p>Type: Counter Labels: None Description: Total number of requests made to the <code>/metrics</code> endpoint (i.e., how many times Prometheus or a user scraped metrics).</p> <p>Example: <pre><code>copilot_exporter_requests_total 42\n</code></pre></p>"},{"location":"gcp-metrics-exporter/#2-copilot_exporter_requests_failed_total","title":"2. <code>copilot_exporter_requests_failed_total</code>","text":"<p>Type: Counter Labels: - <code>status_code</code>: The HTTP status code or error type for the failure (e.g., <code>404</code>, <code>500</code>, <code>env_missing</code>, <code>request_exception</code>).</p> <p>Description: Counts the number of failed or empty responses from the GitHub Copilot metrics API, labeled by the type of failure or HTTP status code.</p> <p>Example: <pre><code>copilot_exporter_requests_failed_total{status_code=\"env_missing\"} 1\ncopilot_exporter_requests_failed_total{status_code=\"500\"} 2\n</code></pre></p>"},{"location":"gcp-metrics-exporter/#3-copilot_exporter_github_api_requests","title":"3. <code>copilot_exporter_github_api_requests</code>","text":"<p>Type: Counter Labels: None Description: Total number of requests made to the GitHub Copilot metrics API.</p> <p>Example: <pre><code>copilot_exporter_github_api_requests 40\n</code></pre></p>"},{"location":"gcp-metrics-exporter/#4-copilot_exporter_cache_hits","title":"4. <code>copilot_exporter_cache_hits</code>","text":"<p>Type: Counter Labels: None Description: Total number of times the cached Copilot metrics were used instead of making a new API call.</p> <p>Example: <pre><code>copilot_exporter_cache_hits 25\n</code></pre></p>"},{"location":"gcp-metrics-exporter/#5-copilot-metric-gauges","title":"5. Copilot Metric Gauges","text":"<p>Each of the following metrics is exposed as a Prometheus Gauge with the labels: - <code>editor</code> - <code>language</code> - <code>stream</code> - <code>org</code></p> <p>Metric Names: - <code>total_engaged_users</code> - <code>is_custom_model</code> - <code>total_chat_copy_events</code> - <code>total_chat_insertion_events</code> - <code>total_chats</code> - <code>total_code_acceptances</code> - <code>total_code_lines_accepted</code> - <code>total_code_lines_suggested</code> - <code>total_code_suggestions</code> - <code>total_active_users</code></p> <p>Description: These metrics represent various usage and engagement statistics for GitHub Copilot within your organization. Each metric is labeled by editor, language, stream, and organization for detailed analysis.</p> <p>Example: <pre><code>total_active_users{editor=\"vscode\",language=\"python\",stream=\"main\",org=\"my-org\"} 123\ntotal_code_suggestions{editor=\"vscode\",language=\"python\",stream=\"main\",org=\"my-org\"} 456\n</code></pre></p>"},{"location":"gcp-metrics-exporter/#notes","title":"Notes","text":"<ul> <li>All metrics are available at the <code>/metrics</code> endpoint.</li> <li>Use the <code>status_code</code> label on <code>copilot_exporter_requests_failed_total</code> to diagnose API or configuration issues.</li> <li>Cache hits help you monitor how often the exporter is serving data from cache versus making new API calls.</li> </ul>"},{"location":"gitlab-ci-cheat-sheet/","title":"Gitlab ci cheat sheet","text":""},{"location":"gitlab-ci-cheat-sheet/#gitlab-cicd-cheat-sheet-for-new-joiners","title":"GitLab CI/CD Cheat Sheet for New Joiners","text":"<p>Welcome to your quick-reference guide to GitLab CI/CD! This cheat sheet is designed to help you understand the most important GitLab CI/CD concepts, directives, and real-world examples in one place. Ideal for beginners and new team members.</p>"},{"location":"gitlab-ci-cheat-sheet/#1-stages","title":"\ud83e\udde9 1. Stages","text":"<p>Purpose: Define phases of the pipeline.</p> <p>Directive:</p> <pre><code>stages:\n  - build\n  - test\n  - deploy\n</code></pre> <p>Example: In an e-commerce app, you may build Docker images, test payment services, and deploy to staging or production.</p>"},{"location":"gitlab-ci-cheat-sheet/#2-jobs","title":"\ud83d\udd28 2. Jobs","text":"<p>Purpose: Define the tasks in each stage.</p> <p>Directive:</p> <pre><code>build_app:\n  stage: build\n  script:\n    - npm install\n    - npm run build\n</code></pre> <p>Example: A job to compile frontend code before packaging it into a Docker image.</p>"},{"location":"gitlab-ci-cheat-sheet/#3-pipeline-triggering","title":"\ud83d\ude80 3. Pipeline Triggering","text":"<p>Purpose: Automatically trigger on events.</p> <p>Default: Triggered on push, merge request, etc.</p> <p>Example: When a developer pushes code to the <code>main</code> branch, a pipeline runs automatically.</p>"},{"location":"gitlab-ci-cheat-sheet/#4-runners","title":"\ud83c\udfc3 4. Runners","text":"<p>Purpose: Execute jobs on GitLab Runner (shared or custom).</p> <p>Directive:</p> <pre><code>tags: [custom-runner]\n</code></pre> <p>Example: A self-managed runner deployed in a secure cloud environment for sensitive data.</p>"},{"location":"gitlab-ci-cheat-sheet/#5-environment-variables","title":"\ud83d\udd10 5. Environment Variables","text":"<p>Purpose: Configure settings dynamically.</p> <p>Directive:</p> <pre><code>variables:\n  NODE_ENV: production\n</code></pre> <p>Example: Use <code>NODE_ENV=production</code> to differentiate between test and prod builds.</p>"},{"location":"gitlab-ci-cheat-sheet/#6-secret-masking","title":"\ud83d\udee1\ufe0f 6. Secret Masking","text":"<p>Purpose: Protect sensitive values.</p> <p>Setup: Configure in GitLab UI \u2192 CI/CD \u2192 Variables \u2192 Masked.</p> <p>Example: Mask AWS access keys during deployment.</p>"},{"location":"gitlab-ci-cheat-sheet/#7-before_script","title":"\u2699\ufe0f 7. <code>before_script</code>","text":"<p>Purpose: Run setup steps before job.</p> <p>Directive:</p> <pre><code>before_script:\n  - npm install\n</code></pre> <p>Example: Install dependencies before running tests.</p>"},{"location":"gitlab-ci-cheat-sheet/#8-testing-reports","title":"\ud83e\uddea 8. Testing &amp; Reports","text":"<p>Purpose: Execute and collect test results.</p> <p>Directive:</p> <pre><code>artifacts:\n  reports:\n    junit: test-results.xml\n</code></pre> <p>Example: Publish unit test results from Jest or Mocha.</p>"},{"location":"gitlab-ci-cheat-sheet/#9-code-coverage","title":"\ud83d\udcc8 9. Code Coverage","text":"<p>Purpose: Show coverage % in GitLab UI.</p> <p>Directive:</p> <pre><code>coverage: '/TOTAL\\s+\\d+\\s+\\d+\\s+(\\d+%)/'\n</code></pre> <p>Example: Display how much of the backend logic is covered by unit tests.</p>"},{"location":"gitlab-ci-cheat-sheet/#10-services-eg-dbs","title":"\ud83e\uddf0 10. Services (e.g., DBs)","text":"<p>Purpose: Attach DB or services to jobs.</p> <p>Directive:</p> <pre><code>services:\n  - postgres:latest\n</code></pre> <p>Example: Run integration tests using a temporary PostgreSQL DB.</p>"},{"location":"gitlab-ci-cheat-sheet/#11-caching","title":"\ud83d\udce6 11. Caching","text":"<p>Purpose: Speed up jobs by reusing data.</p> <p>Directive:</p> <pre><code>cache:\n  paths:\n    - node_modules/\n</code></pre> <p>Example: Avoid re-downloading packages for every pipeline.</p>"},{"location":"gitlab-ci-cheat-sheet/#12-docker-build-push","title":"\ud83d\udc33 12. Docker Build &amp; Push","text":"<p>Purpose: Build &amp; deploy container images.</p> <p>Directive:</p> <pre><code>script:\n  - docker build -t $CI_REGISTRY_IMAGE:$CI_COMMIT_REF_NAME .\n  - docker push $CI_REGISTRY_IMAGE:$CI_COMMIT_REF_NAME\n</code></pre> <p>Example: Push microservice container to GitLab Container Registry.</p>"},{"location":"gitlab-ci-cheat-sheet/#13-environments","title":"\ud83c\udf0d 13. Environments","text":"<p>Purpose: Identify deployment target.</p> <p>Directive:</p> <pre><code>environment:\n  name: staging\n</code></pre> <p>Example: Use different K8s namespaces for staging vs production.</p>"},{"location":"gitlab-ci-cheat-sheet/#14-manual-deployments","title":"\u270b 14. Manual Deployments","text":"<p>Purpose: Require approval before continuing.</p> <p>Directive:</p> <pre><code>when: manual\n</code></pre> <p>Example: Promote to production only after QA signs off.</p>"},{"location":"gitlab-ci-cheat-sheet/#15-kubernetes-deployments","title":"\u2638\ufe0f 15. Kubernetes Deployments","text":"<p>Purpose: Automate app deployment to K8s.</p> <p>Example Job:</p> <pre><code>script:\n  - kubectl apply -f k8s/deployment.yaml\n</code></pre> <p>Example: Deploy a Helm chart or manifest to a cluster from pipeline.</p>"},{"location":"gitlab-ci-cheat-sheet/#16-parallel-execution","title":"\ud83c\udfa2 16. Parallel Execution","text":"<p>Purpose: Speed up tasks using multiple workers.</p> <p>Directive:</p> <pre><code>parallel:\n  matrix:\n    - NODE_VERSION: [14, 16]\n</code></pre> <p>Example: Test a NodeJS app against multiple versions of Node.</p>"},{"location":"gitlab-ci-cheat-sheet/#17-job-dependencies","title":"\ud83d\udd17 17. Job Dependencies","text":"<p>Purpose: Define job sequence across stages.</p> <p>Directive:</p> <pre><code>needs: [build_app]\n</code></pre> <p>Example: Only test the app if the build was successful.</p>"},{"location":"gitlab-ci-cheat-sheet/#18-timeouts","title":"\u23f1\ufe0f 18. Timeouts","text":"<p>Purpose: Stop jobs that take too long.</p> <p>Directive:</p> <pre><code>timeout: 10 minutes\n</code></pre> <p>Example: Prevent infinite loops in e2e test scripts.</p>"},{"location":"gitlab-ci-cheat-sheet/#19-rules","title":"\ud83e\udde0 19. Rules","text":"<p>Purpose: Run jobs based on conditions.</p> <p>Directive:</p> <pre><code>rules:\n  - if: '$CI_COMMIT_BRANCH == \"main\"'\n</code></pre> <p>Example: Only deploy if the commit is to <code>main</code> branch.</p>"},{"location":"gitlab-ci-cheat-sheet/#20-reusable-job-templates","title":"\ud83e\udde9 20. Reusable Job Templates","text":"<p>Purpose: Avoid duplication.</p> <p>Directive:</p> <pre><code>extends: .base_job\n</code></pre> <p>Example: Reuse the same deploy steps for staging, QA, and prod.</p>"},{"location":"gitlab-ci-cheat-sheet/#21-security-testing","title":"\ud83d\udd0d 21. Security Testing","text":"<p>Purpose: Add SAST, DAST, Secret Scanning.</p> <p>Directive:</p> <pre><code>include:\n  - template: Security/SAST.gitlab-ci.yml\n</code></pre> <p>Example: Scan for common code vulnerabilities in every MR.</p>"},{"location":"gitlab-ci-cheat-sheet/#22-notifications","title":"\ud83d\udce3 22. Notifications","text":"<p>Purpose: Alert on job status.</p> <p>Example: Send Slack message using webhook after deployment.</p>"},{"location":"gitlab-ci-cheat-sheet/#23-scheduled-pipelines","title":"\ud83d\udd50 23. Scheduled Pipelines","text":"<p>Purpose: Trigger pipeline at intervals.</p> <p>Setup: GitLab UI \u2192 CI/CD \u2192 Schedules</p> <p>Example: Run database backup every night at 2 AM.</p>"},{"location":"gitlab-ci-cheat-sheet/#24-auto-devops","title":"\u2705 24. Auto DevOps","text":"<p>Purpose: Use GitLab's pre-configured pipeline.</p> <p>Setup: GitLab UI \u2192 Enable Auto DevOps</p> <p>Example: Auto-detect language and deploy app with zero config.</p>"},{"location":"gitlab-ci-cheat-sheet/#gitlab-cicd-cheat-sheet-solar-system-nodejs-project","title":"\ud83d\udef0\ufe0f GitLab CI/CD Cheat Sheet \u2013 Solar System NodeJS Project","text":""},{"location":"gitlab-ci-cheat-sheet/#1-workflow-controlling-pipeline-execution","title":"1. \ud83d\udcdc <code>workflow</code> \u2013 Controlling Pipeline Execution","text":"<pre><code>workflow:\n  name: Solar System NodeJS Pipeline\n  rules:\n    - if: $CI_COMMIT_BRANCH == 'main' || $CI_COMMIT_BRANCH =~ /^feature/\n      when: always\n    - if: $CI_MERGE_REQUEST_SOURCE_BRANCH_NAME =~ /^feature/ &amp;&amp; $CI_PIPELINE_SOURCE == 'merge_request_event'\n      when: always\n</code></pre> <p>\ud83d\udc68\u200d\ud83d\udcbb Scenario: - Developer creates <code>feature/add-auth</code> \u2192 pipeline runs. - Push to <code>main</code> \u2192 pipeline runs. - Push to <code>hotfix/urgent-fix</code> \u2192 pipeline skipped.</p>"},{"location":"gitlab-ci-cheat-sheet/#2-stages-defining-pipeline-steps","title":"2. \ud83e\uddf1 <code>stages</code> \u2013 Defining Pipeline Steps","text":"<pre><code>stages:\n  - test\n  - reporting\n  - containerization\n  - dev-deploy\n  - stage-deploy\n</code></pre>"},{"location":"gitlab-ci-cheat-sheet/#stages","title":"Stages:","text":"<ol> <li><code>test</code>: Run unit tests  </li> <li><code>reporting</code>: Generate reports  </li> <li><code>containerization</code>: Build Docker image  </li> <li><code>dev-deploy</code>: Deploy to development  </li> <li><code>stage-deploy</code>: Manual deploy to staging  </li> </ol>"},{"location":"gitlab-ci-cheat-sheet/#3-include-reusing-configurations","title":"3. \ud83d\udce6 <code>include</code> \u2013 Reusing Configurations","text":"<pre><code>include:\n  - local: 'template/aws-reports.yml'\n</code></pre> <p>\ud83d\udcc1 Use shared templates for Slack alerts, scanning, etc.</p>"},{"location":"gitlab-ci-cheat-sheet/#4-variables-setting-environment-variables","title":"4. \u2699\ufe0f <code>variables</code> \u2013 Setting Environment Variables","text":"<pre><code>variables:\n  DOCKER_USERNAME: siddharth67\n  IMAGE_VERSION: $CI_PIPELINE_ID\n  K8S_IMAGE: $DOCKER_USERNAME/solar-system:$IMAGE_VERSION\n  MONGO_URI: 'mongodb+srv://supercluster...'\n  MONGO_USERNAME: superuser\n  MONGO_PASSWORD: $M_DB_PASSWORD\n</code></pre> <p>\ud83d\udd10 Store secrets in GitLab \u2192 Settings \u2192 CI/CD \u2192 Variables.</p>"},{"location":"gitlab-ci-cheat-sheet/#5-prepare_nodejs_environment-common-job-template","title":"5. \ud83e\uddf0 <code>.prepare_nodejs_environment</code> \u2013 Common Job Template","text":"<pre><code>.prepare_nodejs_environment:\n  image: node:17-alpine3.14\n  services:\n    - name: siddharth67/mongo-db:non-prod\n      alias: mongo\n  before_script:\n    - npm install\n</code></pre> <p>\ud83c\udfaf Use <code>extends:</code> to inherit this setup in jobs.</p>"},{"location":"gitlab-ci-cheat-sheet/#6-unit_testing-running-tests-and-collecting-results","title":"6. \u2705 <code>unit_testing</code> \u2013 Running Tests and Collecting Results","text":"<pre><code>unit_testing:\n  stage: test\n  extends: .prepare_nodejs_environment\n  script:\n    - npm test\n  artifacts:\n    reports:\n      junit: test-results.xml\n</code></pre> <p>\ud83e\uddea View test results in GitLab UI.</p>"},{"location":"gitlab-ci-cheat-sheet/#7-reporting-custom-reporting-or-alerts","title":"7. \ud83d\udcca <code>reporting</code> \u2013 Custom Reporting or Alerts","text":"<pre><code>reporting:\n  stage: reporting\n  tags:\n    - docker\n    - linux\n    - aws\n</code></pre> <p>\ud83d\udce4 Push metrics or test results to S3, notify teams, etc.</p>"},{"location":"gitlab-ci-cheat-sheet/#8-containerization-stages-commented","title":"8. \ud83d\udd10 Containerization Stages (Commented)","text":"<p>Jobs like: - <code>docker_build</code>: Builds and saves <code>.tar</code> - <code>docker_test</code>: Loads and tests image - <code>docker_push</code>: Pushes to Docker Hub</p> <p>\ud83d\udca1 Automate end-to-end container workflow.</p>"},{"location":"gitlab-ci-cheat-sheet/#9-kubernetes-deployment-dev-and-stage","title":"9. \u2601\ufe0f Kubernetes Deployment (Dev and Stage)","text":"<pre><code>environment:\n  name: development\n  url: https://$INGRESS_URL\n</code></pre>"},{"location":"gitlab-ci-cheat-sheet/#k8s_dev_deploy-auto-deploys","title":"<code>k8s_dev_deploy</code>: Auto deploys","text":""},{"location":"gitlab-ci-cheat-sheet/#k8s_stage_deploy-manual-trigger","title":"<code>k8s_stage_deploy</code>: Manual trigger","text":"<pre><code>kubectl -n $NAMESPACE create secret generic mongo-db-creds --from-literal=MONGO_URI=$MONGO_URI --from-literal=MONGO_USERNAME=$MONGO_USERNAME --from-literal=MONGO_PASSWORD=$MONGO_PASSWORD\n</code></pre> <p>\ud83d\udd10 Store DB secrets securely.</p>"},{"location":"gitlab-ci-cheat-sheet/#10-integration-testing-in-kubernetes","title":"10. \ud83e\uddea Integration Testing in Kubernetes","text":"<pre><code>curl -s -k https://$INGRESS_URL/live | jq -r .status | grep -i live\n</code></pre> <p>\ud83d\udce1 Check health post-deployment.</p>"},{"location":"gitlab-ci-cheat-sheet/#summary-for-new-joiners","title":"\u2705 Summary for New Joiners","text":"Section Purpose What You Do <code>workflow</code> Control triggers Define branch/MR rules <code>stages</code> Define pipeline Order job execution <code>include</code> Reuse logic DRY for templates <code>variables</code> Set configs Use GitLab CI/CD Variables <code>extends</code> Share setup Common environment for jobs <code>artifacts</code> Save outputs Store test/report files <code>docker_*</code> Container lifecycle Build, test, push images <code>k8s_*</code> Deployment Auto/manual K8s deployment <code>integration_test</code> Health check Verify deployment works # Refered gitlab-ci.yml <pre><code>workflow:\n    name: Solar System NodeJS Pipeline\n    rules:\n        - if: $CI_COMMIT_BRANCH == 'main' || $CI_COMMIT_BRANCH =~ /^feature/\n          when: always\n        - if: $CI_MERGE_REQUEST_SOURCE_BRANCH_NAME =~ /^feature/ &amp;&amp; $CI_PIPELINE_SOURCE == 'merge_request_event'\n          when: always\n\nstages:\n  - test\n  - reporting\n  - containerization\n  - dev-deploy\n  - stage-deploy\n\ninclude:\n  - local: 'template/aws-reports.yml'\n  - component: gitlab.com/gitlab-components/code-quality/code-quality@1.0\n  - template: Jobs/SAST.gitlab-ci.yml\n  - component: gitlab.com/gitlab-components/secret-detection/secret-detection@1.0\n  - template: Security/Container-Scanning.gitlab-ci.yml\n\nvariables:\n    DOCKER_USERNAME: siddharth67\n    IMAGE_VERSION: $CI_PIPELINE_ID\n    K8S_IMAGE: $DOCKER_USERNAME/solar-system:$IMAGE_VERSION\n    MONGO_URI: 'mongodb+srv://supercluster.d83jj.mongodb.net/superData'\n    MONGO_USERNAME: superuser\n    MONGO_PASSWORD: $M_DB_PASSWORD\n    SCAN_KUBERNETES_MANIFESTS: \"true\"\n\n.prepare_nodejs_environment:\n  image: node:17-alpine3.14\n  services:\n    - name: siddharth67/mongo-db:non-prod\n      alias: mongo\n      pull_policy: always\n  variables:\n    MONGO_URI: 'mongodb://mongo:27017/superData'\n    MONGO_USERNAME: non-prod-user\n    MONGO_PASSWORD: non-prod-password\n  cache:\n    policy: pull-push\n    when: on_success\n    paths:\n      - node_modules\n    key:\n      files:\n        - package-lock.json\n      prefix: node_modules\n  before_script:\n    - npm install  \n\n.prepare_deployment_environment: &amp;kuberntes_deploy_job\n  image:\n    name: alpine:3.7\n  dependencies: []\n  before_script:\n    - wget https://storage.googleapis.com/kubernetes-release/release/$(wget -q -O - https://storage.googleapis.com/kubernetes-release/release/stable.txt)/bin/linux/amd64/kubectl\n    - chmod +x ./kubectl\n    - mv ./kubectl /usr/bin/kubectl\n    - apk add --no-cache gettext\n    - envsubst -V  \n\ncode_quality:\n  stage: \".pre\"\n  variables:\n    REPORT_FORMAT: html\n  artifacts:\n    paths: [gl-code-quality-report.html]\n    reports:\n      codequality: []\n\nsast:\n  stage: .pre\n\nsecret_detection:\n  stage: .pre\n  variables:\n    SECRET_DETECTION_HISTORIC_SCAN: \"true\"\n\ncontainer_scanning:\n  stage: containerization\n  needs:\n    - docker_push\n  variables:\n    CS_IMAGE: docker.io/$DOCKER_USERNAME/solar-system:$IMAGE_VERSION\n\nunit_testing:\n  stage: test\n  extends: .prepare_nodejs_environment\n  script:\n    - npm test\n  artifacts:\n    when: always\n    expire_in: 3 days\n    name: Moca-Test-Result\n    paths:\n      - test-results.xml\n    reports:\n      junit: test-results.xml\n\nreporting:\n  stage: reporting\n  tags:\n    - docker\n    - linux\n    - aws\n\ncode_coverage:\n  stage: test\n  extends: .prepare_nodejs_environment\n  script:\n    - npm run coverage\n  artifacts:\n    name: Code-Coverage-Result\n    when: always\n    expire_in: 3 days\n    reports:\n      coverage_report:\n        coverage_format: cobertura\n        path: coverage/cobertura-coverage.xml\n  coverage: /All files[^|]*\\|[^|]*\\s+([\\d\\.]+)/\n  allow_failure: true\n\ndocker_build:\n  stage: containerization\n  image: docker:24.0.5\n  dependencies: []\n  services:\n    - docker:24.0.5-dind\n  script:\n    - docker build -t $DOCKER_USERNAME/solar-system:$IMAGE_VERSION .\n    - docker images $DOCKER_USERNAME/solar-system:$IMAGE_VERSION\n    - mkdir image\n    - docker save $DOCKER_USERNAME/solar-system:$IMAGE_VERSION &gt; image/solar-system-image-$IMAGE_VERSION.tar\n  artifacts:\n    paths:\n      - image\n    when: on_success\n    expire_in: 3 days\n\ndocker_test:\n  stage: containerization\n  image: docker:24.0.5\n  needs:\n    - docker_build\n  services:\n    - docker:24.0.5-dind\n  script:\n    - docker load -i image/solar-system-image-$IMAGE_VERSION.tar\n    - docker run --name solar-system-app -d -p 3000:3000 $DOCKER_USERNAME/solar-system:$IMAGE_VERSION\n    - export IP=$(docker inspect -f '{{range .NetworkSettings.Networks}}{{.IPAddress}}{{end}}' solar-system-app)\n    - echo $IP\n    - docker run  alpine wget -q -O - http://$IP:3000/live | grep live\n\ndocker_push:  \n  stage: containerization\n  needs:\n    - docker_build\n    - docker_test\n  image: docker:24.0.5\n  services:\n    - docker:24.0.5-dind\n  script:\n    -  docker load -i image/solar-system-image-$IMAGE_VERSION.tar\n    -  docker login --username=$DOCKER_USERNAME --password=$DOCKER_PASSWORD\n    -  docker push $DOCKER_USERNAME/solar-system:$IMAGE_VERSION\n\npublish_gitlab_container_registry:  \n  stage: containerization\n  needs:\n    - docker_build\n    - docker_test\n  image: docker:24.0.5\n  services:\n    - docker:24.0.5-dind\n  script:\n    -  docker load -i image/solar-system-image-$CI_PIPELINE_ID.tar\n    -  echo \"$CI_REGISTRY -u $CI_REGISTRY_USER -p $CI_REGISTRY_PASSWORD $CI_REGISTRY_IMAGE\"\n    -  docker login $CI_REGISTRY -u $CI_REGISTRY_USER -p $CI_REGISTRY_PASSWORD\n    -  docker tag $DOCKER_USERNAME/solar-system:$IMAGE_VERSION $CI_REGISTRY_IMAGE/ss-image:$IMAGE_VERSION \n    -  docker images\n    -  docker push $CI_REGISTRY_IMAGE/ss-image:$IMAGE_VERSION\n\nk8s_dev_deploy:\n  &lt;&lt;: *kuberntes_deploy_job\n  stage: dev-deploy\n  needs:\n    - docker_push\n  script:\n    - export KUBECONFIG=$DEV_KUBE_CONFIG\n    - kubectl version -o yaml\n    - kubectl config get-contexts\n    - kubectl get nodes\n    - export INGRESS_IP=$(kubectl -n ingress-nginx get services ingress-nginx-controller -o jsonpath=\"{.status.loadBalancer.ingress[0].ip}\")\n    - echo $INGRESS_IP\n    - kubectl -n $NAMESPACE create secret generic mongo-db-creds --from-literal=MONGO_URI=$MONGO_URI --from-literal=MONGO_USERNAME=$MONGO_USERNAME --from-literal=MONGO_PASSWORD=$MONGO_PASSWORD --save-config --dry-run=client -o yaml | kubectl apply -f -\n    - for i in kubernetes/manifest/*.yaml; do envsubst &lt; $i | kubectl apply -f -; done\n    - kubectl -n $NAMESPACE get all,secret,ing\n    - echo \"INGRESS_URL=$(kubectl -n $NAMESPACE get ing -o jsonpath=\"{.items[0].spec.tls[0].hosts[0]}\")\" &gt;&gt; app_ingress_url.env\n  artifacts:\n    reports:\n      dotenv: app_ingress_url.env\n  environment:\n    name: development\n    url: https://$INGRESS_URL\n\nk8s_dev_integration_testing:\n  stage: dev-deploy\n  image: alpine:3.4\n  needs:\n    - k8s_dev_deploy\n  before_script:\n    - apk --no-cache add curl\n    - apk --no-cache add jq\n  script:\n    - echo $INGRESS_URL\n    - curl -s -k https://$INGRESS_URL/live | jq -r .status | grep -i live\n    - curl -s -k https://$INGRESS_URL/ready | jq -r .status | grep -i ready\n\nk8s_stage_deploy:\n  &lt;&lt;: *kuberntes_deploy_job\n  stage: stage-deploy\n  when: manual\n  script:\n    - temp_kube_config_file=$(printenv KUBECONFIG)\n    - cat $temp_kube_config_file\n    - kubectl config get-contexts\n    - kubectl config use-context demos-group/solar-system:kk-gitlab-agent\n    - kubectl get po -A\n    - export INGRESS_IP=$(kubectl -n ingress-nginx get services ingress-nginx-controller -o jsonpath=\"{.status.loadBalancer.ingress[0].ip}\")\n    - echo $INGRESS_IP\n    - kubectl -n $NAMESPACE create secret generic mongo-db-creds --from-literal=MONGO_URI=$MONGO_URI --from-literal=MONGO_USERNAME=$MONGO_USERNAME --from-literal=MONGO_PASSWORD=$MONGO_PASSWORD --save-config --dry-run=client -o yaml | kubectl apply -f -\n    - for i in kubernetes/manifest/*.yaml; do envsubst &lt; $i | kubectl apply -f -; done\n    - kubectl -n $NAMESPACE get all,secret,ing\n    - echo \"INGRESS_URL=$(kubectl -n $NAMESPACE get ing -o jsonpath=\"{.items[0].spec.tls[0].hosts[0]}\")\" &gt;&gt; app_ingress_url.env\n  artifacts:\n    reports:\n      dotenv: app_ingress_url.env\n  environment:\n    name: staging\n    url: https://$INGRESS_URL\n\nk8s_stage_integration_testing:\n  stage: stage-deploy\n  image: !reference [k8s_dev_integration_testing, image]\n  needs:\n    - k8s_stage_deploy\n  before_script: !reference [k8s_dev_integration_testing, before_script]\n  script: !reference [k8s_dev_integration_testing, script]\n</code></pre>"},{"location":"harbor-api/","title":"Accessing Harbor registry through REST API","text":"<p>Install the registry ref: Setting up Basic Harbor Registry in a Kubernetes Cluster</p>"},{"location":"harbor-api/#01-setup","title":"01. Setup","text":"<pre><code># check and change these values if needed\ncat&lt;&lt; EOF &gt; config.sh\nexport HARBOR_URL=\"registry.127.0.0.1.nip.io\"\nexport ADMIN_USER=admin\nexport ADMIN_PASSWORD=\"Harbor12345\"\nOUTDIR=$PWD/out\nEOF\nsource config.sh\n</code></pre>"},{"location":"harbor-api/#getting-stats-about-the-registry","title":"Getting stats about the registry","text":"<pre><code>rm -rf $PWD/out\nexport HARBOR_URL=registry.127.0.0.1.nip.io\ngit clone https://github.com/naren4b/harbor-registry.git\ncd harbor-registry/harbor-api\nbash 01_getProjects.sh\ncp out/projects.txt .\nbash run.sh projects.txt\n</code></pre>"},{"location":"harbor-api/#02-create-a-project","title":"02. Create a Project","text":"<pre><code>project=$1 #demo\ncat&lt;&lt;EOF &gt;$project.json\n{\n  \"project_name\": \"$project\",\n  \"public\": false,\n  \"metadata\": {\n    \"public\": \"false\"\n  }\n}\nEOF\n\ncurl -k -s \\\n        -X POST \\\n        -u ${ADMIN_USER}:${ADMIN_PASSWORD} \\\n        -H 'accept: application/json' \\\n        -H 'Content-Type: application/json' \\\n        \"https://${HARBOR_URL}/api/v2.0/projects\" \\\n        -d @$project.json\n\ndocker login $HARBOR_URL -u $ADMIN_USER -p $ADMIN_PASSWORD\ndocker pull nginx:latest\ndocker tag nginx:latest $HARBOR_URL/library/nginx:latest\ndocker push $HARBOR_URL/library/nginx:latest\n</code></pre>"},{"location":"harbor-api/#03-get-projects-in-the-harbor-registry","title":"03. Get Projects in the Harbor Registry","text":"<pre><code>page=1\nwhile :; do\n    response=$(curl -k -s \\\n        -X GET \\\n        -u ${ADMIN_USER}:${ADMIN_PASSWORD} \\\n        -H 'accept: application/json' \\\n        \"https://${HARBOR_URL}/api/v2.0/projects?page=$page&amp;page_size=100\")\n    if [[ \"$response\" == \"[]\" ]]; then\n        break\n    fi\n    echo $response &gt;&gt;out/projects.json\n    page=$((page + 1))\ndone\n\necho \"Find projects list at out/projects.txt\"\ncat out/projects.json | jq -r '.[].name' &gt;out/projects.txt\n</code></pre>"},{"location":"harbor-api/#04-get-project-repositories-in-a-project","title":"04. Get Project Repositories in a Project","text":"<pre><code>project=$1\necho \"Get info for $project\"\nrm -rf out/$project\nmkdir -p out/$project\npage=1\nwhile :; do\n    response=$(curl -k -s \\\n        -X GET \\\n        -u ${ADMIN_USER}:${ADMIN_PASSWORD} \\\n        -H 'accept: application/json' \\\n        \"https://${HARBOR_URL}/api/v2.0/projects/${project}/repositories?page=$page&amp;page_size=100\")\n    if [[ \"$response\" == \"[]\" ]]; then\n        break\n    fi\n    echo $response &gt;&gt;out/$project/repositories.json\n    page=$((page + 1))\ndone\n\n# cat out/$project/repositories.json | jq .\n</code></pre>"},{"location":"harbor-api/#05-get-details-of-repository","title":"05. Get Details of Repository","text":"<pre><code>cat out/$1/repositories.json | jq -r '.[].name' &gt;out/repos.txt\n\nwhile IFS= read -r line; do\n    project=$(echo \"$line\" | cut -d'/' -f1)\n    repo=$(echo \"$line\" | cut -d'/' -f2-)\n    echo \"$project,$repo\" &gt;&gt;out/$1/repositories.csv\ndone &lt;out/repos.txt\n</code></pre>"},{"location":"harbor-api/#06-get-image-details-of-a-repository","title":"06. Get Image Details of a repository","text":"<pre><code>file=out/$1/repositories.csv\n\ngetRepoDetails() {\n    project=$1\n    repo=$2\n    rm -rf out/$project/$repo\n    mkdir -p out/$project/$repo\n    page=1\n    while :; do\n        URL=\"\"\"https://${HARBOR_URL}/api/v2.0/projects/${project}/repositories/${repo}/artifacts?page_size=100&amp;page=${page}\"\"\"\n        response=$(curl -k -s \\\n            -X GET \\\n            -u ${ADMIN_USER}:${ADMIN_PASSWORD} \\\n            -H 'accept: application/json' \\\n            $URL)\n        if [[ \"$response\" == \"[]\" ]]; then\n\n            break\n        fi\n        echo $response | jq . &gt;out/$project/$repo/response.json\n        cat out/$project/$repo/response.json | jq -r '.[] | \"\\(.addition_links.build_history.href),\\(.tags[0].name),\\(.size)\"' | sed 's,/additions/build_history,,g' | sed 's,/api/v2.0/projects/,,g' &gt;&gt;out/$HARBOR_URL-size.csv\n        cat out/$project/$repo/response.json | jq -r '.[] | \"\\(.addition_links.build_history.href),\\(.tags[0].name),\\(.size)\"' | sed 's,/additions/build_history,,g' | sed 's,/api/v2.0/projects/,,g' &gt;&gt;out/$project/$repo/size.csv\n        page=$((page + 1))\n    done\n}\n\nif [ -e \"$file\" ] &amp;&amp; [ -s \"$file\" ]; then\n    while IFS=, read -r project repo; do\n        echo \"Fetching for $project/$repo\"\n        getRepoDetails $project \"${repo//\\//%2F}\"\n    done &lt;\"$file\"\nelse\n    if [ ! -e \"$file\" ]; then\n        echo \"Error: $file does not exist.\"\n    elif [ ! -s \"$file\" ]; then\n        echo \"Error: $file is empty.\"\n    fi\nfi\n</code></pre>"},{"location":"harbor-api/#07-create-a-remote-registry","title":"07. Create a remote registry","text":"<pre><code>url=REMOTE-Harbor.todo.com\naccess_key=TODO\naccess_secret=TODO\ncat&lt;&lt;EOF &gt;my-registry.json\n{\n  \"id\": 0,\n  \"url\": \"demo.goharbor.io\",\n  \"name\": \"goharbor\",\n  \"credential\": {\n    \"type\": \"basic\",\n    \"access_key\": \"$access_key\", \n    \"access_secret\": \"$access_secret\"\n  },\n  \"type\": \"harbor\",\n  \"insecure\": true,\n  \"description\": \"Demo harbor account\",\n  \"status\": \"string\",\n  \"creation_time\": \"2024-01-28T02:10:19.389Z\",\n  \"update_time\": \"2024-01-28T02:10:19.389Z\"\n}\nEOF\n\ncurl -k -s \\\n        -X POST \\\n        -u ${ADMIN_USER}:${ADMIN_PASSWORD} \\\n        -H 'accept: application/json' \\\n        -H 'Content-Type: application/json' \\\n        \"https://${HARBOR_URL}/api/v2.0/registries\" \\\n        -d @my-registry.json\n</code></pre>"},{"location":"harbor-api/#integrating-with-skopeo","title":"Integrating with Skopeo","text":"<pre><code>URL=&lt;url&gt;\nREGISTRY_ADMIN_USER=&lt;user-id&gt;\nREGISTRY_PASSWORD=&lt;token&gt;\nCRED=\"$REGISTRY_ADMIN_USER:$REGISTRY_PASSWORD\"\nout_file_name=$URL-image-tags-list.csv\nout_file_name_summary=$URL-repository-summary-report.csv\ntouch $out_file_name\ntouch $out_file_name_summary\necho Repository ,No of Tags,Latest Image Version, Latest Image Size &gt;$out_file_name_summary\necho URL,IMAGE_PATH,Tag,Image&gt; $out_file_name\npage=1\nwhile true;do\n    response=$(curl -ks -u $CRED  -H 'accept: application/json' \"https://$URL/api/v2.0/repositories?page=${page}&amp;page_size=100&amp;with_detail=true\")\n    for repository in $(echo $response |  jq -r '.[].name'); do\n        echo $repository \n        IMAGE_PATH=$repository \n        tags=$(docker run quay.io/skopeo/stable:latest list-tags docker://$URL/$IMAGE_PATH --tls-verify=false --creds=$CRED | jq -r '.Tags[]')\n        latest_image_tag=$(echo $tags | awk '{print $NF}')\n        no_of_Tags=$(echo $tags | wc | awk '{print $2}')\n        latest_image_size=$(docker run quay.io/skopeo/stable:latest inspect docker://$URL/$IMAGE_PATH:$latest_image_tag --tls-verify=false --creds=$CRED --raw | jq -r '.layers[] | .size' | paste -sd+ | bc | awk '{printf \"%.2f\\n\", $1 / 1024 / 1024 / 1024}')\n        echo $repository ,$no_of_Tags,$latest_image_tag,$latest_image_size &gt;&gt;$out_file_name_summary\n        for tag in $tags;do\n            image=$URL/$IMAGE_PATH:$tag\n            echo $image\n            echo $URL,$IMAGE_PATH,$tag,$image &gt;&gt;$out_file_name\n        done    \n    done\n     if [[ \"$response\" == \"[]\" ]]; then\n        break\n     fi    \n    page=$((page + 1))\ndone\n</code></pre>"},{"location":"install-karpenter/","title":"Setup Karpenter in the EKS cluster","text":""},{"location":"install-karpenter/#step-1-setup-environment","title":"Step 1. Setup Environment","text":"<pre><code>export KARPENTER_VERSION=v0.29.2 #todo change\n\n\nexport AWS_PARTITION=\"aws\"\nexport AWS_DEFAULT_REGION=\"us-east-1\"\nexport AWS_ACCOUNT_ID=\"$(aws sts get-caller-identity --query Account --output text)\"\nexport TEMPOUT=$(mktemp)\n\n\nexport CLUSTER_NAME=your-cluster-name\nexport CLUSTER_ENDPOINT=\"$(aws eks describe-cluster --name ${CLUSTER_NAME} --query \"cluster.endpoint\" --output text)\"\n\n# Check\necho KARPENTER_VERSION:$KARPENTER_VERSION\n\necho AWS_DEFAULT_REGION:$AWS_DEFAULT_REGION\necho AWS_ACCOUNT_ID:$AWS_ACCOUNT_ID\necho AWS_PARTITION:$AWS_PARTITION\necho TEMPOUT:$TEMPOUT\n\necho CLUSTER_NAME:$CLUSTER_NAME\necho CLUSTER_ENDPOINT:$CLUSTER_ENDPOINT\n</code></pre>"},{"location":"install-karpenter/#step-2-create-iam-roles-for-karpenter-and-the-karpenter-controller","title":"Step 2. Create IAM roles for Karpenter and the Karpenter controller","text":"<p>ref: https://docs.aws.amazon.com/eks/latest/userguide/create-node-role.html</p>"},{"location":"install-karpenter/#21-karpenterinstancenoderole","title":"2.1. KarpenterInstanceNodeRole","text":"<pre><code>AmazonEKSWorkerNodePolicy\nAmazonEKS_CNI_Policy\nAmazonEC2ContainerRegistryReadOnly\nAmazonSSMManagedInstanceCore\n</code></pre>"},{"location":"install-karpenter/#22-karpentercontrollerrole","title":"2.2 KarpenterControllerRole","text":""},{"location":"install-karpenter/#221-policy-file","title":"2.2.1 policy file","text":"<pre><code>echo '{\n    \"Statement\": [\n        {\n            \"Action\": [\n                \"ssm:GetParameter\",\n                \"iam:PassRole\",\n                \"ec2:DescribeImages\",\n                \"ec2:RunInstances\",\n                \"ec2:DescribeSubnets\",\n                \"ec2:DescribeSecurityGroups\",\n                \"ec2:DescribeLaunchTemplates\",\n                \"ec2:DescribeInstances\",\n                \"ec2:DescribeInstanceTypes\",\n                \"ec2:DescribeInstanceTypeOfferings\",\n                \"ec2:DescribeAvailabilityZones\",\n                \"ec2:DeleteLaunchTemplate\",\n                \"ec2:CreateTags\",\n                \"ec2:CreateLaunchTemplate\",\n                \"ec2:CreateFleet\",\n                \"ec2:DescribeSpotPriceHistory\",\n                \"pricing:GetProducts\"\n            ],\n            \"Effect\": \"Allow\",\n            \"Resource\": \"*\",\n            \"Sid\": \"Karpenter\"\n        },\n        {\n            \"Action\": \"ec2:TerminateInstances\",\n            \"Condition\": {\n                \"StringLike\": {\n                    \"ec2:ResourceTag/Name\": \"*karpenter*\"\n                }\n            },\n            \"Effect\": \"Allow\",\n            \"Resource\": \"*\",\n            \"Sid\": \"ConditionalEC2Termination\"\n        }\n    ],\n    \"Version\": \"2012-10-17\"\n}' &gt; controller-policy.json\n</code></pre>"},{"location":"install-karpenter/#22-apply-the-policy","title":"2.2. Apply the policy","text":"<pre><code>aws iam create-policy --policy-name KarpenterControllerPolicy-${CLUSTER_NAME} --policy-document file://controller-policy.json\n</code></pre>"},{"location":"install-karpenter/#23-create-an-iam-oidc-identity-provider-for-your-cluster-using-this-eksctl-command","title":"2.3. Create an IAM OIDC identity provider for your cluster using this eksctl command","text":"<p>ref: https://docs.aws.amazon.com/eks/latest/userguide/enable-iam-roles-for-service-accounts.html</p>"},{"location":"install-karpenter/#24-create-the-iam-role-for-karpenter-controller-using-the-eksctl-command-associate-the-kubernetes-service-account-and-the-iam-role-using-irsa","title":"2.4. Create the IAM role for Karpenter Controller using the eksctl command. Associate the Kubernetes Service Account and the IAM role using IRSA.","text":"<pre><code>eksctl create iamserviceaccount \\\n  --cluster \"${CLUSTER_NAME}\" --name karpenter --namespace karpenter \\\n  --role-name \"KarpenterControllerRole-${CLUSTER_NAME}\" \\\n  --attach-policy-arn \"arn:aws:iam::${AWS_ACCOUNT_ID}:policy/KarpenterControllerPolicy-${CLUSTER_NAME}\" \\\n  --role-only \\\n  --approve\n</code></pre>"},{"location":"install-karpenter/#step-3-add-tags-to-subnets-and-security-groups","title":"Step 3. Add tags to subnets and security groups","text":""},{"location":"install-karpenter/#31-add-tags-to-node-group-subnets-so-that-karpenter-knows-the-subnets-to-use","title":"3.1. Add tags to node group subnets so that Karpenter knows the subnets to use.","text":"<pre><code># Check\nfor NODEGROUP in $(aws eks list-nodegroups --cluster-name ${CLUSTER_NAME} \\\n    --query 'nodegroups' --output text); do echo aws ec2 create-tags \\\n        --tags \"Key=karpenter.sh/discovery,Value=${CLUSTER_NAME}\" \\\n        --resources $(aws eks describe-nodegroup --cluster-name ${CLUSTER_NAME} \\\n        --nodegroup-name $NODEGROUP --query 'nodegroup.subnets' --output text )\ndone\n\n# Apply\nfor NODEGROUP in $(aws eks list-nodegroups --cluster-name ${CLUSTER_NAME} \\\n    --query 'nodegroups' --output text); do aws ec2 create-tags \\\n        --tags \"Key=karpenter.sh/discovery,Value=${CLUSTER_NAME}\" \\\n        --resources $(aws eks describe-nodegroup --cluster-name ${CLUSTER_NAME} \\\n        --nodegroup-name $NODEGROUP --query 'nodegroup.subnets' --output text )\ndone\n</code></pre>"},{"location":"install-karpenter/#32-add-tags-to-security-groups","title":"3.2. Add tags to security groups.","text":"<p>The following commands add tags only to the security groups of the first node group. If you have multiple node groups or multiple security groups, you must decide the security group that Karpenter will use</p> <pre><code>NODEGROUP=$(aws eks list-nodegroups --cluster-name ${CLUSTER_NAME} --query 'nodegroups[0]' --output text)\n\nLAUNCH_TEMPLATE=$(aws eks describe-nodegroup --cluster-name ${CLUSTER_NAME} \\\n    --nodegroup-name ${NODEGROUP} --query 'nodegroup.launchTemplate.{id:id,version:version}' \\\n    --output text | tr -s \"\\t\" \",\")\n\n# If your EKS setup is configured to use only Cluster security group, then please execute -\n\nSECURITY_GROUPS=$(aws eks describe-cluster --name ${CLUSTER_NAME} --query cluster.resourcesVpcConfig.clusterSecurityGroupId | tr -d '\"')\n\n# If your setup uses the security groups in the Launch template of a managed node group, then :\n\nSECURITY_GROUPS=$(aws ec2 describe-launch-template-versions \\\n    --launch-template-id ${LAUNCH_TEMPLATE%,*} --versions ${LAUNCH_TEMPLATE#*,} \\\n    --query 'LaunchTemplateVersions[0].LaunchTemplateData.[NetworkInterfaces[0].Groups||SecurityGroupIds]' \\\n    --output text)\n\naws ec2 create-tags \\\n    --tags \"Key=karpenter.sh/discovery,Value=${CLUSTER_NAME}\" \\\n    --resources ${SECURITY_GROUPS}\n</code></pre>"},{"location":"install-karpenter/#step-4-update-aws-auth-configmap","title":"Step 4. Update aws-auth ConfigMap","text":""},{"location":"install-karpenter/#41-update-the-aws-auth-configmap-in-the-cluster-to-allow-the-nodes-that-use-the-karpenterinstancenoderole-iam-role-to-join-the-cluster-run-the-following-command","title":"4.1. Update the aws-auth ConfigMap in the cluster to allow the nodes that use the KarpenterInstanceNodeRole IAM role to join the cluster. Run the following command:","text":"<pre><code>kubectl edit configmap aws-auth -n kube-system\n</code></pre>"},{"location":"install-karpenter/#42-add-a-section-to-maproles-that-looks-similar-to-this-example","title":"4.2. Add a section to mapRoles that looks similar to this example:","text":"<p>Note: Replace the ${AWS_ACCOUNT_ID} variable with your account, but don't replace {{EC2PrivateDNSName}}.</p> <pre><code>- groups:\n  - system:bootstrappers\n  - system:nodes\n  rolearn: arn:aws:iam::${AWS_ACCOUNT_ID}:role/KarpenterInstanceNodeRole\n  username: system:node:{{EC2PrivateDNSName}}\n</code></pre>"},{"location":"install-karpenter/#step-5-install-karpenter-set-nodeaffinity","title":"Step 5. Install Karpenter , Set nodeAffinity","text":"<pre><code>helm template karpenter oci://public.ecr.aws/karpenter/karpenter --version ${KARPENTER_VERSION} --namespace karpenter \\\n    --set settings.aws.defaultInstanceProfile=KarpenterInstanceProfile \\\n    --set settings.aws.clusterEndpoint=\"${CLUSTER_ENDPOINT}\" \\\n    --set settings.aws.clusterName=${CLUSTER_NAME} \\\n    --set serviceAccount.annotations.\"eks\\.amazonaws\\.com/role-arn\"=\"arn:aws:iam::${AWS_ACCOUNT_ID}:role/KarpenterControllerRole-${CLUSTER_NAME}\" &gt; karpenter.yaml\n\n# Set the node affinity\naffinity:\n  nodeAffinity:\n    requiredDuringSchedulingIgnoredDuringExecution:\n      nodeSelectorTerms:\n      - matchExpressions:\n        - key: karpenter.sh/provisioner-name\n          operator: DoesNotExist\n      - matchExpressions:\n        - key: eks.amazonaws.com/nodegroup\n          operator: In\n          values:\n          - ${NODEGROUP}\n\n\nkubectl create namespace karpenter\nkubectl create -f https://raw.githubusercontent.com/aws/karpenter/${KARPENTER_VERSION}/pkg/apis/crds/karpenter.sh_provisioners.yaml\nkubectl create -f https://raw.githubusercontent.com/aws/karpenter/${KARPENTER_VERSION}/pkg/apis/crds/karpenter.k8s.aws_awsnodetemplates.yaml\nkubectl apply -f karpenter.yaml\n</code></pre>"},{"location":"install-karpenter/#step-6-create-sample-provisioner-awsnodetemplate","title":"Step 6. Create sample Provisioner &amp; AWSNodeTemplate","text":"<pre><code>cat &lt;&lt;EOF | kubectl apply -f -\napiVersion: karpenter.sh/v1alpha5\nkind: Provisioner\nmetadata:\n  name: default\nspec:\n  requirements:\n    - key: karpenter.k8s.aws/instance-category\n      operator: In\n      values: [c, m, r]\n    - key: karpenter.k8s.aws/instance-generation\n      operator: Gt\n      values: [\"2\"]\n  providerRef:\n    name: default\n  ttlSecondsAfterEmpty: 30\n---\napiVersion: karpenter.k8s.aws/v1alpha1\nkind: AWSNodeTemplate\nmetadata:\n  name: default\nspec:\n  subnetSelector:\n    karpenter.sh/discovery: \"${CLUSTER_NAME}\"\n  securityGroupSelector:\n    karpenter.sh/discovery: \"${CLUSTER_NAME}\"\nEOF\n</code></pre>"},{"location":"install-karpenter/#step-7-keep-reserved-nodes-for-the-critial-services","title":"Step 7. Keep reserved nodes for the critial services","text":"<pre><code>aws eks update-nodegroup-config --cluster-name ${CLUSTER_NAME} \\\n    --nodegroup-name ${NODEGROUP} \\\n    --scaling-config \"minSize=2,maxSize=2,desiredSize=2\"\n</code></pre>"},{"location":"install-karpenter/#step-8-check-the-karpenter-logs","title":"Step 8. Check the karpenter logs","text":"<pre><code>kubectl logs -f -n karpenter -c controller -l app.kubernetes.io/name=karpenter\n</code></pre> <p>ref:</p> <ul> <li>https://repost.aws/knowledge-center/eks-install-karpenter</li> <li>https://karpenter.sh/docs/getting-started/getting-started-with-karpenter/</li> </ul>"},{"location":"install-keycloak/","title":"Install keycloak","text":""},{"location":"install-keycloak/#setup-production-grade-keycloak-in-kubernetes1n","title":"Setup Production Grade Keycloak in Kubernetes(1/n)","text":"<p>Add authentication to applications and secure services with minimum effort.Keycloak provides user federation, strong authentication, user management, fine-grained authorization, and more.</p>"},{"location":"install-keycloak/#prerequisite","title":"prerequisite","text":"<ul> <li>KIND ClusterHost your test kubernetes cluster</li> <li>Ingress controller</li> <li>Storage Class</li> <li>Certificates Setting up your Own PKI with OpenSSL</li> </ul>"},{"location":"install-keycloak/#install-postgres","title":"Install postgres","text":"<pre><code>kubectl apply -f https://raw.githubusercontent.com/naren4b/nks/main/apps/postgres/pg-password.yaml\nkubectl apply -f https://raw.githubusercontent.com/naren4b/nks/main/apps/postgres/pgdb.yaml\n\nkubectl get pod\n</code></pre>"},{"location":"install-keycloak/#install-keycloak","title":"Install keycloak","text":"<pre><code>kubectl apply -f https://raw.githubusercontent.com/naren4b/nks/main/apps/keycloak/ingress.yaml\nkubectl apply -f https://raw.githubusercontent.com/naren4b/nks/main/apps/keycloak/keycloak-tls.yaml\nkubectl apply -f https://raw.githubusercontent.com/naren4b/nks/main/apps/keycloak/keycloak-deploy.yaml\nkubectl apply -f https://raw.githubusercontent.com/naren4b/nks/main/apps/keycloak/keycloak-svc.yaml\n\nkubectl get pod\nkubectl get ing\n</code></pre>"},{"location":"jira-report-python-2/","title":"Prepare Custom Report Using JIRA and PANDAS library of Python","text":"<p>https://github.com/naren4b/my_jira</p>"},{"location":"k8s-auto-scaler/","title":"K8s auto scaler","text":"<p>Horizontal Pod Autoscaler: a HorizontalPodAutoscaler automatically updates a workload resource (such as a Deployment or StatefulSet), with the aim of automatically scaling the workload to match demand.</p> <p>Cluster Autoscaler - a component that automatically adjusts the size of a Kubernetes Cluster so that all pods have a place to run and there are no unneeded nodes. Supports several public cloud providers. Version 1.0 (GA) was released with kubernetes 1.8.</p> <p>Vertical Pod Autoscaler - a set of components that automatically adjust the amount of CPU and memory requested by pods running in the Kubernetes Cluster. Current state - beta.</p> <p>reference: https://github.com/kubernetes/autoscaler/tree/master</p>"},{"location":"k8s-nks-pki-cert-manager/","title":"Setting up NKS PKI (Own Certificate via Cert-Manager in a Kubernetes cluster)","text":""},{"location":"k8s-nks-pki-cert-manager/#lets-have-kubernetes-cluster","title":"Let's have kubernetes cluster","text":"<pre><code>curl https://raw.githubusercontent.com/naren4b/dotfiles/main/ws/install.sh | bash\n</code></pre>"},{"location":"k8s-nks-pki-cert-manager/#ingress-controller","title":"Ingress controller","text":"<pre><code># Above Step does installs by default \nkubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/main/deploy/static/provider/kind/deploy.yaml\nkubectl label nodes controlplane ingress-ready=\"true\"\nkubectl wait --for=condition=ready pod -n ingress-nginx -l app.kubernetes.io/component=controller\n</code></pre>"},{"location":"k8s-nks-pki-cert-manager/#install-cert-manager","title":"Install cert-manager","text":"<p><pre><code>helm repo add jetstack https://charts.jetstack.io --force-update\nhelm repo update\nkubectl apply -f https://github.com/cert-manager/cert-manager/releases/download/v1.14.1/cert-manager.crds.yaml\nhelm install \\\n  cert-manager jetstack/cert-manager \\\n  --namespace cert-manager \\\n  --create-namespace \\\n  --version v1.14.1\nkubectl get pod -n cert-manager\n</code></pre> </p>"},{"location":"k8s-nks-pki-cert-manager/#create-your-root-ca","title":"Create your Root CA","text":"<pre><code>CA_NAME=\"NKS Certificate Authority\"\nopenssl genrsa -out ca.key 2048\nopenssl req -x509 -new -nodes -key ca.key -subj \"/CN=${CA_NAME}\" -days 10000 -out ca.crt\n\nkubectl create secret tls nks-pki-tls --cert=ca.crt --key=ca.key -n cert-manager\n</code></pre>"},{"location":"k8s-nks-pki-cert-manager/#install-clusterissuer","title":"Install ClusterIssuer","text":"<p><pre><code>cat&lt;&lt;EOF &gt; nks-pki-issuer.yaml\napiVersion: cert-manager.io/v1\nkind: ClusterIssuer\nmetadata:\n  name: nks-pki-issuer\nspec:\n  ca:\n    secretName: nks-pki-tls\nEOF\n\nkubectl apply -f nks-pki-issuer.yaml\n</code></pre> </p>"},{"location":"k8s-nks-pki-cert-manager/#install-application-and-certificates","title":"Install Application and Certificates","text":"<p><pre><code>kubectl create deployment echoserver --image k8s.gcr.io/echoserver:1.10\nkubectl expose deployment echoserver --port=8080\nkubectl create ingress echoserver   --class=nginx   --rule=\"echoserver.127.0.0.1.nip.io/*=echoserver:8080,tls=echoserver-ingress-tls\"\n\ncat&lt;&lt;EOF &gt; echoserver-certificate.yaml\napiVersion: cert-manager.io/v1\nkind: Certificate\nmetadata:\n  name:  echoserver-certificate\nspec:\n  isCA: false\n  duration: 2160h # 90d\n  renewBefore: 360h # 15d\n  commonName: echoserver.127.0.0.1.nip.io\n  dnsNames:\n  - echoserver.127.0.0.1.nip.io\n  - www.echoserver.127.0.0.1.nip.io\n  secretName: echoserver-ingress-tls\n  privateKey:\n    algorithm: RSA\n    encoding: PKCS1\n    size: 2048\n  issuerRef:\n    name: nks-pki-issuer\n    kind: ClusterIssuer\n    group: cert-manager.io\nEOF\nkubectl apply -f echoserver-certificate.yaml\n</code></pre> </p>"},{"location":"k8s-nks-pki-cert-manager/#test-the-certificate","title":"Test the certificate","text":"<p><pre><code>sudo openssl s_client -connect echoserver.127.0.0.1.nip.io:443 -showcerts &lt;/dev/null \n</code></pre> </p> <ul> <li>Demo Environment: https://killercoda.com/playgrounds/scenario/kubernetes</li> </ul>"},{"location":"k8s-resource-management/","title":"Kubernetes Resource Management","text":""},{"location":"k8s-resource-management/#resource-management-for-pods-and-containers","title":"Resource Management for Pods and Containers","text":""},{"location":"k8s-resource-management/#request","title":"Request:","text":"<p>When you specify the resource\u00a0request\u00a0for containers in a Pod, the\u00a0kube-scheduler\u00a0uses this information to decide which node to place the Pod on</p>"},{"location":"k8s-resource-management/#limit","title":"Limit:","text":"<p>When you specify a resource\u00a0limit\u00a0for a container, the\u00a0kubelet\u00a0enforces those limits so that the running container is not allowed to use more of that resource than the limit you set.</p> <p></p>"},{"location":"k8s-resource-management/#notes","title":"Notes:","text":"<p>If the node where a Pod is running has enough resources available, it is possible(and allowed) for a container to use more resource than it has request for. However, a container is not allowed to use more than its resource limit.</p>"},{"location":"k8s-resource-management/#cpu","title":"CPU:","text":"<ul> <li>Limits and requests for CPU resources are measured in cpu units. In Kubernetes, 1 CPU unit is equivalent to 1 physical CPU core, or 1 virtual core, depending on whether the node is a physical host or a virtual machine running inside a physical machine.</li> <li>If CPU limit reached, then it will be throttled</li> <li>Defined at container level</li> </ul>"},{"location":"k8s-resource-management/#memory","title":"Memory:","text":"<ul> <li>Limits and requests for memory are measured in bytes. You can express memory as a plain integer or as a fixed-point number using one of these quantity suffixes: E, P, T, G, M, k. You can also use the power-of-two equivalents: Ei, Pi, Ti, Gi, Mi, Ki. For example, the following represent roughly the same value.</li> <li>If memory limit reached, then it will be OOMKilled &amp; restarted</li> <li>Defined at container level</li> </ul>"},{"location":"k8s-resource-management/#kubernetes-checks-the-requestunallocatedask-while-scheduling","title":"Kubernetes checks the request(unallocated&lt;ask) while scheduling","text":""},{"location":"k8s-resource-management/#over-committed-state","title":"Over committed state","text":"<p>Limit for a pod can be set to any value(6Gi+6Gi) , but should not sum of total increase to node resource(8Gi) If actual consumption increases( 3+6=9Gi) which is more than node limit (8Gi) One of the pod /container will be killed This is an Over committed state</p> <p></p>"},{"location":"k8s-resource-management/#quality-of-serviceqos","title":"Quality of Service(QoS)","text":""},{"location":"k8s-resource-management/#guaranteed","title":"Guaranteed :","text":"<p>When both request and limit are defined and are equal Pods that are Guaranteed have the strictest resource limits and are least likely to face eviction. They are guaranteed not to be killed until they exceed their limits or there are no lower-priority Pods that can be preempted from the Node. They may not acquire resources beyond their specified limits. These Pods can also make use of exclusive CPUs using the static CPU management policy.</p>"},{"location":"k8s-resource-management/#burstable","title":"Burstable :","text":"<p>When both request and limit are defined and are NOT equal Pods that are Burstable have some lower-bound resource guarantees based on the request, but do not require a specific limit. If a limit is not specified, it defaults to a limit equivalent to the capacity of the Node, which allows the Pods to flexibly increase their resources if resources are available. In the event of Pod eviction due to Node resource pressure, these Pods are evicted only after all BestEffort Pods are evicted. Because a Burstable Pod can include a Container that has no resource limits or requests, a Pod that is Burstable can try to use any amount of node resources</p>"},{"location":"k8s-resource-management/#besteffort","title":"BestEffort:","text":"<p>When both request and limit are NOT defined Pods in the BestEffort QoS class can use node resources that aren't specifically assigned to Pods in other QoS classes The kubelet prefers to evict BestEffort Pods if the node comes under resource pressure</p>"},{"location":"k8s-resource-management/#limit-range","title":"Limit Range","text":"<p>A LimitRange is a policy to constrain the resource allocations (limits and requests) that you can specify for each applicable object kind (such as Pod or PersistentVolumeClaim) in a namespace.</p> <p>A LimitRange provides constraints that can: Enforce minimum and maximum compute resources usage per Pod or Container in a namespace. Enforce minimum and maximum storage request per PersistentVolumeClaim in a namespace. Enforce a ratio between request and limit for a resource in a namespace. Set default request/limit for compute resources in a namespace and automatically inject them to Containers at runtime.</p> <p>A LimitRange is enforced in a particular namespace when there is a LimitRange object in that namespace.</p>"},{"location":"k8s-resource-management/#resourcequota","title":"ResourceQuota","text":"<p>When several users or teams share a cluster with a fixed number of nodes, there is a concern that one team could use more than its fair share of resources. Resource quotas are a tool for administrators to address this concern. A resource quota, defined by a ResourceQuota object, provides constraints that limit aggregate resource consumption per namespace. It can limit the quantity of objects that can be created in a namespace by type, as well as the total amount of compute resources that may be consumed by resources in that namespace.</p> <p>Resource quotas work like this:</p> <p>Different teams work in different namespaces. This can be enforced with RBAC. The administrator creates one ResourceQuota for each namespace. Users create resources (pods, services, etc.) in the namespace, and the quota system tracks usage to ensure it does not exceed hard resource limits defined in a ResourceQuota. If creating or updating a resource violates a quota constraint, the request will fail with HTTP status code 403 FORBIDDEN with a message explaining the constraint that would have been violated. If quota is enabled in a namespace for compute resources like cpu and memory, users must specify requests or limits for those values; otherwise, the quota system may reject pod creation.</p> <p></p> <p>\ud83d\udca5Hint: Use the LimitRanger admission controller to force defaults for pods that make no compute resource requirements.</p> <p>references:</p> <ul> <li>https://kubernetes.io/docs/concepts/policy/resource-quotas/</li> <li>https://kubernetes.io/docs/tasks/configure-pod-container/quality-service-pod/</li> <li>https://kubernetes.io/docs/concepts/policy/limit-range/</li> <li>https://www.youtube.com/watch?v=MbgFIQoVh6w</li> </ul>"},{"location":"kafka-setup-2/","title":"Interacting with Strimzi Kafka cluster through Kafka Bridge(2/n)","text":"<p>Continuing from : (Deploying Basic Kafka cluster with the help of Strimzi operators (1/n))[https://naren4b.github.io/nks/docs/kafka-setup.html ]</p>"},{"location":"kafka-setup-2/#deploy-a-kafka-topic-topic-name-my-topic","title":"Deploy a Kafka Topic (topic name: my-topic)","text":"<pre><code>kubectl apply -n kafka -f https://raw.githubusercontent.com/strimzi/strimzi-kafka-operator/main/examples/topic/kafka-topic.yaml\n</code></pre>"},{"location":"kafka-setup-2/#deploy-a-kafka-bridge","title":"Deploy a Kafka Bridge","text":"<pre><code>kubectl apply -n kafka -f https://raw.githubusercontent.com/strimzi/strimzi-kafka-operator/main/examples/bridge/kafka-bridge.yaml\n</code></pre>"},{"location":"kafka-setup-2/#deploy-an-ingress","title":"Deploy an Ingress","text":"<pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: kafka-ingress-bridge\n  namespace: kafka\n  annotations:\n    nginx.ingress.kubernetes.io/rewrite-target: /\n    nginx.ingress.kubernetes.io/ssl-redirect: \"false\"\nspec:\n  rules:\n    - host: kafka-bridge.local.com\n      http:\n        paths:\n          - path: /\n            pathType: Prefix\n            backend:\n              service:\n                name: my-bridge-bridge-service\n                port:\n                  number: 8080\n</code></pre>"},{"location":"kafka-setup-2/#deploy-a-consumer","title":"Deploy a consumer","text":"<p>Notes: Run inside the cluster for the 'my-topic' Check the send messages are received  <pre><code>kubectl -n kafka run kafka-consumer -n kafka -ti --image=quay.io/strimzi/kafka:0.39.0-kafka-3.6.1 \\\n                                                     --rm=true --restart=Never -- bin/kafka-console-consumer.sh \\\n                                                     --bootstrap-server my-cluster-kafka-bootstrap:9092 \\\n                                                     --topic my-topic --from-beginning\n</code></pre></p>"},{"location":"kafka-setup-2/#verifying-the-setup","title":"Verifying the setup","text":"<p>Notes: Check the ingress(kafka-bridge.local.com) is accessiable  let's post messages to our topic : 'my-topic'</p> <pre><code> curl -X POST   http://kafka-bridge.local.com/topics/my-topic   -H 'content-type: application/vnd.kafka.json.v2+json'   -d '{\n    \"records\": [\n        {\n            \"key\": \"naren4b-key\",\n            \"value\": \"hello from naren4b 0001\"\n        }\n    ]\n}'\n</code></pre> <p> </p> <p>ref: https://strimzi.io/docs/bridge/latest/#proc-producing-messages-from-bridge-topics-partitions-bridge</p>"},{"location":"kafka-setup/","title":"Deploying Basic Kafka cluster with the help of Strimzi operators (1/n)","text":""},{"location":"kafka-setup/#apache-zookeeper","title":"Apache ZooKeeper","text":"<p>Apache ZooKeeper is a core dependency for Kafka as it provides a cluster coordination service, storing and tracking the status of brokers and consumers. ZooKeeper is also used for controller election.</p>"},{"location":"kafka-setup/#kafka-connect","title":"Kafka Connect","text":"<p>Kafka Connect is an integration toolkit for streaming data between Kafka brokers and other systems using Connector plugins. Kafka Connect provides a framework for integrating Kafka with an external data source or target, such as a database, for import or export of data using connectors. Connectors are plugins that provide the connection configuration needed.   1. A source connector pushes external data into Kafka.   2. A sink connector extracts data out of Kafka   3. External data is translated and transformed into the appropriate format.</p>"},{"location":"kafka-setup/#kafka-mirrormaker","title":"Kafka MirrorMaker","text":"<p>Kafka MirrorMaker replicates data between two Kafka clusters, within or across data centers. MirrorMaker takes messages from a source Kafka cluster and writes them to a target Kafka cluster.</p>"},{"location":"kafka-setup/#kafka-bridge","title":"Kafka Bridge","text":"<p>Kafka Bridge provides an API for integrating HTTP-based clients with a Kafka cluster.</p>"},{"location":"kafka-setup/#kafka-exporter","title":"Kafka Exporter","text":"<p>Kafka Exporter extracts data for analysis as Prometheus metrics, primarily data relating to offsets, consumer groups, consumer lag and topics. Consumer lag is the delay between the last message written to a partition and the message currently being picked up from that partition by a consumer</p>"},{"location":"kafka-setup/#prerequisite","title":"prerequisite","text":"<ul> <li>Killercoda play ground : https://killercoda.com/killer-shell-ckad/scenario/playground or </li> <li>Kind cluster : https://kind.sigs.k8s.io/docs/user/quick-start/#creating-a-cluster</li> </ul> <pre><code># Validate docker installation\ndocker ps\ndocker version\n\n# Validate kind\nkind version\n\n# Validate kubectl\nkubectl version\n</code></pre>"},{"location":"kafka-setup/#install-kafka","title":"Install kafka","text":""},{"location":"kafka-setup/#create-namespace","title":"create namespace","text":"<pre><code>kubectl create namespace kafka\n</code></pre>"},{"location":"kafka-setup/#kafka-operaor-clusterroles-clusterrolebindings-crd","title":"kafka Operaor ClusterRoles, ClusterRoleBindings CRD","text":"<pre><code>kubectl create -f 'https://strimzi.io/install/latest?namespace=kafka' -n kafka\nkubectl get pod -n kafka -w\nkubectl logs deployment/strimzi-cluster-operator -n kafka -f\n</code></pre>"},{"location":"kafka-setup/#install-single-node-kafka-cluster","title":"Install Single node kafka cluster","text":"<pre><code>kubectl apply -f https://strimzi.io/examples/latest/kafka/kafka-persistent-single.yaml -n kafka \nkubectl get pod -n kafka \nkubectl get pod -n kafka -w\nkubectl wait kafka/my-cluster --for=condition=Ready --timeout=300s -n kafka \n</code></pre>"},{"location":"kafka-setup/#send-messages","title":"Send Messages","text":"<pre><code>kubectl -n kafka run kafka-producer -ti --image=quay.io/strimzi/kafka:0.39.0-kafka-3.6.1 --rm=true --restart=Never -- bin/kafka-console-producer.sh --bootstrap-server my-cluster-kafka-bootstrap:9092 --topic my-topic\n</code></pre>"},{"location":"kafka-setup/#receive-messages","title":"Receive Messages","text":"<pre><code>kubectl -n kafka run kafka-consumer -ti --image=quay.io/strimzi/kafka:0.39.0-kafka-3.6.1 --rm=true --restart=Never -- bin/kafka-console-consumer.sh --bootstrap-server my-cluster-kafka-bootstrap:9092 --topic my-topic --from-beginning\n</code></pre>"},{"location":"kafka-setup/#clean-up","title":"Clean up","text":"<p><pre><code>kubectl -n kafka delete $(kubectl get strimzi -o name -n kafka)\nkubectl -n kafka delete -f 'https://strimzi.io/install/latest?namespace=kafka'\n</code></pre> ref:  - https://strimzi.io/docs/operators/latest/overview - https://strimzi.io/quickstarts/</p>"},{"location":"kafka/","title":"Kafka","text":"# Title Why Kafka (Problem) When to Use It How to Apply It What Kafka Concept Solves It Example Scenario 1 Explosion of Data Volume Need to handle billions of events at scale When your system ingests continuous, massive streams Scale horizontally with partitions Topics + Partitions LinkedIn handling profile views, job clicks, ad impressions 2 Multiple Consumers, One Data Source Multiple teams need the same data When analytics, fraud detection, and reporting all need identical streams Use independent consumer groups Consumer Groups Payment service \u2192 Fraud team + Analytics + Data Warehouse 3 Replay for Debugging &amp; ML Debugging &amp; ML need data replay When you need to reprocess past data Enable log retention &amp; control offsets Offsets + Retention Replaying 1 year of user clicks to re-train an ML model 4 One Pipeline for Real-time + Batch Must support both real-time + batch When you want one pipeline for dashboards + ETL Mix stream consumers &amp; batch consumers (via Connect) Kafka Connect + Stream APIs E-commerce system: live inventory + nightly sales reports 5 Fault Tolerance by Design Prevent data loss on failures When uptime and durability are mission-critical Replicate partitions across brokers Replication + Leader/Follower Bank processing transactions with 0 tolerance for loss 6 Decoupling Producers &amp; Consumers Avoid producer-consumer coupling When new consumers must join without disturbing producers Producers write once, consumers subscribe freely Decoupled Pub/Sub IoT sensor data \u2192 analytics, monitoring, ML pipelines 7 Guaranteed Event Ordering Need event ordering When events per user/session must stay in order Key-based partitioning Partition + Ordering Guarantee Chat app \u2192 messages for same user land in order 8 Stream Processing in Motion Process streams in motion When you must enrich/filter/aggregate data on the fly Use Kafka Streams or KSQL Stream Processing Ride-sharing: calculate ETA with live traffic data 9 Plug-and-Play Integrations Integrate with existing systems When data must flow to DBs, cloud storage, or other clusters Use Kafka Connect connectors Connectors Push logs to Elastic, load sales into Snowflake 10 Global Data Distribution Build geo-distributed systems When you need multi-DC/cloud active-active setups Replicate topics across clusters MirrorMaker / Cluster Linking Global SaaS syncing events across regions"},{"location":"kubernetes-adduser/","title":"Configuring User Access to Your Kubernetes Cluster: A Step-by-Step Guide","text":"<p>In this guide, I'll walk you through the essential steps for configuring user access to your Kubernetes cluster. Whether you're managing a single-node cluster or a multi-cluster environment, ensuring secure and fine-grained access for your team members is crucial. We'll cover everything from creating user accounts and generating client certificates to setting RBAC (Role-Based Access Control) policies and implementing authentication mechanisms. By the end of this tutorial, you'll be able to grant and manage user access, enhancing the security and productivity of your Kubernetes deployments.</p>"},{"location":"kubernetes-adduser/#csr-k8s-openssl-user-kubernetes-rbac","title":"csr #k8s #openssl #user #kubernetes #rbac","text":"<pre><code>username=\"${1:-my-user}\"\ngroup=\"${2:-edit}\"\nexpirationSeconds=\"${3:-86400}\"\nmkdir ${username}\ncd ${username}\n</code></pre>"},{"location":"kubernetes-adduser/#generate-a-key","title":"Generate a key","text":"<pre><code>openssl genrsa -out ${username}.key 2048\n</code></pre>"},{"location":"kubernetes-adduser/#for-git-for-windows","title":"For git-for-windows","text":"<pre><code>export MSYS_NO_PATHCONV=1\n</code></pre>"},{"location":"kubernetes-adduser/#create-the-csr","title":"Create the CSR","text":"<pre><code>openssl req -new -key ${username}.key -out ${username}.csr -subj \"/CN=${username}/O=${group}\"\n</code></pre>"},{"location":"kubernetes-adduser/#verify-the-csr","title":"Verify the CSR","text":"<pre><code>openssl req -in ${username}.csr -noout -text\n</code></pre>"},{"location":"kubernetes-adduser/#create-the-certificatesigningrequest","title":"Create the CertificateSigningRequest","text":"<pre><code>cat &lt;&lt;EOF &gt;${username}.yaml\napiVersion: certificates.k8s.io/v1\nkind: CertificateSigningRequest\nmetadata:\n  name: ${username}\nspec:\n  request: $(cat ${username}.csr | base64 | tr -d '\\n')\n  signerName: kubernetes.io/kube-apiserver-client\n  expirationSeconds: $expirationSeconds\n  usages:\n  - client auth\nEOF\n</code></pre>"},{"location":"kubernetes-adduser/#check-all-files-are-available","title":"Check all files are available","text":""},{"location":"kubernetes-adduser/#approve-the-csr","title":"Approve the CSR","text":""},{"location":"kubernetes-adduser/#connect-to-your-cluster","title":"Connect to your cluster","text":"<pre><code>if kubectl config current-context &amp;&gt;/dev/null; then\n    current_context=$(kubectl config current-context)\n    echo \"Connected to Kubernetes cluster using kubeconfig: $kubeconfig_path\"\n    echo \"Current context: $current_context\"\nelse\n    echo \"Not connected to a Kubernetes cluster using kubeconfig: $kubeconfig_path\"\n    exit 1\nfi\n\nif [ ! -e ${username}.yaml ]; then\n    echo \"CertificateSigningRequest for ${username}.yaml file does not exist.\"\nfi\n</code></pre>"},{"location":"kubernetes-adduser/#apply-and-approve-the-csr","title":"Apply and approve the CSR","text":"<pre><code>kubectl apply -f ${username}.yaml\nkubectl certificate approve ${username}\nkubectl get csr ${username}  -o jsonpath='{.status.certificate}'| base64 -d &gt; ${username}.crt\n</code></pre>"},{"location":"kubernetes-adduser/#set-up-the-user-kubeconfig","title":"Set up the User kubeconfig","text":"<pre><code>currentContext=$(kubectl config get-contexts | grep \"*\" | awk '{print $2}')\ncurrentCluster=$(kubectl config get-contexts | grep \"*\" | awk '{print $3}')\n\nkubectl config set-credentials ${username} --client-key=${username}.key --client-certificate=${username}.crt --embed-certs=true\nkubectl config set-context     ${username} --user=${username} --cluster=${currentCluster}\nkubectl config use-context     ${username}\nkubectl config view --raw --minify --flatten &gt; ${username}-kubeconfig\n\nkubectl config use-context     ${currentContext}\n</code></pre>"},{"location":"kubernetes-adduser/#setup-the-rbac-for-the-user","title":"Setup the RBAC for the user","text":"<pre><code>read -p \"Choose cluster role [admin, edit, view] \" role\n\necho \"This will add ${username} as a ${role} for all namespaces.\"\nread -p \"Proceed? [y/N] \" confirm\n\nif [[ \"${confirm}\" != \"y\" ]]; then\n  echo \"Aborting\"\n  exit 0\nfi\n\nkubectl create clusterrolebinding ${username}-${role} --user=${username} --clusterrole=${role}\necho list pod -- $(kubectl auth can-i list pod --as ${username})\necho create pod -- $(kubectl auth can-i create pod --as ${username})\necho delete pod -- $(kubectl auth can-i delete pod --as ${username})\n</code></pre>"},{"location":"kubernetes-adduser/#share-the-details-kubeconfig-file","title":"Share the details kubeconfig file","text":"<pre><code>ls -lrt\n</code></pre> <p>ref:</p> <ul> <li>add-user.sh</li> <li>Doc reference for kubernetes.io</li> <li>Demo cluster killercoda.com</li> </ul>"},{"location":"local-docker-registry/","title":"Setting up a secured local registry (local docker or k8s kind cluster)","text":""},{"location":"local-docker-registry/#prepare-the-certs","title":"Prepare the certs","text":"<pre><code>REGISTRY_URL=registry.nks.local\n\nrm -rf registry/\nCURRENT_PATH=${PWD}\nmkdir -p ${CURRENT_PATH}/registry/certs &amp;&amp; cd \"$_\"\nopenssl req -x509 -newkey rsa:4096 -days 365 -nodes -sha256             -keyout ${CURRENT_PATH}/registry/certs/tls.key             -out ${CURRENT_PATH}/registry/certs/tls.crt -subj \"/CN=$REGISTRY_URL\"             -addext \"subjectAltName = DNS:$REGISTRY_URL\"\ncd ..\n</code></pre>"},{"location":"local-docker-registry/#prepare-the-user-credentials","title":"Prepare the user credentials","text":"<pre><code>mkdir auth\ndocker run \\\n  --entrypoint htpasswd \\\n  httpd:2 -Bbn testuser testpassword &gt; auth/htpasswd\n</code></pre>"},{"location":"local-docker-registry/#to-setup-in-docker","title":"To setup in docker","text":"<pre><code>docker run -d \\\n  --restart=always \\\n  --name registry \\\n  -v \"$(pwd)\"/certs:/certs \\\n  -e REGISTRY_HTTP_ADDR=0.0.0.0:443 \\\n  -e REGISTRY_HTTP_TLS_CERTIFICATE=/certs/tls.crt \\\n  -e REGISTRY_HTTP_TLS_KEY=/certs/tls.key \\\n  -v \"$(pwd)\"/auth:/auth \\\n  -e \"REGISTRY_AUTH=htpasswd\" \\\n  -e \"REGISTRY_AUTH_HTPASSWD_REALM=Registry Realm\" \\\n  -e REGISTRY_AUTH_HTPASSWD_PATH=/auth/htpasswd \\\n  -p 443:443 \\\n  registry:2\n\n# https://docs.docker.com/registry/insecure/#deploy-a-plain-http-registry\n</code></pre>"},{"location":"local-docker-registry/#lets-check-the-client-part","title":"Let's check the client part","text":"<pre><code>mkdir -p /etc/docker/certs.d/$REGISTRY_URL\ncp certs/tls.crt  /etc/docker/certs.d/$REGISTRY_URL/ca.crt\ncp certs/tls.crt /usr/local/share/ca-certificates/$REGISTRY_URL.crt\necho \"127.0.0.1 $REGISTRY_URL\" &gt;&gt; /etc/hosts\n\necho $REGISTRY_URL\nvi /etc/docker/daemon.json\n\n{\n  \"insecure-registries\" : [\"registry.nks.local\"]\n}\n\nsystemctl restart docker\n</code></pre>"},{"location":"local-docker-registry/#lets-try-some-operation","title":"Let's try some operation","text":"<pre><code>systemctl restart docker\n\n\ndocker pull alpine:3.14\ndocker tag alpine:3.14 $REGISTRY_URL/my-alpine:3.14\n\nexport REGISTRY_AUTH_USER=testuser\nexport REGISTRY_AUTH_PASSWORD=testpassword\ndocker login $REGISTRY_URL -u $REGISTRY_AUTH_USER -p $REGISTRY_AUTH_PASSWORD\n\ndocker push $REGISTRY_URL/my-alpine:3.14\ndocker pull $REGISTRY_URL/my-alpine:3.14\n</code></pre>"},{"location":"local-docker-registry/#lets-do-some-more-images-that-needs-to-be-pushed-to-local-registry","title":"Let's do some more images that needs to be pushed to local-registry","text":"<pre><code>cat &gt; images.txt &lt;&lt;EOF\nghcr.io/siderolabs/flannel:v0.19.2\nEOF\n</code></pre>"},{"location":"local-docker-registry/#push-the-images","title":"Push the images","text":"<pre><code>for image in `cat images.txt`; do docker pull $image; done\n\nfor image in `cat images.txt`; do \\\n    docker tag $image `echo $image | sed -E 's#^[^/]+/#registry.nks.local/#'`; \\\n  done\n\nfor image in `cat images.txt`; do \\\n    docker push `echo $image | sed -E 's#^[^/]+/#registry.nks.local/#'`; \\\n  done\n</code></pre>"},{"location":"local-docker-registry/#check","title":"Check","text":"<pre><code>docker exec -it registry ls /var/lib/registry/docker/registry/v2/repositories\n</code></pre>"},{"location":"local-docker-registry/#setuping-up-same-in-a-kubernetes-cluster-kind","title":"Setuping up same in a kubernetes cluster (kind)","text":""},{"location":"local-docker-registry/#lets-setup-certficate-and-password-for-docker-pod-in-k8s","title":"Let's setup certficate and password for docker pod in k8s","text":"<pre><code>kubectl create secret tls certs-secret --cert=${CURRENT_PATH}/registry/certs/tls.crt --key=${CURRENT_PATH}/registry/certs/tls.key\nkubectl create secret tls certs-secret --cert=${CURRENT_PATH}/registry/certs/tls.crt --key=${CURRENT_PATH}/registry/certs/tls.key\nkubectl create secret generic auth-secret --from-file=${CURRENT_PATH}/registry/auth/htpasswd\n</code></pre>"},{"location":"local-docker-registry/#create-the-pv-registry-pvyaml","title":"Create the PV registry-pv.yaml","text":"<pre><code>cat &lt;&lt;EOF | kubectl apply -f -\napiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: docker-repo-pv\nspec:\n  capacity:\n    storage: 1Gi\n  accessModes:\n  - ReadWriteOnce\n  hostPath:\n    path: /tmp/repository\nEOF\n</code></pre>"},{"location":"local-docker-registry/#create-your-pvc-registry-pvcyaml","title":"Create your PVC registry-pvc.yaml","text":"<pre><code>cat &lt;&lt;EOF | kubectl apply -f -\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: docker-repo-pvc\nspec:\n  accessModes:\n  - ReadWriteOnce\n  resources:\n    requests:\n      storage: 1Gi\nEOF\n</code></pre>"},{"location":"local-docker-registry/#create-the-pod-registry-podyaml","title":"Create the pod registry-pod.yaml","text":"<pre><code>cat &lt;&lt;EOF | kubectl apply -f -\napiVersion: v1\nkind: Pod\nmetadata:\n  name: docker-registry-pod\n  labels:\n    app: registry\nspec:\n  containers:\n  - name: registry\n    image: registry:2.6.2\n    volumeMounts:\n    - name: repo-vol\n      mountPath: \"/var/lib/registry\"\n    - name: certs-vol\n      mountPath: \"/certs\"\n      readOnly: true\n    - name: auth-vol\n      mountPath: \"/auth\"\n      readOnly: true\n    env:\n    - name: REGISTRY_AUTH\n      value: \"htpasswd\"\n    - name: REGISTRY_AUTH_HTPASSWD_REALM\n      value: \"Registry Realm\"\n    - name: REGISTRY_AUTH_HTPASSWD_PATH\n      value: \"/auth/htpasswd\"\n    - name: REGISTRY_HTTP_TLS_CERTIFICATE\n      value: \"/certs/tls.crt\"\n    - name: REGISTRY_HTTP_TLS_KEY\n      value: \"/certs/tls.key\"\n  volumes:\n  - name: repo-vol\n    persistentVolumeClaim:\n      claimName: docker-repo-pvc\n  - name: certs-vol\n    secret:\n      secretName: certs-secret\n  - name: auth-vol\n    secret:\n      secretName: auth-secret\nEOF\n</code></pre>"},{"location":"local-docker-registry/#check-the-pod-service-and-files","title":"Check the pod, service and files","text":"<pre><code>kubectl get pod,svc\nkubectl port-forward docker-registry-pod 5000 --address 0.0.0.0\n</code></pre> <p>Demo at : https://killercoda.com/killer-shell-cks/scenario/container-namespaces-docker</p>"},{"location":"multiple-kind-k8s-cluster/","title":"\ud83d\udc2cStep-by-Step Guide to Setting Up Multiple Kind Clusters on a Single Host \ud83d\udc2c","text":""},{"location":"multiple-kind-k8s-cluster/#guide-to-creating-kubernetes-clusters-using-kind-and-podman","title":"Guide to Creating Kubernetes Clusters Using Kind and Podman","text":""},{"location":"multiple-kind-k8s-cluster/#setup-the-host","title":"Setup the host","text":"<p>kind version: v0.24.0 kubernetes Version: v1.31.0</p> <p>Install KIND <pre><code>[ $(uname -m) = x86_64 ] &amp;&amp; curl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.24.0/kind-linux-amd64\nchmod +x ./kind\nsudo mv ./kind /usr/local/bin/kind\n</code></pre> Pre Load the image <pre><code>podman  pull docker.io/kindest/node:v1.31.0@sha256:53df588e04085fd41ae12de0c3fe4c72f7013bba32a20e7325357a1ac94ba865\n</code></pre> - ref: https://github.com/kubernetes-sigs/kind/tags - ref: https://kind.sigs.k8s.io/docs/user/quick-start/</p>"},{"location":"multiple-kind-k8s-cluster/#install-the-org-control-plane-kind-cluster","title":"Install the org-control-plane kind cluster","text":"<p><pre><code>export KIND_EXPERIMENTAL_PROVIDER=podman\nexport ORG_CONTROL_PLANE_K8S=org\nclusterName=$ORG_CONTROL_PLANE_K8S\nkind delete cluster --name=${clusterName}\n\napiServerPort=6443\ncat &lt;&lt; EOF &gt; ${clusterName}-cluster-config.yaml\nkind: Cluster\napiVersion: kind.x-k8s.io/v1alpha4\nname: ${clusterName}\nnetworking:\n apiServerAddress: \"0.0.0.0\"\n apiServerPort: $apiServerPort\nnodes:\n- role: control-plane\nEOF\n\nkind create cluster --config ${clusterName}-cluster-config.yaml --kubeconfig ./kubeconfig\ncat kubeconfig | sed \"s|https://:${apiServerPort}|https://0.0.0.0:${apiServerPort}|g\"  &gt; ./config\nkind get kubeconfig --name=${clusterName} | sed \"s|https://:${apiServerPort}|https://${clusterName}-control-plane:6443|g\"  &gt; ${clusterName}-config\n</code></pre> </p>"},{"location":"multiple-kind-k8s-cluster/#setup-the-edge-k8s-clusters","title":"Setup the EDGE k8s clusters","text":"<pre><code>clusterName=edge-1 #Change me\nkind delete cluster --name=${clusterName}\napiServerPort=6444 # change me\ncat &lt;&lt; EOF &gt; ${clusterName}-cluster-config.yaml\nkind: Cluster\napiVersion: kind.x-k8s.io/v1alpha4\nname: ${clusterName}\nnetworking:\n apiServerAddress: \"0.0.0.0\"\n apiServerPort: $apiServerPort\nnodes:\n- role: control-plane\nEOF\n\nkind create cluster --config ${clusterName}-cluster-config.yaml --kubeconfig ./kubeconfig\ncat kubeconfig | sed \"s|https://:${apiServerPort}|https://0.0.0.0:${apiServerPort}|g\"  &gt; ./config\nkind get kubeconfig --name=${clusterName} | sed \"s|https://:${apiServerPort}|https://${clusterName}-control-plane:6443|g\"  &gt; ${clusterName}-config\n</code></pre>"},{"location":"multiple-kind-k8s-cluster/#copy-the-kubeconfig","title":"Copy the kubeconfig","text":"<p><pre><code>kubectl config use-context kind-$ORG_CONTROL_PLANE_K8S\nkubectl run test --image=docker.io/alpine -- sleep infinte\nclusterName=edge-1\nkubectl  cp ${clusterName}-config test:/config\nkubectl exec -it test sh\napk add curl\ncurl -O https://s3.us-west-2.amazonaws.com/amazon-eks/1.28.3/2023-11-14/bin/linux/amd64/kubectl\nchmod +x ./kubectl\nmv kubectl /usr/local/bin\nkubectl cluster-info --kubeconfig=config\n</code></pre> </p>"},{"location":"mykindk8scluster/","title":"Mykindk8scluster","text":""},{"location":"mykindk8scluster/#create-the-certificates","title":"Create the Certificates","text":"<pre><code>NAME=konark\nCLUSTER_NAME=$NAME-cluster\nHOST=$NAME.local.com\nAPP_NAME=$NAME-app\nNAMESPACE=$NAME-demo\necho $CLUSTER_NAME, $HOST,$APP_NAME,$NAMESPACE\n\nmkdir ${APP_NAME}\ncd ${APP_NAME}\n</code></pre>"},{"location":"mykindk8scluster/#create-kind-cluster","title":"Create KIND cluster","text":"<pre><code>mkdir -p $NAME\ncd $NAME\ncat &gt; ${CLUSTER_NAME}-config.yaml &lt;&lt;EOF\nkind: Cluster\napiVersion: kind.x-k8s.io/v1alpha4\nname: ${CLUSTER_NAME}\nnodes:\n- role: control-plane\n  image: kindest/node:v1.22.2\n  extraPortMappings:\n  - containerPort: 80\n    hostPort: 80\n  - containerPort: 443\n    hostPort: 443\n  - containerPort: 1024\n    hostPort: 1024\n- role: worker\n  image: kindest/node:v1.22.2\n- role: worker\n  image: kindest/node:v1.22.2\n- role: worker\n  image: kindest/node:v1.22.2\nEOF\n\nkind create cluster --name ${CLUSTER_NAME} --config ${CLUSTER_NAME}-config.yaml\n\nkubectl cluster-info --context kind-${CLUSTER_NAME}\n\nkubectl label nodes ${CLUSTER_NAME}-control-plane ingress-ready=\"true\"\n</code></pre>"},{"location":"mykindk8scluster/#deploy-ingress-controller","title":"Deploy ingress controller","text":"<pre><code>kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/main/deploy/static/provider/kind/deploy.yaml\nkubectl get pod -n ingress-nginx\n</code></pre>"},{"location":"mykindk8scluster/#deploy-storage-class","title":"Deploy Storage class","text":"<pre><code>kubectl apply -f https://raw.githubusercontent.com/rancher/local-path-provisioner/master/deploy/local-path-storage.yaml\n# For tetsing\nkubectl create -f https://raw.githubusercontent.com/rancher/local-path-provisioner/master/examples/pvc/pvc.yaml\nkubectl create -f https://raw.githubusercontent.com/rancher/local-path-provisioner/master/examples/pod/pod.yaml\nkubectl get pv\nkubectl get pvc\nkubectl exec volume-test -- sh -c \"echo local-path-test &gt; /data/test\"\nkubectl exec volume-test -- sh cat /data/test\n\nssh &lt;kubernetes node ip&gt;\n$ ls /var/lib/rancher/k3s/storage/\n</code></pre>"},{"location":"mykindk8scluster/#build-a-sample-application","title":"Build a sample application","text":"<pre><code># Create an index page\n\ncat &gt; index.html &lt;&lt;EOF\n&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n&lt;body&gt;\n&lt;img src=\"https://github.com/naren4b/nks/assets/3488520/08e281f5-640e-4fe6-915d-6c11526b79b2\"  width=\"600\" height=\"500\"&gt;\n&lt;/body&gt;\n&lt;/html&gt;\nEOF\n\n# An Error Page\n\ncat &gt; error.html &lt;&lt;EOF\n&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n&lt;body&gt;\n&lt;p style=\"color:red;\"&gt;You don't have access to this page at ${HOST} &lt;/p&gt;\n&lt;/body&gt;\n&lt;/html&gt;\nEOF\n\n# Dockerfile to build the image\n\ncat &gt; Dockerfile &lt;&lt;EOF\nFROM nginx:alpine\nCOPY . /usr/share/nginx/html\nEOF\n\n# Build the image and load into kind cluster\n\ndocker build -t ${APP_NAME}:0.0.1 .\nkind load docker-image ${APP_NAME}:0.0.1 --name ${CLUSTER_NAME}\n</code></pre>"},{"location":"mykindk8scluster/#create-k8s-manifest-files","title":"Create k8s manifest files","text":"<pre><code># Create namespace\n\nkubectl create ns ${NAMESPACE}\n\ncat &gt; ${APP_NAME}.yaml &lt;&lt;EOF\nkind: Pod\napiVersion: v1\nmetadata:\n  name: ${APP_NAME}\n  labels:\n    app: ${APP_NAME}\nspec:\n  containers:\n  - name: ${APP_NAME}\n    image: ${APP_NAME}:0.0.1\n---\n# Create Service\nkind: Service\napiVersion: v1\nmetadata:\n  name: ${APP_NAME}\nspec:\n  selector:\n    app: ${APP_NAME}\n  ports:\n  - port: 80\n---\n\n# Create Ingress\n\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: ${APP_NAME}\nspec:\n  rules:\n  - host: ${HOST}\n    http:\n      paths:\n      - pathType: Prefix\n        path: \"/\"\n        backend:\n          service:\n            name: ${APP_NAME}\n            port:\n             number: 80\n  tls:\n  - hosts:\n      - ${HOST}\n    secretName: ${APP_NAME}-tls\n\nEOF\n</code></pre>"},{"location":"mykindk8scluster/#apply-the-manifest-file","title":"Apply the Manifest file","text":"<pre><code>k create -f ${APP_NAME}.yaml -n ${NAMESPACE}\n\n# Create the TLS Secret\ncd ..\nkubectl create secret generic ${APP_NAME}-tls --from-file=tls.crt=${HOST}.crt --from-file=tls.key=${HOST}.key --from-file=ca.crt=ca.crt -n ${NAMESPACE}\n\ncurl -ks https://$HOST\n</code></pre>"},{"location":"mykindk8scluster/#special-notes","title":"Special Notes","text":"<pre><code>echo \"127.0.0.1 ${HOST}\" &gt;&gt; /etc/hosts\nfor windows update the same line in \"C:\\Windows\\System32\\drivers\\etc\\hosts file\"\n</code></pre>"},{"location":"mykindk8scluster/#create-certificates-inside-a-container-optional","title":"Create Certificates inside a container (Optional)","text":"<pre><code>docker run -it --rm -e HOST=${HOST} -v ${HOME}:/root/ -v ${PWD}:/work -w /work --net host quay.io/jitesoft/alpine:3.17.1 sh\napk add openssl\nopenssl req -x509 -sha256 -newkey rsa:4096 -keyout ca.key -out ca.crt -days 356 -nodes -subj \"/CN=${HOST} Cert Authority\"\nopenssl req -new -newkey rsa:4096 -keyout ${HOST}.key -out ${HOST}.csr -nodes -subj \"/CN=${HOST}\"\nopenssl x509 -req -sha256 -days 365 -in ${HOST}.csr -CA ca.crt -CAkey ca.key -set_serial 01 -out ${HOST}.crt\n</code></pre>"},{"location":"mykindk8scluster/#delete-the-cluster","title":"Delete the cluster","text":"<pre><code>kind delete clusters ${CLUSTER_NAME}\n</code></pre>"},{"location":"openssl-certificate/","title":"Setting up your Own PKI with OpenSSL","text":""},{"location":"openssl-certificate/#generate-cakey-and-cacrt","title":"Generate ca.key and ca.crt","text":"<pre><code>CA_NAME=\"NKS Certificate Authority\"\nopenssl genrsa -out ca.key 2048\nopenssl req -x509 -new -nodes -key ca.key -subj \"/CN=${CA_NAME}\" -days 10000 -out ca.crt\n</code></pre>"},{"location":"openssl-certificate/#prepare-the-csrconf-repeat-for-each-certificate-name-cn","title":"Prepare the csr.conf (Repeat for each Certificate Name CN)","text":"<pre><code>CERT_NAME=\"My Server/Client\"\nNODE_IP=127.0.0.1\nDOMAIN=nks.in\n\ncat&lt;&lt; EOF &gt;csr.conf\n[ req ]\ndefault_bits = 2048\nprompt = no\ndefault_md = sha256\nreq_extensions = req_ext\ndistinguished_name = dn\n\n[ dn ]\nC = IN\nST = Karnataka\nL = Bangalore\nO = Naren4Biz\nOU = nks\nCN = $CERT_NAME\n\n[ req_ext ]\nsubjectAltName = @alt_names\n\n[ alt_names ]\nDNS.1 = kubernetes\nDNS.2 = kubernetes.default\nDNS.3 = kubernetes.default.svc\nDNS.4 = kubernetes.default.svc.cluster\nDNS.5 = kubernetes.default.svc.cluster.local\nDNS.6 = *.$NODE_IP.nip.io\nDNS.7 = $DOMAIN\nDNS.8 = *.$DOMAIN\nIP.1 = $NODE_IP\n\n\n[ v3_ext ]\nauthorityKeyIdentifier=keyid,issuer:always\nbasicConstraints=CA:FALSE\nkeyUsage=keyEncipherment,dataEncipherment,digitalSignature,nonRepudiation\nextendedKeyUsage=serverAuth,clientAuth\nsubjectAltName=@alt_names\nEOF\n</code></pre>"},{"location":"openssl-certificate/#generate-tls-key-tls-crt","title":"Generate tls Key &amp; tls crt","text":"<pre><code>openssl genrsa -out tls.key 2048\nopenssl req -new -key tls.key -out tls.csr -config csr.conf\nopenssl x509 -req -in tls.csr -CA ca.crt -CAkey ca.key     -CAcreateserial -out tls.crt -days 10000     -extensions v3_ext -extfile csr.conf -sha256\n</code></pre>"},{"location":"openssl-certificate/#view-the-certificate","title":"View the certificate","text":"<pre><code>openssl req  -noout -text -in ./tls.csr\nopenssl x509  -noout -text -in ./tls.crt\n</code></pre>"},{"location":"openssl-certificate/#distribution","title":"Distribution","text":"<pre><code>NAME=$1 # Name of the Server or client Certificate\ncat tls.crt ca.crt &gt; $NAME.crt\nmv tls.key $NAME.key\nrm tls.*\n</code></pre>"},{"location":"prepare-k8s-image-scanning-report/","title":"Check all running images's vulnerability &amp; size running in k8s cluster","text":""},{"location":"prepare-k8s-image-scanning-report/#collect-the-list-of-images","title":"Collect the list of images","text":"<pre><code>mkdir -p /tmp/k8s-image-list/\n  kubectl get pods --all-namespaces -o jsonpath='{range .items[*]}{\"\\n\"}{.metadata.namespace}{\"\\t\"}{range .spec.containers[*]}{.image}{\", \"}{end}{end}' |sort | uniq -c | awk '{print $2, $3, $4}' | tr \",\" \" done\" &gt; /tmp/k8s-image-list/k8s-images-details.txt\n</code></pre>"},{"location":"prepare-k8s-image-scanning-report/#segregate-the-images","title":"Segregate the images","text":"<pre><code>import os\n\ndef process_dc_image_txt_files(directory):\n    unique_images=set()\n    files = os.listdir(directory)\n    txt_files = [file for file in files if file.endswith('.txt')]\n    for txt_file in txt_files:\n        input_file = os.path.join(directory, txt_file)\n        print(\"Fetch for: \",input_file)\n        # namespace = collect_unique_ns(input_file)\n        # # print(\"Unique Namespaces:\")\n        # # print(\"------------------\")\n        # # for ns in set(namespace):\n        # #     print(ns)\n        # # print()\n\n        images = collect_images(input_file)\n        unique_images.update(images)\n    return unique_images\n\ndef collect_unique_ns(input_file):\n    all_words_except_first = []\n\n    # Read the file and collect all words except the first word\n    with open(input_file, 'r') as file:\n        for line in file:\n            words = line.strip().split()\n            if len(words) &gt; 1:\n                all_words_except_first.extend(words[0:1])\n\n    return all_words_except_first\n\ndef collect_images(input_file):\n\n    all_words_except_first = []\n\n    # Read the file and collect all words except the first word\n    with open(input_file, 'r') as file:\n        for line in file:\n            words = line.strip().split()\n            if len(words) &gt; 1:\n                all_words_except_first.extend(words[1:])\n\n    return all_words_except_first\n\ndef prepare_image_file(output_file, unique_images):\n    with open(output_file, 'w') as file:\n        for image in sorted(unique_images): \n             file.write(str(image) + '\\n')\n\n\nif __name__ == \"__main__\":\n    directory = \"/tmp/k8s-image-list\"  # Replace with your directory path\n    unique_images=process_dc_image_txt_files(directory)\n    output_file = \"images.txt\"\n    prepare_image_file(output_file,unique_images)\n    print(\"done\")\n</code></pre>"},{"location":"prepare-k8s-image-scanning-report/#scan-image-list","title":"Scan Image List","text":"<pre><code>#!/bin/bash\n\n###########################################################################\n# How to run ? bash scan_images.sh &lt;images-list-file-full-path&gt; &lt;metrics-prefix&gt;  #\n# example : bash scan_images.sh images.list demo                                  #\n# Author: Narendranath Panda                                              #\n###########################################################################\n\nimage_list_file=$1\nimage_list_file=${image_list_file:-images.txt}\n\nscan_version=$(date +\"%d-%m-%Y\")\nmkdir -p $scan_version\n\necho\n#prom_file=$scan_version/image_scanning_result.prom\n\nreports_dir=$scan_version/reports\nmkdir -p $reports_dir\n\nlogs_dir=$scan_version/logs\nmkdir -p $logs_dir\n\nmetrics_dir=$scan_version/metrics\nmkdir -p $metrics_dir\n\necho \"\" &gt;$logs_dir/failed-images.log\necho \"\" &gt;$logs_dir/images-exists.log\necho \"\" &gt;$logs_dir/processed-images.log\nmetric_prefix=$2\ndefault_value=\"my\"\nmetric_prefix=${metric_prefix:-$default_value}\nnumber=0\nif [ -e \"$image_list_file\" ]; then\n    echo \"Scanning starts...\"\n    for image_name in $(cat $image_list_file); do\n        ((number++))\n        echo $number\". \"$image_name\n        image_report=($(echo $image_name | tr \"/\" _ | tr \":\" _))\n        grype_file_name=$reports_dir/$image_report-grype.json\n        prom_file_name=$metrics_dir/$image_report.prom\n        if [ -e \"$grype_file_name\" ]; then\n            echo \"$image_name\" &gt;&gt;$logs_dir/images-exists.log\n        else\n            grype $image_name --quiet --add-cpes-if-none --output json &gt;grype.out 2&gt;&amp;1\n            if [ $? -ne 0 ]; then\n                echo \"$image_name\" &gt;&gt;$logs_dir/failed-images.log\n            else\n                mv grype.out $grype_file_name\n                H_vul=$(cat $grype_file_name | grep -i High | wc -l)\n                L_vul=$(cat $grype_file_name | grep -i Low | wc -l)\n                M_vul=$(cat $grype_file_name | grep -i Medium | wc -l)\n                C_vul=$(cat $grype_file_name | grep -i Critical | wc -l)\n                N_vul=$(cat $grype_file_name | grep -i Negligible | wc -l)\n                digests=$(cat $grype_file_name | jq '.source.target.repoDigests[0]')\n\n                docker images $image_name --digests --format \"{{json . }}\" &gt;$reports_dir/$image_report-docker.json\n                size=$(cat $reports_dir/$image_report-docker.json | jq -sc '.[] | {Repository, Size}' | jq -s 'map(. + {\"originalSize\": .Size})' | jq -r '\n                map(\n                    if .Size | endswith(\"GB\") then\n                        .Size |= (gsub(\"GB\"; \"\") | tonumber * 1024 * 1024 * 1024)\n                    else\n                        if .Size | endswith(\"MB\") then\n                            .Size |= (gsub(\"MB\"; \"\") | tonumber * 1024 * 1024)\n                        else\n                            if .Size | endswith(\"KB\") then\n                                .Size |= (gsub(\"KB\"; \"\") | tonumber * 1024)\n                            else\n                                .size | tonumber\n                            end\n                        end\n                    end\n                )' | jq .[0].Size)\n\n                repository=$(cat $reports_dir/$image_report-docker.json | jq '.Repository')\n                image_registry=$(cat $reports_dir/$image_report-docker.json |jq -r '.Repository' | awk -F'/' '{print $1}' )\n                tag=$(cat $reports_dir/$image_report-docker.json | jq '.Tag')\n                IMAGE_ID=$(cat $reports_dir/$image_report-docker.json | jq -r \".ID\")\n\n                echo \"${metric_prefix}_image_info{image_digest=$digests,image_name=\\\"$image_name\\\",image_registry=\\\"$image_registry\\\",image_repository=$repository,image_tag=$tag} 1\" &gt;&gt;$prom_file_name\n                echo \"${metric_prefix}_image_vulnerability_severity_count{image_digest=$digests,severity=\\\"Critical\\\"} $C_vul\" &gt;&gt;$prom_file_name\n                echo \"${metric_prefix}_image_vulnerability_severity_count{image_digest=$digests,severity=\\\"High\\\"} $H_vul\" &gt;&gt;$prom_file_name\n                echo \"${metric_prefix}_image_vulnerability_severity_count{image_digest=$digests,severity=\\\"Medium\\\"} $M_vul\" &gt;&gt;$prom_file_name\n                echo \"${metric_prefix}_image_vulnerability_severity_count{image_digest=$digests,severity=\\\"Low\\\"} $L_vul\" &gt;&gt;$prom_file_name\n                echo \"${metric_prefix}_image_vulnerability_severity_count{image_digest=$digests,severity=\\\"Negligible\\\"} $N_vul\" &gt;&gt;$prom_file_name\n                echo \"${metric_prefix}_image_size_in_bytes{image_digest=$digests} $size\" &gt;&gt;$prom_file_name\n                docker rmi $IMAGE_ID &gt;/dev/null 2&gt;&amp;1\n                echo image_id=$IMAGE_ID, image_digest=$digests,image_name=\\\"$image_name\\\",image_registry=\\\"$image_registry\\\",image_repository=$repository,image_tag=$tag &gt;&gt;$logs_dir/processed-images.log\n            fi\n        fi\n\n    done\n    echo\n    echo\n    ls -lrt $metrics_dir\n    echo\n    echo\n    echo \"Scanning done...\"\n    echo\n    echo\n    echo \"execute this command to copy to node_exporter/textfilecollector\"\n    echo \"cp 20-11-2023/metrics/*.prom /var/lib/node-exporter/textfile_collector/\"\n    echo\n\nelse\n    echo \"File '$image_list_file' does not exist in the directory.!\"\nfi\n</code></pre>"},{"location":"prepare-k8s-image-scanning-report/#visulaize-the-metrics","title":"Visulaize the metrics","text":"<ul> <li>Setting up Monitoring Stack in a Node (docker container)</li> <li>Node-exporter: http://localhost:9100/metrics</li> <li>Prometheus   : http://localhost:9090/graph?g0.expr=my_image_info&amp;g0.tab=1&amp;g0.stacked=0&amp;g0.show_exemplars=0&amp;g0.range_input=1h</li> <li>Grafana      : https://github.com/naren4b/nks/tree/main/apps/image-scanner </li> </ul>"},{"location":"private-helm-charts/","title":"Private helm charts","text":""},{"location":"private-helm-charts/#part-1-how-to-create-package-push-host-deploy-private-helm-charts-using-chartmuseum","title":"Part-1 How to create, package , push, host, deploy Private Helm Charts using Chartmuseum","text":""},{"location":"private-helm-charts/#1host-hosting-the-chartmuseum-via-docker","title":"1.host: Hosting the chartmuseum via Docker","text":"<pre><code>mkdir charts\nchmod 667 charts\ndocker run --rm -it -d -p 8080:8080   -e DEBUG=1   -e STORAGE=local   -e STORAGE_LOCAL_ROOTDIR=/charts   -v $(pwd)/charts:/charts ghcr.io/helm/chartmuseum:v0.14.0\n</code></pre>"},{"location":"private-helm-charts/#2create-at-developer-machine-creatingarranging-a-helm-templates-with-values","title":"2.create: at Developer Machine creating/arranging a helm templates with values","text":"<pre><code>git clone https://github.com/naren4b/helm-charts.git\ncd helm-charts/charts\nhelm create nks-web\ngit add -A\ngit commit -m \"nks-web chart added\"\ngit push\n</code></pre>"},{"location":"private-helm-charts/#3package-push-at-build-machine-packaging-and-pushing-the-helm-chart","title":"3.package &amp; push : at build machine packaging and pushing the helm chart","text":"<pre><code>helm plugin install https://github.com/chartmuseum/helm-push\ngit clone https://github.com/naren4b/helm-charts.git\ncd helm-charts/charts\nhelm package nks-web --version 1.0.0 --app-version 1.0.0\ncurl --data-binary \"@nks-web-1.0.0.tgz\" http://nks-charts:8080/api/charts\n//or\nhelm repo add nks http://nks-charts:8080\nhelm cm-push nks-web nks --version 0.1.0 --app-version 0.1.0\n</code></pre>"},{"location":"private-helm-charts/#4deploy-to-a-kind-demo-cluster","title":"4.deploy: to a kind demo cluster","text":"<pre><code>helm repo add nks-charts http://nks-charts:8080/\nhelm repo update\nhelm search repo nks-charts\nhelm install nks/nks-web --generate-name\n</code></pre>"},{"location":"private-helm-charts/#assumption","title":"Assumption","text":""},{"location":"private-helm-charts/#ref","title":"ref:","text":"<ul> <li>https://github.com/helm/chartmuseum/blob/main/README.md</li> <li>https://github.com/chartmuseum/helm-push</li> </ul>"},{"location":"project-kube-edge/","title":"Project 1: KubeEdge \u2013 Extending Kubernetes to the Edge","text":""},{"location":"project-kube-edge/#this-guide-will-cover","title":"This guide will cover:","text":"<ul> <li>\u2705 Introduction &amp; Purpose</li> <li>\u2705 Architecture &amp; Components</li> <li>\u2705 Installation &amp; Setup</li> <li>\u2705 Deploying Edge Applications</li> <li>\u2705 Device Management &amp; IoT Integration</li> <li>\u2705 Security Best Practices</li> <li>\u2705 Monitoring &amp; Troubleshooting</li> </ul>"},{"location":"project-kube-edge/#introduction","title":"\ud83e\udded Introduction","text":"<p>Kubernetes is powerful, but it was designed for cloud and data centers. KubeEdge extends Kubernetes to edge computing environments, allowing applications to run on edge nodes (e.g., IoT devices, industrial sensors, retail systems).</p>"},{"location":"project-kube-edge/#why-use-kubeedge","title":"Why Use KubeEdge?","text":"<ul> <li>\u2705 Brings Kubernetes to edge devices for real-time processing</li> <li>\u2705 Reduces cloud dependency and latency</li> <li>\u2705 Works offline \u2013 edge devices keep running even if disconnected</li> <li>\u2705 Seamless Kubernetes integration for managing edge workloads</li> </ul>"},{"location":"project-kube-edge/#use-cases","title":"Use Cases","text":"<ul> <li>\ud83d\udea6 Smart Cities: Traffic monitoring, environmental sensors</li> <li>\ud83c\udfed Industrial IoT: Machine data collection, predictive maintenance</li> <li>\ud83d\uded2 Retail: Smart checkout systems, in-store analytics</li> <li>\ud83c\udfe5 Healthcare: Remote patient monitoring</li> </ul>"},{"location":"project-kube-edge/#kubeedge-architecture","title":"\ud83e\uddf1 KubeEdge Architecture","text":"<pre><code>graph TD\n  subgraph Cloud\n    A[CloudCore]\n    B[Kubernetes API Server]\n    C[EdgeNode CRDs]\n  end\n  subgraph Edge\n    D[EdgeCore]\n    E[MQTT Broker]\n    F[IoT Devices]\n  end\n\n  A --&gt; D[Workload Sync]\n  A --&gt; D[Device Twin Sync]\n  D --&gt; A[Telemetry Data]\n  D --&gt; F[MQTT]\n  F --&gt; D[Sensor Data]\n</code></pre>"},{"location":"project-kube-edge/#cloud-side-cloudcore","title":"Cloud Side (CloudCore)","text":"<ul> <li>Runs in a Kubernetes cluster (public cloud, private data center)</li> <li>Manages edge nodes using custom CRDs (Custom Resource Definitions)</li> <li>Syncs workloads between cloud and edge</li> </ul>"},{"location":"project-kube-edge/#edge-side-edgecore","title":"Edge Side (EdgeCore)","text":"<ul> <li>Runs on edge devices (Raspberry Pi, industrial gateways, on-prem servers)</li> <li>Processes data locally to reduce cloud traffic</li> <li>Manages devices connected via Bluetooth, MQTT, or Modbus</li> <li><pre><code>graph TD\n  A[Kubernetes Cluster - CloudCore] --&gt;|Manages| B[Edge Node - EdgeCore]\n  B --&gt;|Connects to| C[IoT Devices Sensors/Actuators]\n  C --&gt;|Protocol| D(MQTT/Bluetooth/Modbus)\n</code></pre></li> </ul>"},{"location":"project-kube-edge/#installing-kubeedge","title":"\u2699\ufe0f Installing KubeEdge","text":""},{"location":"project-kube-edge/#step-1-install-kubernetes-on-the-cloud","title":"Step 1: Install Kubernetes on the Cloud","text":"<pre><code>curl -LO https://storage.googleapis.com/kubernetes-release/release/$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt)/bin/linux/amd64/kubectl\nchmod +x kubectl &amp;&amp; sudo mv kubectl /usr/local/bin/\nkubectl version --client\n</code></pre>"},{"location":"project-kube-edge/#step-2-install-cloudcore","title":"Step 2: Install CloudCore","text":"<pre><code>wget https://github.com/kubeedge/kubeedge/releases/download/v1.12.0/keadm-v1.12.0-linux-amd64.tar.gz\ntar -xvzf keadm-*.tar.gz &amp;&amp; sudo mv keadm /usr/local/bin/\nkeadm init --advertise-address=\"&lt;Cloud Public IP&gt;\"\nkubectl get pods -n kubeedge\n</code></pre>"},{"location":"project-kube-edge/#step-3-install-edgecore-on-edge-node","title":"Step 3: Install EdgeCore on Edge Node","text":"<pre><code>wget https://github.com/kubeedge/kubeedge/releases/download/v1.12.0/keadm-v1.12.0-linux-arm64.tar.gz\ntar -xvzf keadm-*.tar.gz &amp;&amp; sudo mv keadm /usr/local/bin/\nkeadm join --cloudcore-ip=&lt;Cloud Public IP&gt;\nkubectl get nodes\n</code></pre>"},{"location":"project-kube-edge/#deploying-applications-to-edge-nodes","title":"\ud83d\ude80 Deploying Applications to Edge Nodes","text":""},{"location":"project-kube-edge/#step-1-nginx-deployment-yaml","title":"Step 1: Nginx Deployment YAML","text":"<pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: edge-nginx\nspec:\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      nodeSelector:\n        node-role.kubernetes.io/edge: \"true\"\n      containers:\n        - name: nginx\n          image: nginx:latest\n          ports:\n            - containerPort: 80\n</code></pre>"},{"location":"project-kube-edge/#step-2-expose-via-nodeport","title":"Step 2: Expose via NodePort","text":"<pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: edge-nginx-service\nspec:\n  type: NodePort\n  selector:\n    app: nginx\n  ports:\n    - protocol: TCP\n      port: 80\n      nodePort: 30080\n</code></pre> <p>Access via: <code>http://&lt;EdgeNode_IP&gt;:30080</code></p>"},{"location":"project-kube-edge/#device-management-iot-integration","title":"\ud83d\udd17 Device Management &amp; IoT Integration","text":""},{"location":"project-kube-edge/#step-1-mqtt-broker-deployment","title":"Step 1: MQTT Broker Deployment","text":"<pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mosquitto\nspec:\n  selector:\n    matchLabels:\n      app: mosquitto\n  template:\n    metadata:\n      labels:\n        app: mosquitto\n    spec:\n      containers:\n        - name: mosquitto\n          image: eclipse-mosquitto:latest\n          ports:\n            - containerPort: 1883\n</code></pre>"},{"location":"project-kube-edge/#step-2-iot-device-publishing-via-mqtt","title":"Step 2: IoT Device Publishing via MQTT","text":"<pre><code>import paho.mqtt.client as mqtt\n\nclient = mqtt.Client()\nclient.connect(\"edge-node-ip\", 1883, 60)\nclient.publish(\"sensor/temperature\", \"23.5\")\n</code></pre>"},{"location":"project-kube-edge/#security-best-practices","title":"\ud83d\udd10 Security Best Practices","text":"<ul> <li>\u2705 Use Kubernetes RBAC to limit access to edge nodes</li> <li>\u2705 Enable TLS encryption for MQTT and API communication</li> <li>\u2705 Configure firewall rules to protect edge devices</li> <li>\u2705 Ensure secure device authentication using certificates</li> </ul> <pre><code>graph LR\n  RBAC[Kubernetes RBAC] --&gt; AccessControl[Limit Cloud Access]\n  TLS[TLS Encryption] --&gt; SecureComms[Encrypt MQTT and API Traffic]\n  FW[Firewall Rules] --&gt; BlockPorts[Restrict Unnecessary Traffic]\n  CertAuth[Cert Authentication] --&gt; SecureDevices[Mutual TLS for Devices]\n</code></pre>"},{"location":"project-kube-edge/#monitoring-troubleshooting","title":"\ud83d\udd0d Monitoring &amp; Troubleshooting","text":"<pre><code>graph TD\n  NodeHealth[Check Node Health] --&gt; NodeStatus[Node Ready]\n  CloudLogs[CloudCore Logs] --&gt; CloudDiagnostics[Inspect Logs]\n  EdgeLogs[EdgeCore Logs] --&gt; EdgeDiagnostics[Edge Troubleshooting]\n  MQTTMonitor[Monitor MQTT Topics] --&gt; IoTData[Monitor Sensor Data]\n</code></pre>"},{"location":"project-kube-edge/#1-check-edge-node-connectivity","title":"1. Check Edge Node Connectivity","text":"<pre><code>kubectl get nodes\n</code></pre>"},{"location":"project-kube-edge/#2-cloudcore-logs","title":"2. CloudCore Logs","text":"<pre><code>kubectl logs -n kubeedge -l app=cloudcore\n</code></pre>"},{"location":"project-kube-edge/#3-edgecore-logs-on-edge-node","title":"3. EdgeCore Logs (on Edge Node)","text":"<pre><code>journalctl -u edgecore -f\n</code></pre>"},{"location":"project-kube-edge/#4-monitor-mqtt-messages","title":"4. Monitor MQTT Messages","text":"<pre><code>mosquitto_sub -h edge-node-ip -t \"sensor/temperature\"\n</code></pre>"},{"location":"project-kube-edge/#final-thoughts","title":"\ud83e\udde0 Final Thoughts","text":"<p>KubeEdge brings Kubernetes to the edge, enabling: - \u2705 Offline edge computing (devices function without internet) - \u2705 Low-latency data processing at the source - \u2705 Scalability to thousands of edge nodes</p> <p>Ideal for: IoT, Smart Cities, Industrial Automation, Retail, and Healthcare.</p>"},{"location":"prometheus_pushgateway/","title":"Deploy a batch monitoring stack with Prometheus PushGateway in Kubernetes cluster","text":"<p>The Prometheus PushGateway serves as an invaluable tool for managing batch metrics, enabling the integration of Internet of Things (IoT) sensor data into Prometheus. This guide illustrates the process of configuring a PushGateway instance and recording metrics from a basic BASH script.</p> <p>It presents a straightforward approach for system administrators to visualize data external to Kubernetes.</p>"},{"location":"prometheus_pushgateway/#the-setup-comprises-several-core-applications-and-jobs-responsible-for-fetching-the-data","title":"The setup comprises several core applications and jobs responsible for fetching the data:","text":"<ol> <li>Grafana: serves as a robust visualization tool utilized to showcase our metrics. It functions as the 'frontend' of our system.</li> <li>Prometheus: operates as a highly scalable time-series database, serving as the 'backend' of our setup. It is typically configured to regularly scrape metrics data from applications.</li> <li>PushGateway: acts as a 'sink' or 'buffer' for metric data that has a short lifespan, making it unsuitable for Prometheus scraping. This is where our cron jobs will log data, as the containers do not persist long enough for Prometheus to capture them.</li> </ol>"},{"location":"prometheus_pushgateway/#cluster-setup","title":"Cluster Setup","text":"<p>Let's use a Killercoda K8s cluster : killercoda.com</p>"},{"location":"prometheus_pushgateway/#installing-grafana","title":"Installing Grafana","text":"<pre><code>helm repo add grafana https://grafana.github.io/helm-charts\nhelm repo update\ncat&lt;&lt;EOF&gt; /tmp/grafana-values.yaml \ndatasources:\n  datasources.yaml:\n    apiVersion: 1\n    datasources:\n    - name: Prometheus\n      type: prometheus\n      url: http://prometheus-operated:9090\n      access: proxy\n      isDefault: true\nEOF\nhelm upgrade --install grafana grafana/grafana --namespace monitoring --create-namespace -f /tmp/grafana-values.yaml \nkubectl port-forward svc/grafana -n monitoring 3000:80 --address 0.0.0.0 &amp;\n// Get the admin password\nkubectl get secrets -n monitoring grafana -o jsonpath={\".data.admin-password\"} |  base64 -d \n</code></pre>"},{"location":"prometheus_pushgateway/#installing-prometheus-prometheus-operator","title":"Installing Prometheus &amp; Prometheus Operator","text":"<pre><code>helm repo add prometheus-community https://prometheus-community.github.io/helm-charts \nhelm repo update\nhelm upgrade --install prometheus prometheus-community/kube-prometheus-stack --namespace monitoring --create-namespace \\\n    --set=alertmanager.enabled=false,kubeProxy.enabled=false,kubeStateMetrics.enabled=false,nodeExporter.enabled=false,grafana.enabled=false,kubelet.enabled=false,kubeApiServer.enabled=false,kubeEtcd.enabled=false,kubeScheduler.enabled=false,coreDns.enabled=false,kubeControllerManager.enabled=false\nkubectl port-forward svc/prometheus-operated -n monitoring 9090 --address 0.0.0.0 &amp;\n</code></pre>"},{"location":"prometheus_pushgateway/#installing-pushgateway","title":"Installing PushGateway","text":"<pre><code>cat&lt;&lt;EOF &gt; /tmp/push-gateway-values.yaml \n    metrics:\n      enabled: true\n    serviceMonitor:\n      enabled: true\n      namespace: monitoring\n      additionalLabels:\n        app.kubernetes.io/instance: prometheus-pushgateway\n        app.kubernetes.io/name: prometheus-pushgateway\nEOF\n\nhelm upgrade --install prometheus-pushgateway prometheus-community/prometheus-pushgateway --namespace monitoring --create-namespace -f /tmp/push-gateway-values.yaml \nkubectl label servicemonitors.monitoring.coreos.com prometheus-pushgateway release=prometheus -n monitoring\nkubectl port-forward svc/prometheus-pushgateway -n monitoring 9091 --address 0.0.0.0 &amp;\n</code></pre>"},{"location":"prometheus_pushgateway/#check-everything-working-fine","title":"Check Everything working fine","text":"<p><pre><code>kubectl get pod -n monitoring --no-headers -w\n</code></pre> </p>"},{"location":"prometheus_pushgateway/#generate-few-metrics-through-curl-command-into-pushgateway-api-endpoint","title":"Generate few metrics through curl command into pushgateway api endpoint","text":"<p><pre><code>// Simple single metrics\necho \"http_request_duration_seconds 5\" | curl --silent --data-binary @- \"http://localhost:9091/metrics/job/demo\"\n</code></pre> </p>"},{"location":"prometheus_pushgateway/#automation-script-for-generating-random-metrics","title":"Automation Script for generating random metrics","text":"<p><pre><code>cat&lt;&lt;EOF &gt;genmetrics.sh\ntype=(\"apple\" \"banana\" \"orange\" \"grape\")\narray_length=${#type[@]}\nrandom_index=$((RANDOM % array_length))\njob=${type[$random_index]}\nvalue=$((RANDOM % 10 + 1))\necho \"http_request_duration_seconds{client=\\\"x\\\",req=\\\"$(date +%s)\\\",job=\\\"$job\\\"} $value\"\necho \"http_request_duration_seconds{client=\\\"x\\\",req=\\\"$(date +%s)\\\"} $value\" | curl --silent --data-binary @- \"http://localhost:9091/metrics/job/$job\"\nEOF\n</code></pre> </p> <p>Ref:  - https://www.civo.com/learn/deploy-a-batch-monitoring-stack-with-prometheus-pushgateway</p>"},{"location":"python_requests-1/","title":"Python requests 1","text":""},{"location":"python_requests-1/#extract-transform-and-load-etl-of-the-data-using-python1n","title":"Extract, transform, and load (ETL) of the data using python(1/n)","text":""},{"location":"python_requests-1/#-extract-transform-and-load-etl-of-the-data","title":"- Extract, transform, and load (ETL) of the data","text":"<ul> <li>Learn how to pull data from a REST endpoint</li> <li>Data Handling (Json/Yaml)</li> <li>Filter the data (Extraction ) from a Json file</li> <li>Transform the data to String (Transformation)</li> <li>Store the data in a CSV file (Load)</li> </ul>"},{"location":"python_requests-1/#python-program-for-pulling-a-rest-api-data","title":"Python program for pulling a REST API data","text":"<pre><code># python get_json_data.py\nimport requests;\n\ndef getData(url):\n      response=requests.get(url)\n      print(response)\n      return response.text\n\nif __name__ == \"__main__\":\n    data=getData('https://swapi.dev/api/planets/1/')\n    print(data)\n</code></pre>"},{"location":"python_requests-1/#python-program-for-pulling-a-rest-api-endpoint-and-extract-json-data","title":"Python program for pulling a REST API endpoint and extract json data","text":"<pre><code># python get_json_data.py\nimport requests;\nimport json;\n\ndef getData(url):\n      response=requests.get(url)\n      print(response)\n      return response.json()\n\nif __name__ == \"__main__\":\n    data=getData('https://swapi.dev/api/planets/1/')\n    print(json.dumps(data, indent=4, sort_keys=True))\n</code></pre>"},{"location":"python_requests-1/#python-program-to-store-the-json-payload-in-a-json-file","title":"Python program to store the json payload in a json file","text":"<pre><code># mkdir -p out &amp;&amp; python store_json_data.py\n\nimport requests;\nimport json;\nimport time;\n\ndef save_to_file(file_name,data):\n   try:\n      data_file=open(file_name,\"x+\")\n   except Exception as err:\n      print(err)\n   str=json.dumps(data, indent=4, sort_keys=True)\n   data_file.write(str)\n   print('Data saved to file: ',file_name)\n   data_file.close\n\ndef getData(url):\n      response=requests.get(url)\n      print('Data collected from ',url)\n      return response.json()\n\nif __name__ == \"__main__\":\n    print('*** Start ***')\n    timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n    json_file_name =\"out/swapi-planets\" + \"_\"+ timestr + \".json\"\n    data=getData('https://swapi.dev/api/planets/1/')\n    save_to_file(json_file_name,data)\n    print('*** end ***')\n</code></pre>"},{"location":"python_requests-1/#python-program-to-read-rest-api-with-multiple-pages-of-json-data-and-transform-it-to-a-csv-file","title":"Python program to read REST API with multiple pages of json data and transform it to a csv file","text":"<pre><code># mkdir -p out &amp;&amp; python write_json_data_to_csv.py\n\nimport requests;\nimport json;\nimport time;\n\n\ndef write_to_csv(json_file_name,csv_file_name):\n    print(\"JSON File: \",json_file_name)\n    json_file=open(json_file_name)\n    data=json.load(json_file)\n    csv_file=open(csv_file_name,\"w\")\n    csv_file.write('name,url'+'\\n')\n    for d in data:\n       csv_file.write(d['name']+\",\"+d['url']+\"\\n\")\n    print('Data saved to file: ',csv_file_name)\n\n\ndef save_to_file(file_name,data):\n   try:\n      data_file=open(file_name,\"x+\")\n   except Exception as err:\n      print(err)\n   str=json.dumps(data['results'], indent=4, sort_keys=True)\n   data_file.write(str)\n   print('Data saved to file: ',file_name)\n   data_file.close\n\ndef getData(url):\n      response=requests.get(url)\n      print('Data collected from ',url)\n      return response.json()\n\nif __name__ == \"__main__\":\n    print('*** Start ***')\n    timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n    json_file_name =\"out/pokemon-data\" + \"_\"+ timestr + \".json\"\n    csv_file_name =\"out/pokemon-data\" + \"_\"+ timestr + \".csv\"\n    data=getData('https://pokeapi.co/api/v2/pokemon')\n    save_to_file(json_file_name,data)\n    write_to_csv(json_file_name,csv_file_name)\n    print('*** end ***')\n</code></pre>"},{"location":"python_requests-1/#python-program-to-read-rest-api-with-basic-authentication","title":"Python program to read REST API with basic authentication","text":"<p>Create the access token : Go this site and get an access token https://github.com/settings/tokens click on generate token  </p> <pre><code># python test_basic_authentication.py\nimport requests;\n\nresponse = requests.get('https://api.github.com/users/naren4b/repos', auth=('naren4b', 'grA_FJ4lN5LAgXXXXXXXXjxS07Rdv&amp;&amp;&amp;&amp;&amp;&amp;sdadad'))\nprint(response.text)\n</code></pre>"},{"location":"python_requests-1/#ref","title":"ref:","text":"<ul> <li>https://www.scrapingbee.com/blog/best-python-http-clients/</li> <li>https://youtu.be/XtwK8Dq0ZiU</li> <li>https://requests.readthedocs.io/en/latest/</li> <li>https://requests.readthedocs.io/en/latest/user/quickstart/#make-a-request</li> <li>https://swapi.dev/</li> </ul>"},{"location":"s3-bucket-replication/","title":"Set Up Active-Active S3 Bucket Replication (MinIO\ud83e\udd61)","text":"<p>Bucket replication uses rules to synchronize the contents of a bucket on one MinIO deployment to a bucket on a remote MinIO deployment.</p> <p></p>"},{"location":"s3-bucket-replication/#replication-can-be-done-in-any-of-the-following-ways","title":"Replication can be done in any of the following ways:","text":"<ul> <li>Active-Passive Eligible objects replicate from the source bucket to the remote bucket. Any changes on the remote bucket do not replicate back.</li> <li>Active-Active Changes to eligible objects of either bucket replicate to the other bucket in a two-way direction.</li> <li>Multi-Site Active-Active Changes to eligible objects on any bucket set up for bucket replication replicate to all of the other buckets.</li> </ul>"},{"location":"s3-bucket-replication/#bucket-replication-requires-specific-permissions-on-the-source-and-destination-deployments-to-configure-and-enable-replication-rules","title":"Bucket replication requires specific permissions on the source and destination deployments to configure and enable replication rules.","text":"<ul> <li>The EnableRemoteBucketConfiguration statement grants permission for creating a remote target for supporting replication.</li> <li>The EnableReplicationRuleConfiguration statement grants permission for creating replication rules on a bucket. The \"arn:aws:s3:::* resource applies the replication permissions to any bucket on the source deployment. You can restrict the user policy to specific buckets as-needed.</li> </ul>"},{"location":"s3-bucket-replication/#replication-requires-versioning","title":"Replication Requires Versioning","text":"<p>MinIO relies on the immutability protections provided by versioning to support replication and resynchronization.</p> <pre><code>  --config-dir value, -C value  path to configuration folder (default: \"/root/.mc\") [$MC_CONFIG_DIR]                                                                                                                              \n  --quiet, -q                   disable progress bar display [$MC_QUIET]                                                                                                                                                          \n  --disable-pager, --dp         disable mc internal pager and print to raw stdout [$MC_DISABLE_PAGER]                                                                                                                             \n  --no-color                    disable color theme [$MC_NO_COLOR]                                                                                                                                                                \n  --json                        enable JSON lines formatted output [$MC_JSON]                                                                                                                                                     \n  --debug                       enable debug output [$MC_DEBUG]                                                                                                                                                                   \n  --resolve value               resolves HOST[:PORT] to an IP address. Example: minio.local:9000=10.10.75.1 [$MC_RESOLVE]                                                                                                         \n  --insecure                    disable SSL certificate verification [$MC_INSECURE]                                                                                                                                               \n  --limit-upload value          limits uploads to a maximum rate in KiB/s, MiB/s, GiB/s. (default: unlimited) [$MC_LIMIT_UPLOAD]                                                                                                  \n  --limit-download value        limits downloads to a maximum rate in KiB/s, MiB/s, GiB/s. (default: unlimited) [$MC_LIMIT_DOWNLOAD]                                                                                              \n  --id value                    id for the rule, should be a unique value                                                                                                                                                         \n  --tags value                  format '&lt;key1&gt;=&lt;value1&gt;&amp;&lt;key2&gt;=&lt;value2&gt;&amp;&lt;key3&gt;=&lt;value3&gt;', multiple values allowed for multiple key/value pairs                                                                                    \n  --storage-class value         storage class for destination, valid values are either \"STANDARD\" or \"REDUCED_REDUNDANCY\"                                                                                                         \n  --disable                     disable the rule                                                                                                                                                                                  \n  --priority value              priority of the rule, should be unique and is a required field (default: 0)                                                                                                                       \n  --remote-bucket value         remote bucket, should be a unique value for the configuration                                                                                                                                     \n  --replicate value             comma separated list to enable replication of soft deletes, permanent deletes, existing objects and metadata sync (default: \"delete-marker,delete,existing-objects,metadata-sync\")                \n  --path value                  bucket path lookup supported by the server. Valid options are ['auto', 'on', 'off']' (default: \"auto\")                                                                                            \n  --region value                region of the destination bucket (optional)                                                                                                                                                       \n  --bandwidth value             set bandwidth limit in bytes per second (K,B,G,T for metric and Ki,Bi,Gi,Ti for IEC units)                                                                                                        \n  --sync                        enable synchronous replication for this target. default is async                                                                                                                                  \n  --healthcheck-seconds value   health check interval in seconds (default: 60)                                                                                                                                                    \n  --disable-proxy               disable proxying in active-active replication. If unset, default behavior is to proxy\n</code></pre>"},{"location":"s3-bucket-replication/#enable-two-way-server-side-bucket-replication","title":"Enable Two-Way Server-Side Bucket Replication","text":"<p>The procedure on this page creates a new bucket replication rule for two-way <code>active-active</code> synchronization of objects between MinIO buckets.</p> <p>Setting up two minio S3</p>"},{"location":"s3-bucket-replication/#install-minio-clientmc","title":"Install Minio Client(mc)","text":"<pre><code>curl https://dl.min.io/client/mc/release/linux-amd64/mc \\\n  --create-dirs \\\n  -o $HOME/minio-binaries/mc\nchmod +x $HOME/minio-binaries/mc\nexport PATH=$PATH:$HOME/minio-binaries/\n</code></pre>"},{"location":"s3-bucket-replication/#install-certgen","title":"Install certgen","text":"<pre><code>curl https://github.com/minio/certgen/releases/latest/download/certgen-linux-amd64 \\\n   --create-dirs \\\n   -o $HOME/minio-binaries/certgen\nchmod +x $HOME/minio-binaries/certgen\nexport PATH=$PATH:$HOME/minio-binaries/\n</code></pre>"},{"location":"s3-bucket-replication/#pull-the-latest-image-of-minio","title":"Pull the latest image of minio","text":"<pre><code>docker pull quay.io/minio/minio:latest\n</code></pre>"},{"location":"s3-bucket-replication/#lets-get-the-docker-host-ip","title":"Let's get the Docker Host IP","text":"<pre><code>DOCKER_GATEWAY_IP=$(/sbin/ip route|awk '/docker0/ { print $9 }')\necho $DOCKER_GATEWAY_IP\n</code></pre>"},{"location":"s3-bucket-replication/#set-up-minio-1","title":"Set up minio-1","text":"<pre><code>MINIO_NAME_1=minio-1\nmkdir -p ~/$MINIO_NAME_1/data\nmkdir -p ~/$MINIO_NAME_1/certs\n\ncd ~/$MINIO_NAME_1/certs\ncertgen -host \"127.0.0.1,localhost,$MINIO_NAME_1\"\nls\ncd ../..\n\nMINIO_ROOT_USER_1=$MINIO_NAME_1\nMINIO_ROOT_PASSWORD=minio123\n\ndocker run  -d --rm --name $MINIO_NAME_1  \\\n                    -p 9000:9000 \\\n                    -p 9001:9001 \\\n                    -v ~/$MINIO_NAME_1/data:/data \\\n                    -v ~/$MINIO_NAME_1/certs:/opts/certs \\\n                    -e \"MINIO_ROOT_USER=$MINIO_ROOT_USER_1\" \\\n                    -e \"MINIO_ROOT_PASSWORD=$MINIO_ROOT_PASSWORD\" \\\n                    quay.io/minio/minio server /data --console-address \":9001\" --certs-dir /opts/certs\nmc config host add $MINIO_NAME_1 http://${DOCKER_GATEWAY_IP}:9000 $MINIO_ROOT_USER_1 $MINIO_ROOT_PASSWORD\n\n\nwget -O - https://min.io/docs/minio/linux/examples/ReplicationAdminPolicy.json | \\\nmc admin policy create $MINIO_NAME_1 ReplicationAdminPolicy /dev/stdin\nmc admin user add $MINIO_NAME_1 ReplicationAdmin LongRandomSecretKey\nmc admin policy attach $MINIO_NAME_1 ReplicationAdminPolicy --user=ReplicationAdmin\nmc admin policy ls $MINIO_NAME_1\n\nmc admin info $MINIO_NAME_1\nmc mb $MINIO_NAME_1/data\nmc version enable $MINIO_NAME_1/data\n</code></pre>"},{"location":"s3-bucket-replication/#set-up-minio-2","title":"Set up minio-2","text":"<pre><code>MINIO_NAME_2=$MINIO_NAME_2\n\nmkdir -p ~/$MINIO_NAME_2/data\nmkdir -p ~/$MINIO_NAME_2/certs\n\ncd ~/$MINIO_NAME_2/certs\ncertgen -host \"127.0.0.1,localhost,$MINIO_NAME_2\"\nls\ncd ../..\n\nMINIO_ROOT_USER_2=$MINIO_NAME_2\nMINIO_ROOT_PASSWORD=minio123\n\ndocker run  -d --rm --name $MINIO_NAME_2  \\\n                    -p 9002:9000 \\\n                    -p 9003:9001 \\\n                    -v ~/$MINIO_NAME_2/data:/data \\\n                    -v ~/$MINIO_NAME_2/certs:/opts/certs \\\n                    -e \"MINIO_ROOT_USER=$MINIO_ROOT_USER_2\" \\\n                    -e \"MINIO_ROOT_PASSWORD=$MINIO_ROOT_PASSWORD\" \\\n                    quay.io/minio/minio server /data --console-address \":9001\" --certs-dir /opts/certs\nmc config host add $MINIO_NAME_2 http://$DOCKER_GATEWAY_IP:9002 $MINIO_ROOT_USER_2 $MINIO_ROOT_PASSWORD     \n\nwget -O - https://min.io/docs/minio/linux/examples/ReplicationAdminPolicy.json | \\\nmc admin policy create $MINIO_NAME_2 ReplicationAdminPolicy /dev/stdin\nmc admin user add $MINIO_NAME_2 ReplicationAdmin LongRandomSecretKey\nmc admin policy attach $MINIO_NAME_2 ReplicationAdminPolicy --user=ReplicationAdmin\nmc admin policy ls $MINIO_NAME_2\n\nmc admin info $MINIO_NAME_2\nmc mb $MINIO_NAME_2/data\nmc version enable $MINIO_NAME_2/data\n</code></pre>"},{"location":"s3-bucket-replication/#enable-the-replication","title":"Enable the Replication","text":"<p>This procedure creates two-way, active-active replication between two MinIO</p> <pre><code>mc replicate add $MINIO_NAME_1/data --remote-bucket $MINIO_NAME_2/data  --priority 1\n</code></pre>"},{"location":"s3-bucket-replication/#test-the-setup","title":"Test the setup","text":"<p>Upload the Minio-1 <pre><code>echo \"Hello $MINIO_NAME_1 `date` \"&gt;&gt; app.log   \nmc cp app.log  $MINIO_NAME_1/data\n</code></pre> Check <pre><code>echo &amp;&amp; echo $MINIO_NAME_1 &amp;&amp; mc cat   $MINIO_NAME_1/data/app.log &amp;&amp; echo &amp;&amp;echo $MINIO_NAME_2 &amp;&amp; mc cat $MINIO_NAME_2/data/app.log &amp;&amp; echo \n</code></pre> Upload the Minio-2 <pre><code>echo \"Hello $MINIO_NAME_2 `date` \"&gt;&gt; app.log   \nmc cp app.log  $MINIO_NAME_2/data\n</code></pre> Check <pre><code>echo &amp;&amp; echo $MINIO_NAME_1 &amp;&amp; mc cat   $MINIO_NAME_1/data/app.log &amp;&amp; echo &amp;&amp;echo $MINIO_NAME_2 &amp;&amp; mc cat $MINIO_NAME_2/data/app.log &amp;&amp; echo \n</code></pre></p>"},{"location":"secure-local-ingress/","title":"Securing Kubernetes Traffic: A Step-by-Step Guide to Setting up Ingress Controller with Self-Signed Certificates and mTLS","text":""},{"location":"secure-local-ingress/#1-generate-keys-certificates","title":"1. Generate Keys &amp; Certificates:","text":"<p>Begin by creating cryptographic keys and self-signed certificates for secure communication.</p> <pre><code>ENV_ROOT_DOMAIN=127.0.0.1.nip.io\n\nROOT_CERT_DIR=\"/tmp/mycrts\"\nmkdir -p $ROOT_CERT_DIR\n\n\ndefault_value=\"demo\"\nSERVICE_NAME=$1 # Give the service name \nSERVICE_NAME=${SERVICE_NAME:-$default_value}\n\nmkdir -p $SERVICE_NAME\n\nSERVICE_CERT_DIR=$ROOT_CERT_DIR/$SERVICE_NAME\nmkdir -p $SERVICE_CERT_DIR\n\n\n# Root CA\nopenssl req -x509 -sha256 -newkey rsa:2048  -keyout $ROOT_CERT_DIR/rootCA.key -out $ROOT_CERT_DIR/rootCA.crt \\\n                -days 356 -nodes -subj \"/C=IN/ST=Karnataka/L=Bangalore/O=Naren/CN=${ENV_ROOT_DOMAIN}\"\n\n# Client key and csr\nopenssl req -new -newkey rsa:2048 -keyout $SERVICE_CERT_DIR/${SERVICE_NAME}.key -out $SERVICE_CERT_DIR/${SERVICE_NAME}.csr -nodes -subj \"/CN=${SERVICE_NAME}\"\n\ncat &gt;$ROOT_CERT_DIR/domain.ext &lt;&lt;EOF\nauthorityKeyIdentifier=keyid,issuer\nbasicConstraints=CA:FALSE\nsubjectAltName = @alt_names\n[alt_names]\nDNS.1 = *.$ENV_ROOT_DOMAIN\nEOF\n\n#Sign Client csr and generate crt\nopenssl x509 -req -CA $ROOT_CERT_DIR/rootCA.crt -CAkey $ROOT_CERT_DIR/rootCA.key \\\n                  -days 365  -set_serial 01 -CAcreateserial -extfile $ROOT_CERT_DIR/domain.ext \\\n                  -in $SERVICE_CERT_DIR/${SERVICE_NAME}.csr -out $SERVICE_CERT_DIR/${SERVICE_NAME}.crt\n</code></pre>"},{"location":"secure-local-ingress/#2-install-nginx-ingress-controller","title":"2. Install Nginx Ingress Controller:","text":"<p>Deploy the Nginx Ingress Controller to manage incoming traffic to Kubernetes services.</p> <pre><code>wget https://raw.githubusercontent.com/kubernetes/ingress-nginx/main/deploy/static/provider/kind/deploy.yaml \nkubectl label nodes controlplane ingress-ready=\"true\"\nkubectl apply -f deploy.yaml \nkubectl wait --for=condition=ready pod -n ingress-nginx -l app.kubernetes.io/component=controller\n</code></pre>"},{"location":"secure-local-ingress/#3-deploy-http-echo023-service","title":"3. Deploy http-echo:0.2.3 Service:","text":"<p>Set up the http-echo service, including the pod, service, and ingress resources.</p> <pre><code>NS=default\nkubectl run -n $NS $SERVICE_NAME  --image hashicorp/http-echo:0.2.3 -- -text=\"Hello $SERVICE_NAME\"\nkubectl expose pod -n $NS $SERVICE_NAME --port 5678\nkubectl create secret generic -n $NS ${SERVICE_NAME}-tls \\\n            --from-file=tls.crt=$SERVICE_CERT_DIR/${SERVICE_NAME}.crt \\\n            --from-file=tls.key=$SERVICE_CERT_DIR/${SERVICE_NAME}.key \\\n            --from-file=ca.crt=$ROOT_CERT_DIR/rootCA.crt  -o yaml --dry-run=client &gt; ${SERVICE_NAME}/secret.yaml\n</code></pre>"},{"location":"secure-local-ingress/#4-create-ingress-and-tls-secrets","title":"4. Create Ingress and TLS Secrets:","text":"<p>Establish an Ingress resource and TLS secrets to secure communication with the http-echo service. <pre><code>cat &gt; ${SERVICE_NAME}/ing.yaml &lt;&lt;EOF\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: ${SERVICE_NAME}\n  namespace: $NS\n  annotations:\n    nginx.ingress.kubernetes.io/auth-tls-verify-client: \"on\"\n    nginx.ingress.kubernetes.io/auth-tls-secret: \"${NS}/${SERVICE_NAME}-tls\"\n    nginx.ingress.kubernetes.io/auth-tls-error-page: \"https://$ENV_ROOT_DOMAIN/error.html\"\n    nginx.ingress.kubernetes.io/auth-tls-verify-depth: \"1\"\n    nginx.ingress.kubernetes.io/auth-tls-pass-certificate-to-upstream: \"true\"\nspec:\n  ingressClassName: nginx\n  rules:\n  - host: ${SERVICE_NAME}.$ENV_ROOT_DOMAIN\n    http:\n      paths:\n      - pathType: Prefix\n        path: \"/\"\n        backend:\n          service:\n            name: ${SERVICE_NAME}\n            port:\n              number: 5678\n  tls:\n  - hosts:\n      - ${SERVICE_NAME}.$ENV_ROOT_DOMAIN\n    secretName: ${SERVICE_NAME}-tls\nEOF\nkubectl apply -n $NS -f ${SERVICE_NAME}/secret.yaml\nkubectl apply -n $NS -f ${SERVICE_NAME}/ing.yaml\n</code></pre></p>"},{"location":"secure-local-ingress/#5-test-the-service","title":"5. Test the Service:","text":"<p>Verify the setup by accessing the service through the Ingress Controller, ensuring that both Ingress and mTLS configurations are functioning correctly. <pre><code>echo curl -L --cacert $ROOT_CERT_DIR/rootCA.crt  --key $SERVICE_CERT_DIR/${SERVICE_NAME}.key  --cert $SERVICE_CERT_DIR/${SERVICE_NAME}.crt  https://${SERVICE_NAME}.$ENV_ROOT_DOMAIN\necho \"127.0.0.1 ${SERVICE_NAME}.$ENV_ROOT_DOMAIN\" &gt;&gt; /etc/hosts\n</code></pre></p>"},{"location":"secure-local-ingress/#ref","title":"Ref:","text":"<ul> <li>Demo Environment</li> <li>nks-k8s-ingress.git</li> <li>To run :    <pre><code>curl https://raw.githubusercontent.com/naren4b/nks-k8s-ingress/main/run.sh | bash \n</code></pre> ref: https://haproxy-ingress.github.io/docs/getting-started/#deploy-and-expose Home</li> </ul>"},{"location":"secure-s3-minio/","title":"Setup Minio S3 with Transport Layer Security (TLS) 1.2+ encryption","text":""},{"location":"secure-s3-minio/#install-minio-certgen-tool","title":"Install minio certgen tool","text":"<pre><code>curl https://github.com/minio/certgen/releases/latest/download/certgen-linux-amd64 \\\n   --create-dirs \\\n   -o $HOME/minio-binaries/certgen\nchmod +x $HOME/minio-binaries/certgen\nexport PATH=$PATH:$HOME/minio-binaries/\n</code></pre>"},{"location":"secure-s3-minio/#install-minio-server","title":"Install minio server","text":"<pre><code>mkdir -p ~/minio/data\nmkdir -p ~/minio/certs\n\ncd ~/minio/certs\ncertgen -host \"127.0.0.1,localhost\"\nls\ncd ../..\n\nMINIO_ROOT_USER=minio\nMINIO_ROOT_PASSWORD=minio123\n</code></pre> <p>Install the server</p> <pre><code>docker run  -d --rm --name minio  \\\n                    -p 9000:9000 \\\n                    -p 9001:9001 \\\n                    -v ~/minio/data:/data \\\n                    -v ~/minio/certs:/opts/certs \\\n                    -e \"MINIO_ROOT_USER=$MINIO_ROOT_USER\" \\\n                    -e \"MINIO_ROOT_PASSWORD=$MINIO_ROOT_PASSWORD\" \\\n                    quay.io/minio/minio server /data --console-address \":9001\" --certs-dir /opts/certs\n</code></pre>"},{"location":"secure-s3-minio/#access-bucket-via-minio-client-mc","title":"Access bucket via minio client mc","text":"<pre><code>curl https://dl.min.io/client/mc/release/linux-amd64/mc \\\n  --create-dirs \\\n  -o $HOME/minio-binaries/mc\nchmod +x $HOME/minio-binaries/mc\n\nmc alias set myminio https://127.0.0.1:9000 $MINIO_ROOT_USER $MINIO_ROOT_PASSWORD\nmc admin info myminio\nmc mb myminio/demo-bucket\ntouch demo.txt\necho \"Hello My S3 \"&gt; demo.txt\nmc cp demo.txt myminio/demo-bucket\nmc ls myminio/demo-bucket\n</code></pre>"},{"location":"secure-s3-minio/#access-the-bucket-via-s3cmd-client","title":"Access the bucket via s3cmd client","text":"<pre><code>sudo apt install s3cmd\n\ncat&lt;&lt;EOF &gt;  ~/.s3cfg\nhost_base = 127.0.0.1:9000\nhost_bucket = 127.0.0.1:9000\nbucket_location = us-east-1\nuse_https = True\n\n# Setup access keys\naccess_key =  minio\nsecret_key = minio123\n\n# Enable S3 v4 signature APIs\nsignature_v2 = False\n\nEOF\n\ns3cmd -c ~/.s3cfg --ca-certs=~/minio/certs/public.crt ls s3://demo-bucket\n</code></pre> <p>Demo Environment</p> <p>ref:</p> <ul> <li>https://min.io/docs/minio/container/operations/network-encryption.html</li> </ul>"},{"location":"setting-up-argocd-vault-plugin/","title":"Securing Kubernetes Secrets with Argocd-vault-plugin","text":"<p>Managing secrets securely within Kubernetes clusters can be challenging, especially when following GitOps practices with tools like Argo CD. In this guide, we'll explore how to leverage the Argocd-vault-plugin to streamline secret management within Kubernetes clusters using Vault.</p>"},{"location":"setting-up-argocd-vault-plugin/#introduction-to-argocd-vault-plugin","title":"Introduction to Argocd-vault-plugin","text":"<p>The Argocd-vault-plugin is designed to address the complexities of secret management in GitOps workflows with Argo CD. By integrating with Vault, it offers a simple yet robust solution for managing secrets, configurations, and deployments within Kubernetes clusters.</p> <p></p> <p>The setup </p>"},{"location":"setting-up-argocd-vault-plugin/#install-vault","title":"Install Vault","text":"<pre><code>helm repo add hashicorp https://helm.releases.hashicorp.com\nhelm repo update\nhelm install vault hashicorp/vault --set \"server.dev.enabled=true\"\n</code></pre>"},{"location":"setting-up-argocd-vault-plugin/#configure-the-vault","title":"Configure the Vault","text":"<pre><code>kubectl exec -it vault-0 sh\n</code></pre>"},{"location":"setting-up-argocd-vault-plugin/#setup-the-key-value-in-the-kubernetes","title":"Setup the Key-Value in the kubernetes","text":"<pre><code>vault secrets enable -path=mysecret kv-v2\nvault kv put mysecret/database/config username=\"nks-user\" password=\"nks-secret-password\"\n\n# Write out the policy named mysecret that enables the read capability for secrets at path mysecret/data/database/config.\nvault policy write mysecret - &lt;&lt;EOF\npath \"mysecret/data/database/config\" {\n   capabilities = [\"read\"]\n}\nEOF\n</code></pre>"},{"location":"setting-up-argocd-vault-plugin/#enable-kubernetes-authentication","title":"Enable kubernetes authentication","text":"<pre><code># Enable the Kubernetes authentication method.\nvault auth enable kubernetes\n\n# Configure the Kubernetes authentication method to use the location of the Kubernetes API. (when running inside the cluster)\nvault write auth/kubernetes/config \\\n      kubernetes_host=\"https://$KUBERNETES_PORT_443_TCP_ADDR:443\"\n\n\n# Create a Kubernetes authentication role named internal-app.\nvault write auth/kubernetes/role/argocd-server \\\n      bound_service_account_names=argocd-server \\\n      bound_service_account_namespaces=argocd \\\n      policies=mysecret \\\n      ttl=24h\nexit\n</code></pre>"},{"location":"setting-up-argocd-vault-plugin/#basic-argocd-installation-setupp","title":"Basic Argocd Installation Setupp","text":"<pre><code># Install argocd client\ncurl -sSL -o argocd-linux-amd64 https://github.com/argoproj/argo-cd/releases/latest/download/argocd-linux-amd64\nsudo install -m 555 argocd-linux-amd64 /usr/local/bin/argocd\nrm argocd-linux-amd64\n\n# Create the namespace and Install argocd \nkubectl create namespace argocd\nkubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml\nkubectl get pod -n argocd -w\n</code></pre> <pre><code># edit the deployment for insecure installation in local\nkubectl patch deployments.apps -n argocd  argocd-server  \\\n--type=json \\\n-p='[{\"op\": \"add\", \"path\": \"/spec/template/spec/containers/0/args/-\", \"value\": \"--insecure\"}]'\nnohup kubectl  port-forward -n argocd svc/argocd-server 8080:443 --address 0.0.0.0 &amp; \n\nargocd_password=$(kubectl get secrets -n argocd argocd-initial-admin-secret -o jsonpath=\"{.data.password}\" | base64 -d)\necho $argocd_password\nargocd login localhost:8080  --insecure --username=admin --password=$argocd_password\n</code></pre> <pre><code>argocd repo add https://github.com/naren4b/demo-app.git\n\nargocd app create demo \\\n     --repo https://github.com/naren4b/demo-app.git \\\n     --path helm-chart \\\n     --dest-server https://kubernetes.default.svc \\\n     --dest-namespace default \\\n     --sync-policy  automated\n</code></pre>"},{"location":"setting-up-argocd-vault-plugin/#integrate-the-argocd-vault-plugin-as-configmap","title":"Integrate the argocd-vault plugin as configmap","text":"<pre><code>kubectl create -f https://raw.githubusercontent.com/naren4b/demo-app/main/others/cmp-plugin.yaml -n argocd\n</code></pre>"},{"location":"setting-up-argocd-vault-plugin/#inject-the-vault-authentication-for-the-argocd-to-consume","title":"Inject the vault authentication for the argocd to consume","text":"<pre><code>kubectl create secret generic -n argocd argocd-vault-plugin-credentials \\\n    --from-literal=AVP_TYPE=vault \\\n    --from-literal=VAULT_ADDR=http://vault-internal.default.svc.cluster.local:8200 \\\n    --from-literal=AVP_AUTH_TYPE=k8s \\\n    --from-literal=AVP_K8S_ROLE=argocd-server \n</code></pre>"},{"location":"setting-up-argocd-vault-plugin/#patch-the-argocd-repo-server","title":"Patch the argocd-repo-server","text":"<p><pre><code>wget https://raw.githubusercontent.com/naren4b/demo-app/main/others/argocd-repo-server-patch.yaml\nkubectl patch deployment argocd-repo-server -n argocd --patch-file argocd-repo-server-patch.yaml\nkubectl get pod -n argocd -w\n# edit and update the `serviceAccount` Name in the `argocd-repo-server` deployment to use `argocd-server`\n# Restart all the pods\nkubectl get pod -n argocd | awk '{print $1}' | xargs kubectl delete pod -n argocd\n# add the argocd-vault-plugin-helm\nkubectl apply -f https://raw.githubusercontent.com/naren4b/demo-app/main/others/demo-argocd-application.yaml\n</code></pre> </p> <p></p> <p></p>"},{"location":"setting-up-argocd-vault-plugin/#check-the-secret-value","title":"Check the secret value","text":"<p><pre><code>kubectl get secret mysecret -o jsonpath=\"{.data.password}\" | base64 -d \n</code></pre> </p> <p>ref: - https://killercoda.com/killer-shell-ckad/scenario/playground - https://youtu.be/7L6nSuKbC2c?si=q_v-F9Qpv3x5pNQm - https://argocd-vault-plugin.readthedocs.io/en/stable/  - https://medium.com/@raosharadhi11/argocd-with-vault-using-argocd-vault-plugin-dccbc302f0c2</p>"},{"location":"setting-up-argocd/","title":"Setup Argocd with extra cluster and git integrated","text":""},{"location":"setting-up-argocd/#create-main-cluster","title":"Create <code>Main</code> Cluster","text":"<p>Setup the Data  <pre><code>export ORG_CONTROL_PLANE_K8S=main\nclusterName=$ORG_CONTROL_PLANE_K8S\nkind delete cluster --name=${clusterName}\napiServerPort=6443\n</code></pre> Prepare the cluster-config <pre><code>cat &lt;&lt; EOF &gt; ${clusterName}-cluster-config.yaml\nkind: Cluster\napiVersion: kind.x-k8s.io/v1alpha4\nname: ${clusterName}\nnetworking:\n apiServerAddress: \"0.0.0.0\"\n apiServerPort: $apiServerPort\nnodes:\n- role: control-plane\n  extraPortMappings:\n  - containerPort: 443\n    hostPort: 443\n    listenAddress: \"0.0.0.0\"\n  - containerPort: 80\n    hostPort: 80\n    listenAddress: \"0.0.0.0\"\nEOF\nkind create cluster --config ${clusterName}-cluster-config.yaml --kubeconfig ./kubeconfig\ncat kubeconfig | sed \"s|https://:${apiServerPort}|https://0.0.0.0:${apiServerPort}|g\"  &gt; ./config\nkind get kubeconfig --name=${clusterName} | sed \"s|https://:${apiServerPort}|https://${clusterName}-control-plane:6443|g\"  &gt; ${clusterName}-config\n</code></pre></p>"},{"location":"setting-up-argocd/#install-ingress-controller-haproxy","title":"Install Ingress Controller <code>haproxy</code>","text":"<pre><code>helm repo add haproxy-ingress https://haproxy-ingress.github.io/charts\nhelm upgrade --install ingress haproxy-ingress/haproxy-ingress -n ingress-controller --create-namespace --set controller.hostNetwork=true --set controller.ingressClassResource.enabled=true\n\nexport DOMAIN=10.157.53.176.nip.io\nexport INGRESS_CLASS=haproxy\n</code></pre>"},{"location":"setting-up-argocd/#install-argocd-and-argocd-ingress","title":"Install ArgoCD and Argocd Ingress","text":"<pre><code>helm repo add argo https://argoproj.github.io/argo-helm\nhelm repo update\ncat&lt;&lt;EOF &gt;argocd-values.yaml\nconfigs:\n  params:\n    server.insecure: true\nEOF\nhelm upgrade --install root argo/argo-cd -n argocd -f argocd-values.yaml  --create-namespace\nkubectl wait --for=condition=Ready -n argocd pod -l  app.kubernetes.io/name=argocd-server\n#kubectl create ingress argocd --rule=\"argocd.${DOMAIN}/=root-argocd-server:8080\" -n argocd \nARGOCDURL=argocd.${DOMAIN} \ncat &lt;&lt;EOF | kubectl apply -f -\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  annotations:\n  name: argocd\n  namespace: argocd\nspec:\n  ingressClassName: haproxy\n  rules:\n  - host: $ARGOCDURL\n    http:\n      paths:\n      - backend:\n          service:\n            name: root-argocd-server\n            port:\n              number: 443\n        path: /\n        pathType: Prefix\n  tls:\n  - hosts:\n    - $ARGOCDURL\nEOF\n</code></pre>"},{"location":"setting-up-argocd/#add-git-repo","title":"Add git repo","text":"<p>Prepare the values <pre><code>export MY_GIT_NAME={name}\nexport MY_GIT_USER={user}\nexport MY_GIT_TOKEN={token}\nexport MY_GIT_URL={url}\n</code></pre> Apply the Secret <pre><code>cat&lt;&lt;EOF | kubectl create -f -\napiVersion: v1\nkind: Secret\nmetadata:\n  name: ${MY_GIT_NAME}-https-creds\n  namespace: argocd\n  labels:\n   argocd.argoproj.io/secret-type: repo-creds\nstringData:\n  type: git\n  url: ${MY_GIT_URL}\n  username: ${MY_GIT_USER}\n  password: ${MY_GIT_TOKEN}\nEOF\n</code></pre> Check the Repo https://argocd.127.0.0.1.nip.io/settings/repos </p>"},{"location":"setting-up-argocd/#create-edge-cluster","title":"Create <code>Edge</code> Cluster","text":"<p>Setup the data  <pre><code>clusterName=edge-1 #Change me\nkind delete cluster --name=${clusterName}\napiServerPort=6444 # change me\n</code></pre> Create the cluster  <pre><code>cat &lt;&lt; EOF &gt; ${clusterName}-cluster-config.yaml\nkind: Cluster\napiVersion: kind.x-k8s.io/v1alpha4\nname: ${clusterName}\nnetworking:\n apiServerAddress: \"0.0.0.0\"\n apiServerPort: $apiServerPort\nnodes:\n- role: control-plane\nEOF\nkind create cluster --config ${clusterName}-cluster-config.yaml --kubeconfig ./kubeconfig\ncat kubeconfig | sed \"s|https://:${apiServerPort}|https://0.0.0.0:${apiServerPort}|g\"  &gt; ./config\nkind get kubeconfig --name=${clusterName} | sed \"s|https://:${apiServerPort}|https://${clusterName}-control-plane:6443|g\"  &gt; ${clusterName}-config\n</code></pre></p>"},{"location":"setting-up-argocd/#create-the-service-account-argocd","title":"Create The Service Account <code>argocd</code>","text":"<pre><code># Connect to EDGE cluster\n#!/bin/bash\necho \"Create the Service Account\"\nkubectl create sa argocd-manager -n kube-system\ncat&lt;&lt;EOF | kubectl create -f - \napiVersion: v1\nkind: Secret\ntype: kubernetes.io/service-account-token\nmetadata:\n  name: argocd-manager\n  namespace: kube-system\n  annotations:\n    kubernetes.io/service-account.name: \"argocd-manager\"\nEOF\n</code></pre>"},{"location":"setting-up-argocd/#create-cluster-role-binding-argocd-cluster-admin","title":"Create Cluster Role Binding <code>argocd-cluster-admin</code>","text":"<p><pre><code># Note Cluster Admin is a full permission (we can check alternative https://github.com/argoproj/argo-cd/issues/5389)\necho \"Create the rolebinding\"\nkubectl create clusterrolebinding --clusterrole=cluster-admin --serviceaccount=kube-system:argocd-manager argocd-manager-role\n</code></pre> Collect the information <pre><code>caData=$(kubectl config view --raw -o jsonpath=\"{.clusters[0].cluster.certificate-authority-data}\")\ntoken=$(kubectl create token argocd-manager -n kube-system --duration=9999h)\nserver=$(kubectl config view --raw -o jsonpath=\"{.clusters[0].cluster.server}\")\nname=$(kubectl config view --raw -o jsonpath=\"{.clusters[0].name}\")\n</code></pre></p>"},{"location":"setting-up-argocd/#connect-extra-cluster-and-deploy-application","title":"Connect extra cluster and deploy application","text":"<pre><code>kubectx main\n</code></pre>"},{"location":"setting-up-argocd/#add-the-edge-cluster","title":"Add the EDGE cluster","text":"<p><pre><code>cat&lt;&lt;EOF | kubectl apply -f -\napiVersion: v1\nkind: Secret\nmetadata:\n  name: ${name}-cluster-secret\n  namespace: argocd\n  labels:\n    argocd.argoproj.io/secret-type: cluster\ntype: Opaque\nstringData:\n  name: ${name}\n  server: ${server}\n  config: |\n    {\n      \"bearerToken\": \"${token}\",\n      \"tlsClientConfig\": {\n        \"insecure\": false,\n        \"caData\": \"${caData}\"\n      }\n    }\nEOF\n</code></pre> Check https://argocd.127.0.0.1.nip.io/settings/clusters </p>"},{"location":"setting-up-argocd/#deploy-the-demo-argocd-application","title":"Deploy the Demo Argocd Application","text":"<p><pre><code>SERVER=$1 #\"https://edge-1-control-plane:6443\"\n</code></pre> Deploy the application <pre><code>cat&lt;&lt;EOF | kubectl apply  -n argocd -f -\napiVersion: argoproj.io/v1alpha1\nkind: Application\nmetadata:\n  name: 'demo-app'\nspec:\n  destination:\n    server: $SERVER\n    namespace: default\n  source:\n    path: sample-app\n    repoURL: https://github.com/naren4b/demo-app.git\n    targetRevision: HEAD  \n  project: default\n  syncPolicy:\n    automated: {}\nEOF\n</code></pre> </p>"},{"location":"setting-up-kubernetes-vault-agent/","title":"Secrets Injection Using Vault Agent Into Kubernetes Pods via Vault Agent Side Containers","text":"<p>Vault Agent : Vault Agent is a client daemon more you can find here : What is Vault Agent?</p>"},{"location":"setting-up-kubernetes-vault-agent/#install-vault","title":"Install Vault","text":"<pre><code>helm repo add hashicorp https://helm.releases.hashicorp.com\nhelm repo update\nhelm install vault hashicorp/vault --set \"server.dev.enabled=true\"\n</code></pre>"},{"location":"setting-up-kubernetes-vault-agent/#configure-vault","title":"Configure Vault","text":"<pre><code>kubectl exec -it vault-0 -- /bin/sh\n</code></pre>"},{"location":"setting-up-kubernetes-vault-agent/#inside-the-pod","title":"Inside the pod","text":"<pre><code># Enable kv-v2 secrets at the path internal.\nvault secrets enable -path=internal kv-v2\n\n#Create a secret at path internal/database/config with a username and password.\nvault kv put internal/database/config username=\"naren\" password=\"mypassword\"\n\n# Check the values\nvault kv get internal/database/config\n</code></pre>"},{"location":"setting-up-kubernetes-vault-agent/#set-up-kubernetes-authentication","title":"Set up kubernetes authentication^","text":"<p>Vault provides a Kubernetes authentication method that enables clients to authenticate with a Kubernetes Service Account Token. This token is provided to each pod when it is created.</p> <pre><code># Enable the Kubernetes authentication method.\nvault auth enable kubernetes\n\n# Configure the Kubernetes authentication method to use the location of the Kubernetes API. (when running inside the cluster)\nvault write auth/kubernetes/config \\\n      kubernetes_host=\"https://$KUBERNETES_PORT_443_TCP_ADDR:443\"\n\n# Write out the policy named internal-app that enables the read capability for secrets at path internal/data/database/config.\nvault policy write internal-app - &lt;&lt;EOF\npath \"internal/data/database/config\" {\n   capabilities = [\"read\"]\n}\nEOF\n\n# Create a Kubernetes authentication role named internal-app.\nvault write auth/kubernetes/role/internal-app \\\n      bound_service_account_names=internal-app \\\n      bound_service_account_namespaces=default \\\n      policies=internal-app \\\n      ttl=24h\n\nexit\n</code></pre>"},{"location":"setting-up-kubernetes-vault-agent/#set-up-in-kubernetes-create-service-account","title":"Set up in Kubernetes Create service account","text":"<pre><code>kubectl create sa internal-app\ncat&lt;&lt;EOF &gt; deployment.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n   name: orgchart\n   labels:\n      app: orgchart\nspec:\n   selector:\n      matchLabels:\n       app: orgchart\n   replicas: 1\n   template:\n      metadata:\n       annotations:\n         vault.hashicorp.com/agent-inject: 'true'\n         vault.hashicorp.com/role: 'internal-app'\n         vault.hashicorp.com/agent-inject-secret-database-config.txt: 'internal/data/database/config'\n       labels:\n         app: orgchart\n      spec:\n       serviceAccountName: internal-app\n       containers:\n         - name: orgchart\n           image: jweissig/app:0.0.1\nEOF\nkubectl apply -f deployment.yaml\n</code></pre>"},{"location":"setting-up-kubernetes-vault-agent/#check-the-secret","title":"Check the Secret","text":"<pre><code>kubectl exec \\\n      $(kubectl get pod -l app=orgchart -o jsonpath=\"{.items[0].metadata.name}\") \\\n      --container orgchart -- cat /vault/secrets/database-config.txt\n</code></pre>"},{"location":"setting-up-kubernetes-vault-agent/#complete-flow","title":"Complete flow","text":"<p>^Vault Authentication Methods</p> <p>Token Authentication:</p> <p>Overview: Token authentication is the simplest and most commonly used method. When Vault is initialized or unsealed, it generates a root token by default. This root token has full access to Vault and should be securely stored and used only for administrative tasks.</p> <p>Usage: Besides the root token, Vault can generate and manage other tokens with limited access and lifetimes. Tokens can be used to authenticate users, applications, or services to access secrets or perform actions within Vault.</p> <p>Authentication Process: Users provide their token to Vault when accessing resources or performing operations. Vault validates the token against its internal token store and grants access based on the token's policies and capabilities.</p> <p>Use Cases: Token authentication is suitable for various scenarios, including initial setup, administrative tasks, and user or application authentication.</p> <p>Kubernetes Authentication:</p> <p>Overview: Kubernetes authentication enables applications running in Kubernetes clusters to authenticate with Vault seamlessly. It uses Kubernetes Service Account Tokens for authentication.</p> <p>Configuration: Vault is configured to trust Kubernetes' token-based authentication system. Kubernetes Pods can authenticate with Vault by presenting their Service Account Token.</p> <p>Dynamic Secrets: Vault can dynamically generate short-lived credentials (e.g., database credentials) for applications based on Kubernetes authentication. This enhances security by minimizing the exposure of long-lived credentials.</p> <p>Use Cases: Kubernetes authentication is valuable in containerized environments where applications run in Kubernetes Pods. It streamlines access management and enhances security by integrating with Kubernetes' native authentication mechanisms.</p> <p>ref:</p> <ul> <li>https://developer.hashicorp.com/vault/tutorials/kubernetes/kubernetes-sidecar#inject-secrets-into-the-pod</li> </ul>"},{"location":"setup-loki-grafana-stack-simple-scalable/","title":"Install Simple Scalable Loki Promtail Grafana stack in KIND cluster","text":"<p>Loki\u2019s simple scalable deployment mode separates execution paths into read, write, and backend targets. These targets can be scaled independently, letting you customize your Loki deployment to meet your business needs for log ingestion and log query so that your infrastructure costs better match how you use Loki. </p> <p>The three execution paths in simple scalable mode are each activated by appending the following arguments to Loki on startup: - <code>-target=write</code> - The write target is stateful and is controlled by a Kubernetes StatefulSet. It contains the following components: \u2013 Distributor \u2013 Ingester</p> <ul> <li> <p><code>-target=read</code> - The read target is stateless and can be run as a Kubernetes Deployment that can be scaled automatically (Note that in the official helm chart it is currently deployed as a stateful set). It contains the following components: \u2013 Query front end \u2013 Queriers</p> </li> <li> <p><code>-target=backend</code> - The backend target is stateful, and is controlled by a Kubernetes StatefulSet. Contains the following components: \u2013 Compactor \u2013 Index gateways \u2013 Query scheduler \u2013 Ruler The simple scalable deployment mode requires a reverse proxy to be deployed in front of Loki, to direct client API requests to either the read or write nodes. The Loki Helm chart includes a default reverse proxy configuration, using Nginx.</p> </li> </ul>"},{"location":"setup-loki-grafana-stack-simple-scalable/#have-a-kind-cluster","title":"Have a KIND cluster","text":"<p>ref: mykindk8scluster or Demo Environment</p>"},{"location":"setup-loki-grafana-stack-simple-scalable/#install-loki","title":"Install Loki","text":"<pre><code>git clone https://github.com/naren4b/loki-app.git\ncd loki-app\nhelm repo add grafana https://grafana.github.io/helm-charts\nhelm repo update\nhelm pull grafana/loki --untar # optional\nhelm upgrade --install  loki grafana/loki  -f simple-scalable-values.yaml -f my-values.yaml \n</code></pre>"},{"location":"setup-loki-grafana-stack-simple-scalable/#install-promtail","title":"Install promtail","text":"<pre><code>cat&lt;&lt;EOF &gt;$PWD/promtail-demo-values.yaml\nconfig:\n  clients:\n    - url: http://loki-gateway/loki/api/v1/push\n      tenant_id: 1\nextraPorts:\n   syslog:\n     name: tcp-syslog\n     annotations: {}\n     labels: {}\n     containerPort: 8514\n     protocol: TCP\n     service:\n       type: ClusterIP\n       clusterIP: null\n       port: 1514\n       externalIPs: []\n       nodePort: null\n       loadBalancerIP: null\n       loadBalancerSourceRanges: []\n       externalTrafficPolicy: null\nEOF\n</code></pre> <pre><code>REPO_NAME=grafana\nREPO_PATH=promtail\nCHART_VERSION=6.15.3\nCHART_APP_VERSION=promtail\nhelm install promtail ${REPO_NAME}/${REPO_PATH}  --version ${CHART_VERSION} -f $PWD/promtail-demo-values.yaml\n</code></pre>"},{"location":"setup-loki-grafana-stack-simple-scalable/#install-grafana","title":"Install Grafana","text":"<p><pre><code>kubectl run grafana --image=grafana/grafana --port=3000\nkubectl expose pod grafana --port=3000 --name=grafana\nkubectl port-forward svc/grafana 3000:3000 --address 0.0.0.0\n#data-source: http://loki-gateway\n#ref: https://github.com/grafana/loki/issues/9756#issuecomment-1918895042\n</code></pre> </p>"},{"location":"setup-loki-grafana-stack-simple-scalable/#add-loki-data-source","title":"[Add Loki data source ]","text":"<p> url: http://loki:3100 Visit http://localhost:3000/connections/datasources/loki </p> <p>Visit http://localhost:3000/dashboard/new?orgId=1</p>"},{"location":"setup-loki-grafana-stack-simple-scalable/#ref","title":"ref:","text":"<ul> <li>deployment-modes</li> <li>scalable-monolithic-mode</li> </ul>"},{"location":"setup-loki-grafana-stack/","title":"Install Single Binary Loki Promtail Grafana stack in KIND cluster","text":"<p>In this blog post, we explore a quick and efficient way to set up a basic Loki-Promtail-Grafana stack in a KIND (Kubernetes in Docker) cluster. The tutorial provides a step-by-step guide, ensuring a seamless installation process. By leveraging these open-source tools, users can enhance their log monitoring and visualization capabilities within a Kubernetes environment. The simplicity of KIND makes it an ideal platform for testing and development, allowing users to easily deploy and manage the Loki logging system, Promtail agent, and Grafana dashboard for effective log analysis. Dive into the details and streamline your Kubernetes log management with this concise guide.</p>"},{"location":"setup-loki-grafana-stack/#have-a-kind-cluster","title":"Have a KIND cluster","text":"<p>ref: mykindk8scluster or Demo Environment</p>"},{"location":"setup-loki-grafana-stack/#install-loki","title":"Install Loki","text":"<p>Loki custom helm value file</p> <pre><code>kubectl create ns monitoring\n\ncat&lt;&lt;EOF &gt;$PWD/loki-demo-values.yaml\n---\nloki:\n  commonConfig:\n    replication_factor: 1\n  schemaConfig:\n    configs:\n      - from: 2024-04-01\n        store: tsdb\n        object_store: s3\n        schema: v13\n        index:\n          prefix: loki_index_\n          period: 24h\n  ingester:\n    chunk_encoding: snappy\n  tracing:\n    enabled: true\n  querier:\n    max_concurrent: 2\n\n\ndeploymentMode: SingleBinary\nsingleBinary:\n  replicas: 1\n  persistence:\n    size: 1Gi # Chageit\nchunksCache:\n  writebackSizeLimit: 10MB\n\n\n\ntest:\n  enabled: false\nlokiCanary:\n  enabled: false\ngateway:\n  enabled: false\nmonitoring:\n  lokiCanary:\n    enabled: false\n  selfMonitoring:\n    enabled: false\n    grafanaAgent:\n      installOperator: false\n\n# Enable minio for storage\nminio:\n  enabled: true\n\n# Zero out replica counts of other deployment modes\nbackend:\n  replicas: 0\nread:\n  replicas: 0\nwrite:\n  replicas: 0\n\ningester:\n  replicas: 0\nquerier:\n  replicas: 0\nqueryFrontend:\n  replicas: 0\nqueryScheduler:\n  replicas: 0\ndistributor:\n  replicas: 0\ncompactor:\n  replicas: 0\nindexGateway:\n  replicas: 0\nbloomCompactor:\n  replicas: 0\nbloomGateway:\n  replicas: 0\nchunksCache:\n  enabled: false\nresultsCache:\n  enabled: false\n\nEOF\n</code></pre>"},{"location":"setup-loki-grafana-stack/#install-helm-chart","title":"Install helm-chart","text":"<pre><code>helm repo add grafana https://grafana.github.io/helm-charts\nhelm repo update\n\nREPO_NAME=grafana\nREPO_PATH=loki\nCHART_VERSION=6.5.2\nCHART_APP_VERSION=loki\nhelm upgrade --install loki ${REPO_NAME}/${REPO_PATH}  --version ${CHART_VERSION} -f $PWD/loki-demo-values.yaml -n monitoring\n#helm uninstall loki  -n monitoring\n</code></pre>"},{"location":"setup-loki-grafana-stack/#test","title":"Test","text":""},{"location":"setup-loki-grafana-stack/#sending-logs-to-loki","title":"Sending logs to Loki","text":"<p>You can send logs from inside the cluster using the cluster DNS: </p> <p>http://loki.monitoring.svc.cluster.local:3100/loki/api/v1/push</p> <p>You can test to send data from outside the cluster by port-forwarding the gateway to your local machine:</p> <pre><code>kubectl port-forward --namespace monitoring svc/loki 3100:3100 &amp;\n</code></pre> <p>And then using http://127.0.0.1:3100/loki/api/v1/push URL as shown below:</p> <p><pre><code>curl -H \"Content-Type: application/json\" -XPOST -s \"http://127.0.0.1:3100/loki/api/v1/push\"  \\\n--data-raw \"{\\\"streams\\\": [{\\\"stream\\\": {\\\"job\\\": \\\"test\\\"}, \\\"values\\\": [[\\\"$(date +%s)000000000\\\", \\\"fizzbuzz\\\"]]}]}\" \\\n-H X-Scope-OrgId:foo\n</code></pre> Then verify that Loki did received the data using the following command: <pre><code>curl \"http://127.0.0.1:3100/loki/api/v1/query_range\" --data-urlencode 'query={job=\"test\"}' -H X-Scope-OrgId:foo | jq .data.result\n</code></pre></p>"},{"location":"setup-loki-grafana-stack/#connecting-grafana-to-loki","title":"Connecting Grafana to Loki","text":"<p>If Grafana operates within the cluster, you'll set up a new Loki datasource by utilizing the following URL: http://loki.monitoring.svc.cluster.local:3100/</p>"},{"location":"setup-loki-grafana-stack/#install-promtail","title":"Install promtail","text":"<pre><code>cat&lt;&lt;EOF &gt;$PWD/promtail-demo-values.yaml\nconfig:\n  clients:\n    - url: http://loki:3100/loki/api/v1/push\n      tenant_id: 1\nextraPorts:\n   syslog:\n     name: tcp-syslog\n     annotations: {}\n     labels: {}\n     containerPort: 8514\n     protocol: TCP\n     service:\n       type: ClusterIP\n       clusterIP: null\n       port: 1514\n       externalIPs: []\n       nodePort: null\n       loadBalancerIP: null\n       loadBalancerSourceRanges: []\n       externalTrafficPolicy: null\nEOF\n</code></pre>"},{"location":"setup-loki-grafana-stack/#install-promtail_1","title":"Install Promtail","text":"<pre><code>REPO_NAME=grafana\nREPO_PATH=promtail\nCHART_VERSION=6.15.3\nCHART_APP_VERSION=promtail\nhelm install promtail ${REPO_NAME}/${REPO_PATH}  --version ${CHART_VERSION} -f $PWD/promtail-demo-values.yaml -n monitoring\n</code></pre>"},{"location":"setup-loki-grafana-stack/#install-grafana","title":"Install Grafana","text":"<pre><code>REPO_NAME=grafana\nREPO_PATH=grafana\nCHART_VERSION=7.3.11\nCHART_APP_VERSION=loki\nhelm upgrade --install grafana ${REPO_NAME}/${REPO_PATH}  --version ${CHART_VERSION} -n monitoring\nkubectl get secret --namespace monitoring grafana -o jsonpath=\"{.data.admin-password}\" | base64 --decode ; echo\nkubectl  -n monitoring port-forward svc/grafana 3000:80 --address 0.0.0.0 &amp;\n</code></pre>"},{"location":"setup-loki-grafana-stack/#add-loki-data-source","title":"[Add Loki data source ]","text":"<pre><code>URL: http://loki:3100\n#ADD Http Header\nX-Scope-OrgID: 1\n</code></pre>"},{"location":"setup-loki-grafana-stack/#deploy-demo-pod","title":"[Deploy demo pod]","text":"<p><pre><code>k run demo --image=narenp/loggen:v2\n</code></pre> visit https://grafana-url.com/explore?container=demo</p>"},{"location":"setup-loki-grafana-stack/#add-basic-dashboard","title":"Add basic dashboard","text":"<pre><code>15141\nhttps://grafana.com/grafana/dashboards/15141-kubernetes-service-logs/\n</code></pre> <p>Visit http://localhost:3000/dashboard/new?orgId=1</p> <p>Add example: loki-general-dashboard.json</p>"},{"location":"setup-monitoring-stack/","title":"Setting up Monitoring Stack in a Node (Docker Container):","text":""},{"location":"setup-monitoring-stack/#alertmanager-setup-run-alertmanagersh","title":"AlertManager Setup | run-alertmanager.sh","text":"<pre><code>#! /bin/bash\n\nname=$1\ndefault_value=\"demo\"\nname=${name:-$default_value}\nmkdir -p alertmanager\ncat &lt;&lt;EOF &gt;${PWD}/alertmanager/alertmanager.yml\nroute:\n  group_by: ['alertname']\n  group_wait: 30s\n  group_interval: 5m\n  repeat_interval: 1h\n  receiver: 'web.hook'\nreceivers:\n  - name: 'web.hook'\n    webhook_configs:\n      - url: 'http://127.0.0.1:5001/'\ninhibit_rules:\n  - source_match:\n      severity: 'critical'\n    target_match:\n      severity: 'warning'\n    equal: ['alertname', 'dev', 'instance']\nEOF\n\nalertmanager_name=${name}-alertmanager\nalertmanager_host_port=9093\nmkdir -p ${alertmanager_name}\n\ndocker rm ${alertmanager_name} -f\ndocker run -d --restart unless-stopped --network host \\\n    -v ${PWD}/alertmanager:/etc/alertmanager \\\n    --name=${alertmanager_name} \\\n    prom/alertmanager\n\ndocker ps -l\n</code></pre>"},{"location":"setup-monitoring-stack/#grafana-setup-run-grafanash","title":"Grafana setup | run-grafana.sh","text":"<pre><code>#! /bin/bash\n\nname=$1\ndefault_value=\"demo\"\nname=${name:-$default_value}\n\n#scanner-grafana\ngrafana_name=${name}-grafana\ngrafana_host_port=3000\ndocker rm ${grafana_name} -f\ndocker run -d --restart unless-stopped --network host \\\n    --name=${grafana_name} \\\n    -v ${PWD}/${grafana_name}/plugins:/var/grafana/plugins \\\n    -v ${PWD}/${grafana_name}/provisioning:/etc/grafana/provisioning \\\n    grafana/grafana\n\ndocker ps -l\n</code></pre>"},{"location":"setup-monitoring-stack/#node-exporter-setup-run-node-exportersh","title":"node-exporter setup | run-node-exporter.sh","text":"<pre><code>#! /bin/bash\ntext_collector_dir=/var/lib/node-exporter/textfile_collector\nmkdir -p ${text_collector_dir}\n\ncat &lt;&lt;EOF &gt;/etc/cron.d/directory_size\n*/5 * * * * root du -sb /var/log /var/cache/apt /var/lib/prometheus | sed -ne 's/^\\([0-9]\\+\\)\\t\\(.*\\)$/node_directory_size_bytes{service=\"naren\",directory=\"\\2\"} \\1/p' &gt; ${text_collector_dir}/directory_size.prom.$$ &amp;&amp; mv ${text_collector_dir}/directory_size.prom.$$ ${text_collector_dir}/directory_size.prom\nEOF\nname=$1\ndefault_value=\"demo\"\nname=${name:-$default_value}\nnode_exporter_name=${name}-node-exporter\nnode_exporter_host_port=9100\n\ndocker rm ${node_exporter_name} -f\ndocker run -d --restart unless-stopped --network host \\\n    -v ${text_collector_dir}:${text_collector_dir} \\\n    --name=${node_exporter_name} \\\n    prom/node-exporter --collector.textfile.directory=${text_collector_dir}\n\ndocker ps -l\n</code></pre>"},{"location":"setup-monitoring-stack/#prometheus-setup-run-prometheussh","title":"Prometheus setup | run-prometheus.sh","text":"<pre><code>#! /bin/bash\nname=$1\ndefault_value=\"demo\"\nname=${name:-$default_value}\nprometheus_name=${name}-prometheus\nmkdir -p ${PWD}/$prometheus_name\ncat &lt;&lt;EOF &gt;${PWD}/$prometheus_name/prometheus.yml\nglobal:\n  scrape_interval: 15s\n  scrape_timeout: 10s\n  evaluation_interval: 15s\nscrape_configs:\n  - job_name: prometheus\n    metrics_path: /metrics\n    scheme: http\n    static_configs:\n      - targets:\n          - localhost:9090\n  - job_name: node-exporter\n    metrics_path: /metrics\n    scheme: http\n    static_configs:\n      - targets:\n          - localhost:9100\n  - job_name: victoria-metrics \n    metrics_path: /metrics\n    scheme: http\n    static_configs:\n      - targets:\n          - localhost:8428\n  - job_name: vmagent \n    metrics_path: /metrics\n    scheme: http\n    static_configs:\n      - targets:\n          - localhost:8429\nEOF\n\nprometheus_host_port=9090\ndocker volume create prometheus-data\nmkdir -p ${prometheus_name}\ndocker rm ${prometheus_name} -f\ndocker run -d --restart unless-stopped --network host \\\n  --name=${prometheus_name} \\\n  -v ${PWD}/$prometheus_name/:/etc/prometheus/ \\\n  -v prometheus-data:/prometheus \\\n  prom/prometheus\ndocker ps -l\n</code></pre>"},{"location":"setup-monitoring-stack/#install-vmagent-run-vmagentsh","title":"Install vmagent | run-vmagent.sh","text":"<p>VM Agent which will scrape selected time series from local Prometheus server, where <code>service=\"naren\"</code> <pre><code>#! /bin/bash\nname=$1\ndefault_value=\"demo\"\nname=${name:-$default_value}\nvmagent_name=${name}-vmagent\nmkdir -p ${PWD}/$vmagent_name\n\nremoteWrite_url=\"http://localhost:8428/api/v1/write\"\ncat &lt;&lt;EOF &gt; ${PWD}/$vmagent_name/relabel.yml\n- target_label: \"node\"\n  replacement: \"local\"\nEOF\n\ncat &lt;&lt;EOF &gt;${PWD}/$vmagent_name/prometheus.yml\nscrape_configs:\n  - job_name: 'federate'\n    scrape_interval: 15s\n\n    honor_labels: true\n    metrics_path: '/federate'\n\n    params:\n      'match[]':\n        - '{service=\"naren\"}'\n        - '{__name__=~\"up|vm_.*\"}'\n    static_configs:\n      - targets:\n          - localhost:9090\nEOF\n\ndocker volume create vmagentdata\ndocker rm ${vmagent_name} -f\n\ndocker run -d --restart unless-stopped --network host \\\n  --name=${vmagent_name} \\\n  -v ${PWD}/$vmagent_name:/etc/prometheus/ \\\n  -v ${PWD}/vma:/opt/ \\\n  -v vmagentdata:/vmagentdata \\\n  victoriametrics/vmagent -remoteWrite.url=$remoteWrite_url -remoteWrite.urlRelabelConfig=/etc/prometheus/relabel.yml -remoteWrite.forceVMProto -promscrape.config=/etc/prometheus/prometheus.yml \n\ndocker ps -l\n</code></pre></p>"},{"location":"setup-monitoring-stack/#install-victoria-metrics-run-victoria-metricssh","title":"Install victoria-metrics | run-victoria-metrics.sh","text":"<pre><code>#! /bin/bash\n\nname=$1\ndefault_value=\"demo\"\nname=${name:-$default_value}\ndocker volume create victoria-metrics-data\n# victoria-metrics\nvictoria_metrics_name=${name}-victoria-metrics\nvictoria_metrics_host_port=8428\ndocker rm ${victoria_metrics_name} -f\ndocker run -d --restart unless-stopped --network host \\\n    --name=${victoria_metrics_name} \\\n    -v victoria-metrics-data:/victoria-metrics-data \\\n    victoriametrics/victoria-metrics\n\ndocker ps -l\n</code></pre>"},{"location":"setup-monitoring-stack/#install-the-monitoring-stack-installsh","title":"Install the monitoring stack | install.sh","text":"<pre><code>name=$1\ndefault_value=\"demo\"\nname=${name:-$default_value}\n\necho run-alertmanager.sh $name\nsource run-alertmanager.sh $name 1&gt;/dev/null\necho \"---\"\n\necho run-prometheus.sh $name\nsource run-prometheus.sh $name 1&gt;/dev/null\necho \"---\"\n\necho run-node-exporter.sh $name\nsource run-node-exporter.sh $name 1&gt;/dev/null\necho \"---\"\n\necho run-victoria-metrics.sh $name\nsource run-victoria-metrics.sh $name 1&gt;/dev/null\necho \"---\"\n\necho run-vmagent.sh $name\nsource run-vmagent.sh $name 1&gt;/dev/null\necho \"---\"\n\necho run-grafana.sh $name\nsource run-grafana.sh $name 1&gt;/dev/null\necho \"---\"\n\ndocker ps | grep -E \"STATUS|$name\"\n</code></pre>"},{"location":"setup-monitoring-stack/#uninstall-the-monitoring-stack-uninstallsh","title":"Uninstall the monitoring stack | uninstall.sh","text":"<pre><code>#! /bin/bash\nname=$1\ndefault_value=\"demo\"\nname=${name:-$default_value}\n\ndocker ps | grep $name | awk '{print $1}' | xargs docker rm -f\n</code></pre>"},{"location":"setup-monitoring-stack/#ref","title":"Ref:","text":"<ul> <li>Demo Environment</li> <li>monitoring-stack.git</li> </ul>"},{"location":"sync-my-git-repo/","title":"Sync one git repo to another git repo","text":"<pre><code>#!/bin/bash\n\ngit config --global http.sslVerify false\ngit config --global advice.detachedHead false\n\nSOURCE_GIT_URL=$1  # format: ${USERID}:${PASSWORD}@${GITURL}/$GROUP\nTARGET_GIT_URL=$2  # format: ${GITURL}/$GROUP\n\nsync() {\n            REPO_NAME=$1\n            REPO_VERSION=$2\n\n            echo clone source git ...\n            echo \n\n            git clone https://${SOURCE_GIT_URL}/${REPO_NAME}.git --branch ${REPO_VERSION}  --single-branch\n            cd ${REPO_NAME} || exit\n\n            echo push target git ...\n            echo \n\n            git push https://${TARGET_GIT_URL}/${REPO_NAME}.git tag ${REPO_VERSION}\n            if [ $? -eq 0 ]; then\n                  echo \"${REPO_NAME}.git --branch ${REPO_VERSION} pushed to ${TARGET_GIT_URL} .\"\n            else\n                  echo \"Push failed.\"\n            fi\n\n      }\nsync $3 $4\n</code></pre>"},{"location":"sync-my-git-repo/#run-the-script-give-the-userid-and-password-of-target-git","title":"Run the script &amp; give the userid and password of target git","text":"<pre><code>bash syncmygit #SOURCE_GIT_URL $TARGET_GIT_URL $REPO_NAME $REPO_VERSION\n</code></pre>"},{"location":"unlimited-monitoring-data-by-thanos/","title":"Setting up Thanos for long term storage of prometheus metrics","text":""},{"location":"unlimited-monitoring-data-by-thanos/#generate-1-yr-of-test-data","title":"Generate 1 yr of test data","text":"<pre><code>mkdir -p /root/prom-eu1 &amp;&amp; docker run -i quay.io/thanos/thanosbench:v0.2.0-rc.1 block plan -p continuous-365d-tiny --labels 'cluster=\"eu1\"' --max-time=6h | docker run -v /root/prom-eu1:/prom-eu1 -i quay.io/thanos/thanosbench:v0.2.0-rc.1 block gen --output.dir prom-eu1\nls -lR /root/prom-eu1\n</code></pre>"},{"location":"unlimited-monitoring-data-by-thanos/#setup-prometheus","title":"Setup Prometheus","text":"<pre><code>cat&lt;&lt;EOF&gt;prometheus0_eu1.yml\nglobal:\n  scrape_interval: 5s\n  external_labels:\n    cluster: eu1\n    replica: 0\n    tenant: team-eu # Not needed, but a good practice if you want to grow this to multi-tenant system some day.\n\nscrape_configs:\n  - job_name: 'prometheus'\n    static_configs:\n      - targets: ['127.0.0.1:9090']\n  - job_name: 'sidecar'\n    static_configs:\n      - targets: ['127.0.0.1:19090']\n  - job_name: 'minio'\n    metrics_path: /minio/prometheus/metrics\n    static_configs:\n      - targets: ['127.0.0.1:9000']\n  - job_name: 'querier'\n    static_configs:\n      - targets: ['127.0.0.1:9091']\n  - job_name: 'store_gateway'\n    static_configs:\n      - targets: ['127.0.0.1:19091']\nEOF\n\ndocker run -d --net=host --rm \\\n    -v $(pwd)/prometheus0_eu1.yml:/etc/prometheus/prometheus.yml \\\n    -v $(pwd)/prom-eu1:/prometheus \\\n    -u root \\\n    --name prometheus-0-eu1 \\\n    quay.io/prometheus/prometheus:v2.38.0 \\\n    --config.file=/etc/prometheus/prometheus.yml \\\n    --storage.tsdb.retention.time=1000d \\\n    --storage.tsdb.path=/prometheus \\\n    --storage.tsdb.max-block-duration=2h \\\n    --storage.tsdb.min-block-duration=2h \\\n    --web.listen-address=:9090 \\\n    --web.external-url=https://63c2acb8-8951-4fc9-aae9-0ec136d51ca7-10-244-5-136-9090.papa.r.killercoda.com \\\n    --web.enable-lifecycle \\\n    --web.enable-admin-api\n</code></pre>"},{"location":"unlimited-monitoring-data-by-thanos/#add-a-side-car-which-will-expose-the-prometheus-metrics-at-both-http-and-grpc-port","title":"Add a Side car which will expose the prometheus metrics at both http and grpc port","text":"<pre><code>docker run -d --net=host --rm \\\n    --name prometheus-0-eu1-sidecar \\\n    -u root \\\n    quay.io/thanos/thanos:v0.28.0 \\\n    sidecar \\\n    --http-address 0.0.0.0:19090 \\\n    --grpc-address 0.0.0.0:19190 \\\n    --prometheus.url http://172.17.0.1:9090\n</code></pre>"},{"location":"unlimited-monitoring-data-by-thanos/#then-host-a-querier-which-will-pull-data-from-thanos-side-car-and-expose-it-further","title":"Then host a querier which will pull data from Thanos-side car and expose it further","text":"<pre><code>docker run -d --net=host --rm \\\n    --name querier \\\n    quay.io/thanos/thanos:v0.28.0 \\\n    query \\\n    --http-address 0.0.0.0:9091 \\\n    --query.replica-label replica \\\n    --store 172.17.0.1:19190\n</code></pre>"},{"location":"unlimited-monitoring-data-by-thanos/#run-grafana-to-see-the-metrics","title":"Run Grafana to see the metrics","text":"<pre><code>docker run -d --net=host --name grafana grafana/grafana\n# Create data source http://localhost:9091 , type prometheus \n</code></pre>"},{"location":"unlimited-monitoring-data-by-thanos/#run-local-s3-minio","title":"Run local S3 minio","text":"<pre><code>mkdir /root/minio &amp;&amp; \\\ndocker run -d --rm --name minio \\\n     -v /root/minio:/data \\\n     -p 9000:9000 -e \"MINIO_ACCESS_KEY=minio\" -e \"MINIO_SECRET_KEY=melovethanos\" \\\n     minio/minio:RELEASE.2019-01-31T00-31-19Z \\\n     server /data\n\nmkdir /root/minio/thanos # the new bucket \n</code></pre>"},{"location":"unlimited-monitoring-data-by-thanos/#create-the-s3-configuration","title":"Create the S3 configuration","text":"<pre><code>cat&lt;&lt;EOF &gt; bucket_storage.yaml\ntype: S3\nconfig:\n  bucket: \"thanos\"\n  endpoint: \"172.17.0.1:9000\"\n  insecure: true\n  signature_version2: true\n  access_key: \"minio\"\n  secret_key: \"melovethanos\"\n\nEOF\n</code></pre>"},{"location":"unlimited-monitoring-data-by-thanos/#configure-the-side-car-to-push-the-2hr-chunk-to-s3-minio","title":"Configure the side car to push the 2hr chunk to S3-minio","text":"<pre><code>docker stop prometheus-0-eu1-sidecar\ndocker run -d --net=host --rm \\\n    -v $(pwd)/bucket_storage.yaml:/etc/thanos/minio-bucket.yaml \\\n    -v $(pwd)/prom-eu1:/prometheus \\\n    --name prometheus-0-eu1-sidecar \\\n    -u root \\\n    quay.io/thanos/thanos:v0.28.0 \\\n    sidecar \\\n    --tsdb.path /prometheus \\\n    --objstore.config-file /etc/thanos/minio-bucket.yaml \\\n    --shipper.upload-compacted \\\n    --http-address 0.0.0.0:19090 \\\n    --grpc-address 0.0.0.0:19190 \\\n    --prometheus.url http://172.17.0.1:9090\n</code></pre>"},{"location":"unlimited-monitoring-data-by-thanos/#run-store-gateway-to-pull-from-s3-and-expose-to-querier","title":"Run Store Gateway to pull from S3 and expose to querier","text":"<pre><code>docker run -d --net=host --rm \\\n    -v /root/editor/bucket_storage.yaml:/etc/thanos/minio-bucket.yaml \\\n    --name store-gateway \\\n    quay.io/thanos/thanos:v0.28.0 \\\n    store \\\n    --objstore.config-file /etc/thanos/minio-bucket.yaml \\\n    --http-address 0.0.0.0:19091 \\\n    --grpc-address 0.0.0.0:19191\n</code></pre>"},{"location":"unlimited-monitoring-data-by-thanos/#reconfigure-the-querier-to-pull-from-both-side-car-and-store-and-expose","title":"ReConfigure the querier to pull from both side-car and Store and expose","text":"<pre><code>docker stop querier &amp;&amp; \\\ndocker run -d --net=host --rm \\\n   --name querier \\\n   quay.io/thanos/thanos:v0.28.0 \\\n   query \\\n   --http-address 0.0.0.0:9091 \\\n   --query.replica-label replica \\\n   --store 172.17.0.1:19190 \\\n   --store 172.17.0.1:19191\n ```\n\n# Run Compactor to reduce the metric size by deleting redundant or dupicate metrics  \n```bash\ndocker run -d --net=host --rm \\\n -v /root/bucket_storage.yaml:/etc/thanos/minio-bucket.yaml \\\n    --name thanos-compact \\\n    quay.io/thanos/thanos:v0.28.0 \\\n    compact \\\n    --wait --wait-interval 30s \\\n    --consistency-delay 0s \\\n    --objstore.config-file /etc/thanos/minio-bucket.yaml \\\n    --http-address 0.0.0.0:19095\n</code></pre> <ul> <li>Demo Environment</li> </ul>"},{"location":"vector/","title":"Take Control of Your Observability Data with  vector.dev","text":"<p> A lightweight, ultra-fast tool for building observability pipelines</p> <p>Vector is a high-performance, end-to-end (agent &amp; aggregator) observability data pipeline that puts you in control of your observability data. Collect, transform, and route all your logs and metrics to any vendors you want today and any other vendors you may want tomorrow.  Vector enables dramatic cost reduction, novel data enrichment, and data security where you need it, not where it is most convenient for your vendors.  Additionally, it is open source and up to 10x faster than every alternative in the space.</p> <p>More details: vector.dev </p>"},{"location":"vector/#example-installation","title":"Example Installation","text":""},{"location":"vector/#server-setup","title":"Server Setup","text":"<p>Aggregator <code>vector.yaml</code> Configuration File <pre><code>mkdir -p aggregator\ncat &lt;&lt;-EOF &gt; $PWD/aggregator/vector.yaml\ndata_dir: /vector-data-dir\napi:\n  enabled: true\n  address: 0.0.0.0:8686\nsources:\n  vector:\n     address: 0.0.0.0:6000\n     type: vector\n     version: \"2\"\n  my_internal_logs:\n    type: internal_logs\n  my_internal_metrics:\n    type: internal_metrics\ntransforms:\n  parse_logs:\n    type: \"remap\"\n    inputs: [vector,my_internal_logs,my_internal_metrics]\n    source: |\n      . = parse_syslog!(string!(.message)) \nsinks:\n  stdout:\n    type: console\n    inputs: [parse_logs]\n    encoding:\n        codec: json\nEOF\n</code></pre></p> <p>Configuration Ref: - api:   -   https://vector.dev/docs/reference/api/ - source:   -   https://vector.dev/docs/reference/configuration/sources/vector   -   https://vector.dev/docs/reference/configuration/sources/internal_metrics/   -   https://vector.dev/docs/reference/configuration/sources/internal_logs/</p> <p>- transforms:   -   https://vector.dev/docs/reference/configuration/transforms/remap/</p> <p>- sink:   -   https://vector.dev/docs/reference/configuration/sinks/console/</p> <p>Depoly the Server (vector aggregator) <pre><code># docker rm -f vector-aggregator \ndocker run -d --rm --name vector-aggregator -v $(pwd)/aggregator:/etc/vector/  -p 8686:8686 -p 6000:6000 docker.io/timberio/vector:0.41.1-alpine\n</code></pre></p>"},{"location":"vector/#clients-setup","title":"Client(s) Setup","text":"<p>Vector Agent <code>vector.yaml</code> Configuration File <pre><code>mkdir -p agent\ncat &lt;&lt;-EOF &gt; $PWD/agent/vector.yaml\ndata_dir: /vector-data-dir\napi:\n  enabled: false\n  address: 0.0.0.0:8686\nsources:\n  dummy_logs:\n    type: \"demo_logs\"\n    format: \"syslog\"\n    interval: 1\n  my_internal_logs:\n    type: internal_logs\n  my_internal_metrics:\n    type: internal_metrics\ntransforms:\n  parse_logs:\n    type: \"remap\"\n    inputs: [\"dummy_logs\"]\n    source: |\n      . = parse_syslog!(string!(.message))\nsinks:\n  vector_sink:\n    type: vector\n    inputs:\n      - dummy_logs\n    address: 172.17.0.1:6000  #Change me\n  stdout:\n    type: console\n    inputs: [parse_logs,my_internal_logs,my_internal_metrics]\n    encoding:\n        codec: json\nEOF\n</code></pre> Configuration Ref:</p> <ul> <li>source:</li> <li>https://vector.dev/docs/reference/configuration/sources/demo_logs/</li> <li>https://vector.dev/docs/reference/configuration/sources/internal_metrics/</li> <li> <p>https://vector.dev/docs/reference/configuration/sources/internal_logs/</p> </li> <li> <p>transforms:</p> </li> <li> <p>https://vector.dev/docs/reference/configuration/transforms/remap/</p> </li> <li> <p>sink:</p> </li> <li>https://vector.dev/docs/reference/configuration/sinks/vector/</li> </ul> <p>Depoly the Client (vector agent) <pre><code>#docker rm -f vector-agent \ndocker run -d --name vector-agent -v $(pwd)/agent:/etc/vector/ --rm docker.io/timberio/vector:0.41.1-alpine\n</code></pre> Now, your Vector Aggregator and Vector Agent are set up and running. They can efficiently collect, transform, and route logs and metrics, offering complete control over your observability data pipeline. <pre><code>This `README.md` file provides clear instructions on setting up both the server and client components of the Vector pipeline, including configuration references and deployment commands.\n</code></pre></p>"},{"location":"victorialogs-demo/","title":"Victorialogs demo","text":""},{"location":"victorialogs-demo/#exploring-victorialogs-a-fast-and-resource-efficient-log-management-solution","title":"\ud83d\ude80 Exploring VictoriaLogs: A Fast and Resource-Efficient Log Management Solution","text":"<p>VictoriaLogs is an open-source, user-friendly database for logs from VictoriaMetrics, designed to be efficient, scalable, and easy to operate.</p>"},{"location":"victorialogs-demo/#victorialogs-provides-the-following-features","title":"VictoriaLogs provides the following features:","text":"<ul> <li>It is recource-efficient and fast.</li> <li>It uses up to 30x less RAM and up to 15x less disk space than other solutions such as Elasticsearch and Grafana Loki. See benchmarks and this article for details.</li> <li>VictoriaLogs\u2019 capacity and performance scales linearly with the available resources (CPU, RAM, disk IO, disk space).</li> <li>It runs smoothly on Raspberry PI and on servers with hundreds of CPU cores and terabytes of RAM.</li> <li>It can accept logs from popular log collectors.</li> <li>It is much easier to set up and operate compared to Elasticsearch and Grafana Loki, since it is basically zero-config.</li> <li>It provides easy yet powerful query language with full-text search capabilities across all the log fields. See LogsQL docs.</li> <li>It provides built-in web UI for logs\u2019 exploration.</li> <li>It provides Grafana plugin for building arbitrary dashboards in Grafana.</li> <li>It provides interactive command-line tool for querying VictoriaLogs.</li> <li>It can be seamlessly combined with good old Unix tools for log analysis such as grep, less, sort, jq, etc. See these docs for details.</li> <li>It support log fields with high cardinality (e.g. high number of unique values) such as trace_id, user_id and ip.</li> <li>It is optimized for logs with hundreds of fields (aka wide events).</li> <li>It supports multitenancy.</li> <li>It supports out-of-order logs\u2019 ingestion aka backfilling.</li> <li>It supports live tailing for newly ingested logs.</li> <li>It supports selecting surrounding logs in front and after the selected logs.</li> <li>It supports alerting</li> </ul> <p>\ud83d\udd27 My Setup Steps:</p> <p>1\ufe0f\u20e3 Installed VictoriaLogs using Helm charts.</p> <p>2\ufe0f\u20e3 Exposed the Web UI for quick log exploration.</p> <p>3\ufe0f\u20e3 Deployed Promtail for log forwarding.</p> <p>4\ufe0f\u20e3 Integrated Grafana to visualize logs with dashboards.</p>"},{"location":"victorialogs-demo/#installed-victorialogs-using-helm-charts","title":"Installed VictoriaLogs using Helm charts.","text":"<p><pre><code>helm repo add vm https://victoriametrics.github.io/helm-charts/\nhelm repo update\nhelm search repo vm/victoria-logs-single -l\nhelm show values vm/victoria-logs-single &gt; victoria-logs-single-values.yaml\n</code></pre> Install victoria-logs-single <pre><code>touch  vls-values.yaml\nhelm upgrade --install vls vm/victoria-logs-single -f vls-values.yaml\n#Verify\nkubectl get pod -l app.kubernetes.io/instance=vls,app.kubernetes.io/name=victoria-logs-single\n</code></pre> Check the UI  <pre><code>nohup kubectl port-forward vls-victoria-logs-single-server-0 9428 --address 0.0.0.0 &amp;\n</code></pre> </p>"},{"location":"victorialogs-demo/#deployed-promtail-for-log-forwarding","title":"Deployed Promtail for log forwarding.","text":"<p><pre><code>helm repo add grafana https://grafana.github.io/helm-charts\nhelm repo update\nhelm search repo grafana/promtail  -l\nhelm show values grafana/promtail &gt; promtail-values.yaml\n</code></pre> Prepare the value file  <pre><code>cat&lt;&lt;EOF &gt;gpt-values.yaml\nconfig:\n  clients:\n    - url: \"http://vls-victoria-logs-single-server.default.svc.cluster.local:9428/insert/loki/api/v1/push\"\nEOF\nhelm upgrade --install gpt grafana/promtail -f gpt-values.yaml\n\n#Verify \nkubectl get pod -l app.kubernetes.io/instance=gpt,app.kubernetes.io/name=promtail\nnohup kubectl --namespace default port-forward daemonset/gpt-promtail 3101 --address 0.0.0.0 &amp;\ncurl http://127.0.0.1:3101/metrics\n</code></pre></p>"},{"location":"victorialogs-demo/#exposed-the-web-ui-for-quick-log-exploration-aka-vmui","title":"Exposed the Web UI for quick log exploration aka VMUI","text":"<p>Access the ui : https://localhost:9428/select/vmui </p>"},{"location":"victorialogs-demo/#integrated-grafana-to-visualize-logs-with-dashboards","title":"Integrated Grafana to visualize logs with dashboards.","text":"<p><pre><code>helm repo add grafana https://grafana.github.io/helm-charts\nhelm repo update\n\nhelm search repo grafana/grafana  -l\nhelm show values grafana/grafana &gt; grafana-values.yaml\n\ncat&lt;&lt;EOF &gt;gg-values.yaml\nenv:\n  GF_INSTALL_PLUGINS: \"victoriametrics-logs-datasource\"\nEOF\n# serve_from_sub_path=true\n\nhelm upgrade --install gg grafana/grafana -f gg-values.yaml\n\nkubectl get secret --namespace default gg-grafana -o jsonpath=\"{.data.admin-password}\" | base64 --decode ; echo\n\nexport POD_NAME=$(kubectl get pods --namespace default -l \"app.kubernetes.io/name=grafana,app.kubernetes.io/instance=gg\" -o jsonpath=\"{.items[0].metadata.name}\")\nnohup kubectl --namespace default port-forward svc/gg-grafana 3000:80 --address 0.0.0.0 &amp;\n</code></pre> </p>"},{"location":"vmbackup_and_vmrestore/","title":"Local setup for testing vmbackup and vmrestore","text":""},{"location":"vmbackup_and_vmrestore/#complete-the-test-setup-for-monitoring-stack","title":"Complete the test setup for monitoring-stack","text":"<p>ref: Setting up Monitoring Stack in a Node (docker container)</p>"},{"location":"vmbackup_and_vmrestore/#run-minio-for-locals3-run-miniosh","title":"Run minio for local:S3 | run-minio.sh","text":"<pre><code>#! /bin/bash\n\nname=$1\ndefault_value=\"demo\"\nname=${name:-$default_value}\ndocker volume create minio-data\n\n# minio\nminio_name=${name}-minio\nminio_host_port=9000,9001\ndocker rm ${minio_name} -f\ndocker run -d --restart unless-stopped --network host \\\n    --name=${minio_name} \\\n    -v minio-data:/data \\\n    -e \"MINIO_ROOT_USER=ROOTNAME\" \\\n    -e \"MINIO_ROOT_PASSWORD=CHANGEME123\" \\\n    quay.io/minio/minio server /data --console-address \":9001\"\n\ndocker ps -l\n</code></pre>"},{"location":"vmbackup_and_vmrestore/#install-the-mc-client-for-creating-the-bucket","title":"Install the mc client for creating the bucket","text":"<pre><code>docker run --privileged -v ${PWD}:/tmp -it --network host --entrypoint=/bin/sh minio/mc\n\nS3_ALIAS=demo\nS3_ENDPOINT=http://localhost:9000\nACCESS_KEY=ROOTNAME\nSECRET_KEY=CHANGEME123\nBUCKET_NAME=data\nmc alias set $S3_ALIAS $S3_ENDPOINT $ACCESS_KEY $SECRET_KEY --api \"s3v4\" --path \"auto\"\n\nmc --insecure rm -r --force $S3_ALIAS/$BUCKET_NAME\nmc --insecure mb $BUCKET_NAME\n\n#ref: https://github.com/minio/minio/issues/4769#issuecomment-320319655\n</code></pre>"},{"location":"vmbackup_and_vmrestore/#take-the-backup","title":"Take the Backup","text":"<pre><code>#!/bin/bash\ncat &lt;&lt;EOF &gt;/etc/credentials\n[default]\naws_access_key_id=ROOTNAME\naws_secret_access_key=CHANGEME123\nEOF\n\ndocker run -v victoria-metrics-data:/victoria-metrics-data --network host victoriametrics/vmbackup -storageDataPath=/victoria-metrics-data -snapshot.createURL=http://localhost:8428/snapshot/create -dst=s3://localhost:9000/data -credsFilePath=/etc/credentials -customS3Endpoint=http://localhost:9000\n</code></pre>"},{"location":"vmbackup_and_vmrestore/#do-the-restore","title":"Do the Restore","text":"<pre><code>cat&lt;&lt;EOF&gt;/etc/credentials\n[default]\naws_access_key_id=ROOTNAME\naws_secret_access_key=CHANGEME123\nEOF\n\ndocker run  -v victoria-metrics-data:/victoria-metrics-data --network host victoriametrics/vmrestore -storageDataPath=/victoria-metrics-data -snapshot.createURL=http://localhost:8428/snapshot/create    -src=s3://localhost:9000/data -credsFilePath=/etc/credentials -customS3Endpoint=http://localhost:9000\n</code></pre> <p>https://github.com/VictoriaMetrics/VictoriaMetrics/issues/353</p>"},{"location":"vmbackup_and_vmrestore/#ref","title":"Ref:","text":"<ul> <li>Demo Environment</li> <li>monitoring-stack.git</li> </ul>"}]}