| #  | Title                               | Why Kafka (Problem)                            | When to Use It                                                            | How to Apply It                                      | What Kafka Concept Solves It       | Example Scenario                                            |
| -- | ----------------------------------- | ---------------------------------------------- | ------------------------------------------------------------------------- | ---------------------------------------------------- | ---------------------------------- | ----------------------------------------------------------- |
| 1  | Explosion of Data Volume            | Need to handle **billions of events** at scale | When your system ingests continuous, massive streams                      | Scale horizontally with **partitions**               | **Topics + Partitions**            | LinkedIn handling profile views, job clicks, ad impressions |
| 2  | Multiple Consumers, One Data Source | Multiple teams need the **same data**          | When analytics, fraud detection, and reporting all need identical streams | Use independent **consumer groups**                  | **Consumer Groups**                | Payment service → Fraud team + Analytics + Data Warehouse   |
| 3  | Replay for Debugging & ML           | Debugging & ML need **data replay**            | When you need to reprocess past data                                      | Enable **log retention** & control **offsets**       | **Offsets + Retention**            | Replaying 1 year of user clicks to re-train an ML model     |
| 4  | One Pipeline for Real-time + Batch  | Must support both **real-time + batch**        | When you want one pipeline for dashboards + ETL                           | Mix stream consumers & batch consumers (via Connect) | **Kafka Connect + Stream APIs**    | E-commerce system: live inventory + nightly sales reports   |
| 5  | Fault Tolerance by Design           | Prevent **data loss** on failures              | When uptime and durability are mission-critical                           | Replicate partitions across brokers                  | **Replication + Leader/Follower**  | Bank processing transactions with 0 tolerance for loss      |
| 6  | Decoupling Producers & Consumers    | Avoid **producer-consumer coupling**           | When new consumers must join without disturbing producers                 | Producers write once, consumers subscribe freely     | **Decoupled Pub/Sub**              | IoT sensor data → analytics, monitoring, ML pipelines       |
| 7  | Guaranteed Event Ordering           | Need **event ordering**                        | When events per user/session must stay in order                           | Key-based partitioning                               | **Partition + Ordering Guarantee** | Chat app → messages for same user land in order             |
| 8  | Stream Processing in Motion         | Process **streams in motion**                  | When you must enrich/filter/aggregate data on the fly                     | Use **Kafka Streams or KSQL**                        | **Stream Processing**              | Ride-sharing: calculate ETA with live traffic data          |
| 9  | Plug-and-Play Integrations          | Integrate with existing systems                | When data must flow to DBs, cloud storage, or other clusters              | Use **Kafka Connect** connectors                     | **Connectors**                     | Push logs to Elastic, load sales into Snowflake             |
| 10 | Global Data Distribution            | Build **geo-distributed systems**              | When you need multi-DC/cloud active-active setups                         | Replicate topics across clusters                     | **MirrorMaker / Cluster Linking**  | Global SaaS syncing events across regions                   |
